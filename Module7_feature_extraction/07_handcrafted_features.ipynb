{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d18b5f69-9607-4c14-8d99-45afbfc09836",
   "metadata": {},
   "source": [
    "# 🎓 Module 7: Hand-Crafted Feature Extraction\n",
    "\n",
    "In this module, we’ll **revisit and extend time- and frequency-domain features**, then explore **temporal and statistical descriptors** that underpin many modern audio-analysis pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔑 Key Concepts\n",
    "\n",
    "### 🕒 Basic Time-Domain & Spectral Features\n",
    "- **Zero-Crossing Rate**\n",
    "- **RMS Energy**\n",
    "- **Spectral Centroid**\n",
    "- **Spectral Bandwidth**\n",
    "- **Spectral Roll-off**\n",
    "\n",
    "### 🕺 Temporal Features\n",
    "- **Tempo Estimation**\n",
    "- **Onset Detection**\n",
    "\n",
    "### 📈 Statistical Descriptors\n",
    "- **Higher-order moments of spectral frames**:\n",
    "  - **Skewness**\n",
    "  - **Kurtosis**\n",
    "\n",
    "---\n",
    "\n",
    "## 📓 Notebook Demos\n",
    "\n",
    "### 🎧 Interactive Onset Detector\n",
    "- Compute an **onset strength envelope** (e.g., `librosa.onset_strength`)\n",
    "- Experiment with:\n",
    "  - **Detection threshold**\n",
    "  - **Hop length**\n",
    "- **Listen to \"clicks\"** at detected onset times\n",
    "\n",
    "---\n",
    "\n",
    "### 🎤 MFCC vs. Spectral Contrast\n",
    "- Extract **MFCCs** and **Spectral Contrast** from example clips\n",
    "- Compare feature **trajectories** for **speech vs. music**\n",
    "- Visualize:\n",
    "  - **Mean**\n",
    "  - **Variance** of each feature set\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠 Exercise: Feature-Based Classification\n",
    "\n",
    "### 🎯 Task:\n",
    "Extract a bank of **20 hand-crafted features** including:\n",
    "- **Time-domain**\n",
    "- **Spectral**\n",
    "- **Temporal**\n",
    "- **Statistical** descriptors  \n",
    "from a **collection of audio clips**.\n",
    "\n",
    "### 📊 Analysis:\n",
    "Implement a **simple k-Nearest Neighbors (k-NN) classifier** to distinguish **speech vs. music** using the extracted features.\n",
    "\n",
    "\n",
    "### 📦 Deliverables\n",
    "\n",
    "- ✅ Code to **compute and normalize** each feature\n",
    "- 📁 **Feature matrices** + **labels**\n",
    "- 📈 **Classification accuracy report** and **confusion matrix**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ab4670-f993-4c13-a05e-43f466d89c87",
   "metadata": {},
   "source": [
    "## 🔑 Key Concepts\n",
    "\n",
    "### 🕒 Basic Time-Domain & Spectral Features\n",
    "\n",
    "- **Zero-Crossing Rate (ZCR)**  \n",
    "  The rate at which the signal changes sign (crosses zero) per time frame.  \n",
    "  - **Why it matters:**  \n",
    "    - High ZCR ⇒ noise-like or percussive sounds  \n",
    "    - Low ZCR  ⇒ tonal, voiced speech, or sustained notes  \n",
    "\n",
    "- **RMS Energy**  \n",
    "  The root-mean-square of the signal amplitude over a short window.  \n",
    "  - **Why it matters:**  \n",
    "    - Measures perceptual loudness  \n",
    "    - Used for voice-activity detection, dynamic range analysis  \n",
    "\n",
    "- **Spectral Centroid**  \n",
    "  The “center of mass” of the magnitude spectrum:  \n",
    "  \\[\n",
    "    \\text{centroid} = \\frac{\\sum_k f_k\\,|X[k]|}{\\sum_k |X[k]|}\n",
    "  \\]  \n",
    "  - **Why it matters:**  \n",
    "    - Often correlates with perceived “brightness” of a sound  \n",
    "    - Higher centroid ⇒ brighter/tinny timbres  \n",
    "\n",
    "- **Spectral Bandwidth**  \n",
    "  The spread of the spectrum around its centroid (e.g.\\ standard deviation):  \n",
    "  \\[\n",
    "    \\text{bandwidth} = \\sqrt{\\frac{\\sum_k (f_k - \\text{centroid})^2\\,|X[k]|}{\\sum_k |X[k]|}}\n",
    "  \\]  \n",
    "  - **Why it matters:**  \n",
    "    - Indicates how “wide” or “narrow” the spectrum is  \n",
    "    - Wider bandwidth ⇒ richer harmonic content or noise  \n",
    "\n",
    "- **Spectral Roll-off**  \n",
    "  The frequency below which a fixed percentage (e.g.\\ 85%) of the spectral energy is contained.  \n",
    "  - **Why it matters:**  \n",
    "    - Another measure of brightness / high-frequency content  \n",
    "    - Useful for timbre classification and onset detection  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaab987-2187-4760-8a82-e87512367ee9",
   "metadata": {},
   "source": [
    "## 🔑 Key Concepts\n",
    "\n",
    "### ⏱️ Temporal Features\n",
    "\n",
    "- **Tempo Estimation**  \n",
    "  Determining the underlying “beat” or pulse of a musical excerpt, typically measured in beats per minute (BPM).  \n",
    "  - **Why it matters:**  \n",
    "    - Crucial for synchronization (e.g. time‐stretching, beat‐matching)  \n",
    "    - Used in music information retrieval and DJ software  \n",
    "\n",
    "- **Onset Detection**  \n",
    "  Identifying the precise time instants when new events (notes, drum hits, speech plosives) begin.  \n",
    "  - **Why it matters:**  \n",
    "    - Enables beat tracking, segmentation, and alignment  \n",
    "    - Forms the backbone of rhythm analysis and transcription  \n",
    "  - **Common method:**  \n",
    "    - Compute an onset strength envelope (e.g. spectral flux, energy changes)  \n",
    "    - Detect peaks above a threshold to mark onset times  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506d88c2-a9ce-4f55-81a6-9df6f5e8877c",
   "metadata": {},
   "source": [
    "### 📈 Statistical Descriptors\n",
    "\n",
    "- **Skewness**  \n",
    "  A measure of the asymmetry of the spectral distribution around its mean.  \n",
    "  - **Interpretation:**  \n",
    "    - Positive skew ⇒ longer tail toward higher frequencies (more high-frequency energy)  \n",
    "    - Negative skew ⇒ longer tail toward lower frequencies (more low-frequency energy)  \n",
    "  - **Why it matters:**  \n",
    "    - Can help distinguish instruments or sounds with non-symmetric spectral shapes  \n",
    "\n",
    "- **Kurtosis**  \n",
    "  A measure of the “peakedness” or tail weight of the spectral distribution.  \n",
    "  - **Interpretation:**  \n",
    "    - High kurtosis ⇒ sharp spectral peaks and heavy tails (tonal, resonant sounds)  \n",
    "    - Low kurtosis ⇒ flat, broad spectrum (noisy or diffuse sounds)  \n",
    "  - **Why it matters:**  \n",
    "    - Useful for detecting transient vs. sustained content, or for timbre classification  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430590af-f05a-45a3-82b1-db21a4f1bcef",
   "metadata": {},
   "source": [
    "# 🎯 Demo: Interactive Onset Detection\n",
    "\n",
    "In this demo, you’ll **compute and visualize an onset–strength envelope** to automatically detect **note or beat onsets** in an audio clip, and generate a **“click track”** at each detected onset.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 What the Code Does\n",
    "\n",
    "1. **Loads** your chosen audio file.\n",
    "2. Computes the **onset strength envelope** using **sliding-window spectral flux**  \n",
    "   *(via `librosa.onset_strength`)*.\n",
    "3. **Peak-picks local maxima** in the envelope based on your parameters.\n",
    "4. **Generates a click track** at the detected onset times  \n",
    "   *(using `librosa.clicks`)*.\n",
    "5. Plays back both the **original audio** and the **click track**.\n",
    "6. **Plots**:\n",
    "   - The **onset envelope** over time\n",
    "   - The **detection threshold**\n",
    "   - **Vertical lines** at each detected onset\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ USER SETTINGS (at the top of the cell)\n",
    "\n",
    "- **`FILENAME`**  \n",
    "  Your audio file (`.wav` / `.mp3`) placed in the `sounds/` folder.\n",
    "\n",
    "- **`HOP_LENGTH`** *(samples)*  \n",
    "  Frame step for computing the onset envelope  \n",
    "  *(typical values: 256–1024)*\n",
    "\n",
    "- **`PRE_MAX`, `POST_MAX`** *(frames)*  \n",
    "  Number of frames to look ahead/behind when identifying local peaks  \n",
    "  *(values ≥ 1)*\n",
    "\n",
    "- **`PRE_AVG`, `POST_AVG`** *(frames)*  \n",
    "  Number of frames in the local average baseline  \n",
    "  *(values ≥ 1)*\n",
    "\n",
    "- **`DELTA`** *(onset-strength units)*  \n",
    "  Minimum height above the local average to declare an onset  \n",
    "  *(e.g. 0.1–1.0)*\n",
    "\n",
    "- **`WAIT`** *(frames)*  \n",
    "  Minimum frames between successive onsets  \n",
    "  *(≥ 0)*\n",
    "\n",
    "---\n",
    "\n",
    "## 👀 What to Observe\n",
    "\n",
    "### 🎧 Audio Players\n",
    "\n",
    "- ▶️ **Original Audio** — listen to the raw signal\n",
    "- ▶️ **Onset Clicks** — hear the discrete clicks marking each detected onset\n",
    "\n",
    "---\n",
    "\n",
    "### 📉 Threshold Effect\n",
    "\n",
    "- **Raising `DELTA`** makes detection **more conservative** (fewer clicks)\n",
    "- **Lowering `DELTA`** may detect **spurious** or noise-induced onsets\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Envelope & Detections Plot\n",
    "\n",
    "- **Blue curve**: Onset strength over time\n",
    "- **Dashed red line**: Your chosen **threshold** (`DELTA`)\n",
    "- **Green vertical lines**: Detected **onset times**  \n",
    "  → Check if they align with **perceptual beats** or **attacks** in the audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d6ee82-a21d-45ec-b578-8ff73a313f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── USER SETTINGS ────────────────────────────────────────────────────────────────\n",
    "FILENAME    = 'grusel-melodie-305360.mp3'  # ← place your file in `sounds/`\n",
    "HOP_LENGTH  = 512                         # ← hop length for onset envelope (samples)\n",
    "PRE_MAX     = 1                           # ← lookahead for local peak (frames)\n",
    "POST_MAX    = 1                           # ← lookbehind for local peak (frames)\n",
    "PRE_AVG     = 1                           # ← lookahead for local average (frames)\n",
    "POST_AVG    = 1                           # ← lookbehind for local average (frames)\n",
    "DELTA       = 0.3                         # ← threshold for peak picking (onset strength units)\n",
    "WAIT        = 0                           # ← minimal frames between onsets\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "from IPython.display import Audio, display\n",
    "from pathlib import Path\n",
    "\n",
    "# ── CONFIG (don’t edit below here) ───────────────────────────────────────────────\n",
    "SOUNDS_DIR = Path('sounds')\n",
    "audio_path = SOUNDS_DIR / FILENAME\n",
    "\n",
    "# 1) Load audio\n",
    "y, sr = librosa.load(str(audio_path), sr=None)\n",
    "\n",
    "# 2) Compute onset strength envelope\n",
    "oenv = librosa.onset.onset_strength(y=y, sr=sr, hop_length=HOP_LENGTH)\n",
    "\n",
    "# 3) Detect onset frames with peak-picking\n",
    "onset_frames = librosa.onset.onset_detect(\n",
    "    onset_envelope=oenv,\n",
    "    sr=sr,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    pre_max=PRE_MAX,\n",
    "    post_max=POST_MAX,\n",
    "    pre_avg=PRE_AVG,\n",
    "    post_avg=POST_AVG,\n",
    "    delta=DELTA,\n",
    "    wait=WAIT,\n",
    "    units='frames'\n",
    ")\n",
    "onset_times = librosa.frames_to_time(onset_frames, sr=sr, hop_length=HOP_LENGTH)\n",
    "\n",
    "# 4) Generate click track at each onset time (updated API call)\n",
    "click_track = librosa.clicks(times=onset_times, sr=sr, length=len(y))\n",
    "\n",
    "# 5) Playback\n",
    "print(\"▶️ Original Audio\")\n",
    "display(Audio(data=y,           rate=sr, autoplay=False))\n",
    "print(\"▶️ Onset Clicks\")\n",
    "display(Audio(data=click_track, rate=sr, autoplay=False))\n",
    "\n",
    "# 6) Plot onset envelope and detections\n",
    "times = librosa.frames_to_time(np.arange(len(oenv)), sr=sr, hop_length=HOP_LENGTH)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(times, oenv, label='Onset Strength', color='C0')\n",
    "plt.hlines(DELTA, times[0], times[-1], colors='C1', linestyles='--',\n",
    "           label=f'Threshold = {DELTA}')\n",
    "for t in onset_times:\n",
    "    plt.axvline(t, color='C2', alpha=0.8)\n",
    "plt.title('Onset Strength Envelope & Detected Onsets')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Strength')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96be37c5-7992-4003-b9f8-78d756b31355",
   "metadata": {},
   "source": [
    "# 🎛️ Demo: MFCC vs. Spectral Contrast Comparison\n",
    "\n",
    "In this demo, you’ll **extract and compare two common feature sets**—**MFCCs** and **Spectral Contrast**—for a **speech clip** versus a **music clip**, and **visualize their summary statistics**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 What the Code Does\n",
    "\n",
    "1. **Loads two audio files**:\n",
    "   - One **speech** clip\n",
    "   - One **music** clip  \n",
    "   *(Handles unusual WAV codecs via `SoundFile`)*\n",
    "\n",
    "2. Displays **audio players** so you can **listen** to each clip.\n",
    "\n",
    "3. Computes:\n",
    "   - **MFCCs (Mel-frequency cepstral coefficients)**\n",
    "   - **Spectral Contrast** (difference between peaks and valleys in frequency sub-bands)\n",
    "\n",
    "4. Calculates:\n",
    "   - **Mean** and **variance** of each coefficient/band over time\n",
    "\n",
    "5. Plots:\n",
    "   - Two **side-by-side bar charts** for **MFCC mean & variance**\n",
    "   - Two **side-by-side bar charts** for **Spectral Contrast mean & variance**\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ USER SETTINGS\n",
    "\n",
    "- **`FILENAME_SPEECH`**: Name of your **speech clip** in `sounds/` (`.wav` or `.mp3`)\n",
    "- **`FILENAME_MUSIC`** : Name of your **music clip** in `sounds/` (`.wav` or `.mp3`)\n",
    "- **`SR`**: Target **sampling rate** (`None` to keep native rate)\n",
    "- **`N_MFCC`**: Number of **MFCC coefficients** (e.g., `13`)\n",
    "- **`N_FFT`**: **FFT window length** (e.g., `2048`)\n",
    "- **`HOP_LENGTH`**: Hop size between frames (e.g., `N_FFT // 4`)\n",
    "\n",
    "---\n",
    "\n",
    "## 👀 What to Observe\n",
    "\n",
    "### 🎼 MFCCs\n",
    "- Compare **average MFCC profiles** for **speech vs. music**:  \n",
    "  - **Speech** typically shows a different **spectral envelope shape**\n",
    "- **Variance** reveals how much each coefficient **fluctuates over time**  \n",
    "  - Speech may have more dynamic changes in the **lower coefficients**\n",
    "\n",
    "---\n",
    "\n",
    "### 🎚️ Spectral Contrast\n",
    "- **Mean contrast** shows typical **“peak-to-valley” differences** in each sub-band  \n",
    "  - Music with strong harmonic content often has **higher contrast**\n",
    "- **Variance** shows **temporal variability** in contrast  \n",
    "  - **Percussive music** may produce **larger fluctuations**\n",
    "\n",
    "---\n",
    "\n",
    "🧠 **Insight**:  \n",
    "By comparing **MFCCs and Spectral Contrast**, you can observe how **different types of audio** (speech vs. music) produce **distinct timbral signatures**—a crucial step for tasks like **classification, clustering, and audio understanding**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaaf8a3-c874-4b25-96c8-ca7fb8547f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── USER SETTINGS ────────────────────────────────────────────────────────────────\n",
    "FILENAME_SPEECH = 'speech.WAV'    # ← place your speech clip in `sounds/`\n",
    "FILENAME_MUSIC  = 'music.mp3'     # ← place your music clip in `sounds/`\n",
    "SR              = None            # ← sampling rate (None to use file’s native rate)\n",
    "N_MFCC          = 13              # ← number of MFCC coefficients\n",
    "N_FFT           = 2048            # ← FFT window size (power of two)\n",
    "HOP_LENGTH      = N_FFT // 4      # ← hop length between frames\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio, display\n",
    "from pathlib import Path\n",
    "\n",
    "# ── CONFIG (don’t edit below here) ───────────────────────────────────────────────\n",
    "SOUNDS_DIR = Path('sounds')\n",
    "\n",
    "def load_audio(path, sr=None):\n",
    "    ext = path.suffix.lower()\n",
    "    if ext == '.wav':\n",
    "        y, sr_native = sf.read(str(path), dtype='float32')\n",
    "        return (y, sr_native) if sr is None else (librosa.resample(y, sr_native, sr), sr)\n",
    "    else:\n",
    "        return librosa.load(str(path), sr=sr)\n",
    "\n",
    "# 1) Load audio\n",
    "path_s = SOUNDS_DIR / FILENAME_SPEECH\n",
    "path_m = SOUNDS_DIR / FILENAME_MUSIC\n",
    "y_s, sr_s = load_audio(path_s, sr=SR)\n",
    "y_m, sr_m = load_audio(path_m, sr=SR)\n",
    "\n",
    "# 2) Display audio players\n",
    "print(\"▶️ Speech Clip\")\n",
    "display(Audio(data=y_s, rate=sr_s, autoplay=False))\n",
    "print(\"▶️ Music Clip\")\n",
    "display(Audio(data=y_m, rate=sr_m, autoplay=False))\n",
    "\n",
    "# 3) Extract MFCCs\n",
    "mfcc_s = librosa.feature.mfcc(y=y_s, sr=sr_s,\n",
    "                              n_mfcc=N_MFCC,\n",
    "                              n_fft=N_FFT,\n",
    "                              hop_length=HOP_LENGTH)\n",
    "mfcc_m = librosa.feature.mfcc(y=y_m, sr=sr_m,\n",
    "                              n_mfcc=N_MFCC,\n",
    "                              n_fft=N_FFT,\n",
    "                              hop_length=HOP_LENGTH)\n",
    "\n",
    "# 4) Extract Spectral Contrast\n",
    "contrast_s = librosa.feature.spectral_contrast(y=y_s, sr=sr_s,\n",
    "                                               n_fft=N_FFT,\n",
    "                                               hop_length=HOP_LENGTH)\n",
    "contrast_m = librosa.feature.spectral_contrast(y=y_m, sr=sr_m,\n",
    "                                               n_fft=N_FFT,\n",
    "                                               hop_length=HOP_LENGTH)\n",
    "\n",
    "# 5) Compute statistics\n",
    "mfcc_mean_s = mfcc_s.mean(axis=1)\n",
    "mfcc_var_s  = mfcc_s.var(axis=1)\n",
    "mfcc_mean_m = mfcc_m.mean(axis=1)\n",
    "mfcc_var_m  = mfcc_m.var(axis=1)\n",
    "\n",
    "cont_mean_s = contrast_s.mean(axis=1)\n",
    "cont_var_s  = contrast_s.var(axis=1)\n",
    "cont_mean_m = contrast_m.mean(axis=1)\n",
    "cont_var_m  = contrast_m.var(axis=1)\n",
    "\n",
    "# 6) Plot MFCC mean & variance with constrained layout\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5), constrained_layout=True)\n",
    "idx = np.arange(N_MFCC)\n",
    "\n",
    "axs[0].bar(idx - 0.2, mfcc_mean_s, width=0.4, label='Speech')\n",
    "axs[0].bar(idx + 0.2, mfcc_mean_m, width=0.4, label='Music')\n",
    "axs[0].set_title('MFCC Mean')\n",
    "axs[0].set_xlabel('Coefficient Index')\n",
    "axs[0].set_ylabel('Mean Value')\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].bar(idx - 0.2, mfcc_var_s, width=0.4, label='Speech')\n",
    "axs[1].bar(idx + 0.2, mfcc_var_m, width=0.4, label='Music')\n",
    "axs[1].set_title('MFCC Variance')\n",
    "axs[1].set_xlabel('Coefficient Index')\n",
    "axs[1].set_ylabel('Variance')\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "fig.suptitle('MFCC Statistics: Speech vs. Music', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# 7) Plot Spectral Contrast mean & variance with constrained layout\n",
    "n_bands = contrast_s.shape[0]\n",
    "idx_c   = np.arange(n_bands)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5), constrained_layout=True)\n",
    "\n",
    "axs[0].bar(idx_c - 0.2, cont_mean_s, width=0.4, label='Speech')\n",
    "axs[0].bar(idx_c + 0.2, cont_mean_m, width=0.4, label='Music')\n",
    "axs[0].set_title('Spectral Contrast Mean')\n",
    "axs[0].set_xlabel('Frequency Band Index')\n",
    "axs[0].set_ylabel('Mean Value (dB)')\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].bar(idx_c - 0.2, cont_var_s, width=0.4, label='Speech')\n",
    "axs[1].bar(idx_c + 0.2, cont_var_m, width=0.4, label='Music')\n",
    "axs[1].set_title('Spectral Contrast Variance')\n",
    "axs[1].set_xlabel('Frequency Band Index')\n",
    "axs[1].set_ylabel('Variance')\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "fig.suptitle('Spectral Contrast Statistics: Speech vs. Music', y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122ebb2e-3712-45d0-b050-32bd27955f20",
   "metadata": {},
   "source": [
    "## 🛠 Exercise: Feature-Based Classification\n",
    "\n",
    "### 🎯 Task:\n",
    "Extract a bank of **20 hand-crafted features**—spanning time-domain, spectral, temporal, and statistical descriptors—from a collection of audio clips (speech and music).\n",
    "\n",
    "### 📊 Analysis:\n",
    "1. **Feature Extraction**  \n",
    "   - Compute features such as zero-crossing rate, RMS energy, spectral centroid, bandwidth, roll-off, tempo, onset rate, skewness, kurtosis, MFCC statistics, spectral contrast statistics, etc., for each clip.  \n",
    "   - Normalize or standardize each feature across the dataset.\n",
    "\n",
    "2. **k-Nearest Neighbors Classification**  \n",
    "   - Split your dataset into training and test sets.  \n",
    "   - Train a simple k-NN classifier on the feature vectors to distinguish **speech vs. music**.  \n",
    "   - Tune the number of neighbors \\(k\\) via cross-validation.\n",
    "\n",
    "### 📦 Deliverables:\n",
    "- ✅ A script or notebook section that **computes and normalizes** all 20 features for each audio clip.  \n",
    "- 📁 Two arrays/matrices:  \n",
    "  - **X** of shape \\((n\\_samples, 20)\\) containing feature vectors  \n",
    "  - **y** of shape \\((n\\_samples,)\\) containing binary labels (0 = speech, 1 = music)  \n",
    "- 📈 A **classification report** showing accuracy, precision, recall, and F1-score.  \n",
    "- 📉 A **confusion matrix** visualization for your test set.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1cca68-48e1-4213-9173-e66bf6d51cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
