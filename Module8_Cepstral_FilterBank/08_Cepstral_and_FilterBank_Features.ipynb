{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad600a60-6b9b-4dcf-8fe2-170ec98904d6",
   "metadata": {},
   "source": [
    "## Module 8: Cepstral & Filter-Bank Features\n",
    "\n",
    "In this module, we’ll study how to transform audio into perceptually meaningful representations using cepstral and filter-bank techniques.\n",
    "\n",
    "### Key Concepts\n",
    "- **Cepstrum & Quefrency Domain**  \n",
    "  - The “spectrum of a spectrum” obtained by taking the inverse Fourier transform of log-magnitude spectra.  \n",
    "  - **Liftering** to separate source vs. filter contributions (e.g. pitch vs. envelope).\n",
    "- **Mel-Filter Banks**  \n",
    "  - Triangular filters spaced on the mel scale to mimic human auditory resolution.  \n",
    "  - Build the mel-spectrogram and derive MFCCs by DCT of log-mel energies.\n",
    "- **Δ & ΔΔ (Temporal Derivatives)**  \n",
    "  - Capture short-term dynamics by computing first and second derivatives of cepstral or filter-bank features.\n",
    "\n",
    "---\n",
    "\n",
    "### 📓 Notebook Demos\n",
    "\n",
    "1. **Mel-Filter Bank Visualization**  \n",
    "   - Compute magnitude spectrum of a sample frame  \n",
    "   - Overlay a bank of mel-filters  \n",
    "   - Slider to adjust the number of filters (e.g. 20–80) and see how coverage changes\n",
    "\n",
    "2. **MFCC Reconstruction**  \n",
    "   - Compute MFCCs from an audio clip  \n",
    "   - Reconstruct time-domain audio using only the first 13 coefficients vs. the full log-spectrum  \n",
    "   - Listen and compare how much fine detail is preserved\n",
    "\n",
    "3. **Δ & ΔΔ Feature Visualization**  \n",
    "   - Compute static MFCCs, Δ-MFCCs, and ΔΔ-MFCCs for a speech segment  \n",
    "   - Plot their trajectories over time to observe how dynamics capture transitions\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠 Exercise: Filter-Bank Shape Comparison\n",
    "- **Task:** Evaluate how different auditory filter-bank shapes (triangular vs. gammatone) affect a simple speech-recognition pipeline.\n",
    "- **Steps:**  \n",
    "  1. Implement both triangular mel filters and gammatone filterbanks.  \n",
    "  2. Extract filter-bank features from a small spoken-digit dataset.  \n",
    "  3. Train a lightweight classifier (e.g. logistic regression) and compare recognition accuracy.  \n",
    "- **Deliverables:**  \n",
    "  - Plots of both filter-bank shapes overlaid on a spectrum.  \n",
    "  - Recognition accuracy table for each filter type.  \n",
    "  - Discussion of how filter-bank design influences feature discriminability.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6272141f-96a8-4e4e-bc62-d389b016cc39",
   "metadata": {},
   "source": [
    "## 🔑 Key Concepts\n",
    "\n",
    "- **Cepstrum & Quefrency Domain**  \n",
    "  - The **cepstrum** is the “spectrum of a spectrum” — you take the log-magnitude of the Fourier transform, then apply an inverse Fourier transform to reveal **quefrency** (time-like) components.  \n",
    "  - **Quefrency** bins correspond to periodicities in the log-spectrum, making it easy to separate vocal tract envelope from pitch harmonics.  \n",
    "  - **Liftering** applies a window in the quefrency domain to isolate either the slowly-varying envelope (low-quefrency “filter” lifter) or the rapid pitch information (high-quefrency “source” lifter).  \n",
    "  - This separation underpins techniques like **mel-cepstral analysis** and **pitch–envelope decomposition**.\n",
    "    \n",
    "- **MFCC Reconstruction**  \n",
    "  - Extract the first 13 **Mel-Frequency Cepstral Coefficients** (MFCCs) from an audio signal.  \n",
    "  - Reconstruct the time-domain waveform by inverting the MFCCs—first using only the low-order (1–13) “envelope” coefficients, then using the full log-spectrum.  \n",
    "  - **Listen & compare:**  \n",
    "    - **13-coef reconstruction** retains the broad spectral shape (timbre) but loses fine transient details.  \n",
    "    - **Full-spectrum inversion** (all bins) recovers more detail at the cost of higher computational complexity.\n",
    "      \n",
    "- **Δ & ΔΔ (Temporal Derivatives)**  \n",
    "  - Capture short-term dynamics by computing first (Δ) and second (ΔΔ) time-derivatives of cepstral or filter-bank features, highlighting how spectral patterns evolve over successive frames.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bee230-09c8-401b-9071-18563fd6431d",
   "metadata": {},
   "source": [
    " \n",
    "# 🎚️ Demo: Mel-Filter Bank Visualization\n",
    "\n",
    "In this demo, you’ll explore how a **bank of mel-frequency filters** maps a **linear-frequency spectrum** into **perceptual bands**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 What the Code Does\n",
    "\n",
    "1. **Loads your audio clip** and computes its **STFT magnitude** \\( D \\) using `N_FFT` and `HOP_LENGTH`.\n",
    "2. Displays an **audio player** to listen to the **original signal**.\n",
    "3. **Plots the magnitude spectrum** of a single STFT frame (`FRAME_IDX`) vs. frequency.\n",
    "4. **Overlays a triangular mel-filter bank** (with `N_MELS` filters) on that spectrum.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Inputs (Edit at the Top of the Code Cell)\n",
    "\n",
    "- **`FILENAME`**  \n",
    "  Name of your audio file in `sounds/` (supports `.wav` or `.mp3`)\n",
    "\n",
    "- **`FRAME_IDX`**  \n",
    "  Integer in `[0 … n_frames - 1]` selecting the time frame to display\n",
    "\n",
    "- **`N_FFT`**  \n",
    "  FFT window size (power of 2, e.g., `1024`, `2048`)\n",
    "\n",
    "- **`HOP_LENGTH`**  \n",
    "  Hop size between STFT frames (≤ `N_FFT`, e.g., `N_FFT // 4`)\n",
    "\n",
    "- **`N_MELS`**  \n",
    "  Number of mel filters (e.g., 20 to 80)\n",
    "\n",
    "---\n",
    "\n",
    "## 📤 Outputs to Observe\n",
    "\n",
    "### 🎧 Audio Player\n",
    "- Listen to the **original recording**\n",
    "\n",
    "### 📈 Spectrum Plot\n",
    "- **Blue curve**: Magnitude spectrum \\|STFT\\| at frame `FRAME_IDX`\n",
    "- **Semi-transparent lines**: Each of the `N_MELS` **mel filters** overlaid on the spectrum\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 How to Interpret\n",
    "\n",
    "### 🎼 Low Frequencies\n",
    "- Mel filters are **narrower** and **densely packed**  \n",
    "  → Yields **higher resolution**\n",
    "\n",
    "### 🎼 High Frequencies\n",
    "- Mel filters become **wider** and **sparser**  \n",
    "  → Lower resolution, matching human perception\n",
    "\n",
    "### 🎚️ Changing `N_MELS`\n",
    "- **More filters** → Finer resolution (especially in low frequencies), but **more overlap**\n",
    "- **Fewer filters** → Coarser band coverage\n",
    "\n",
    "---\n",
    "\n",
    "🔍 **Try editing `N_MELS`** in the code and **re-running the cell** to see how the **mel filter bank adapts** to match **human auditory perception**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bb2ebe-76e1-4883-8157-0764962d3817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── USER SETTINGS ────────────────────────────────────────────────────────────────\n",
    "FILENAME    = 'speech.WAV'   # ← place your audio file in `sounds/`\n",
    "FRAME_IDX   = 0              # ← which STFT frame to visualize (0…n_frames-1)\n",
    "N_FFT       = 2048           # ← FFT window size (power of 2)\n",
    "HOP_LENGTH  = 512            # ← hop length between frames\n",
    "N_MELS      = 40             # ← number of mel filters (e.g. 20…80)\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "from pathlib import Path\n",
    "\n",
    "# ── CONFIG (don’t edit below here) ───────────────────────────────────────────────\n",
    "SOUNDS_DIR = Path('sounds')\n",
    "audio_path = SOUNDS_DIR / FILENAME\n",
    "\n",
    "# 1) Load audio and compute STFT magnitudes\n",
    "y, sr = librosa.load(str(audio_path), sr=None)\n",
    "D     = np.abs(librosa.stft(y, n_fft=N_FFT, hop_length=HOP_LENGTH))\n",
    "freqs = librosa.fft_frequencies(sr=sr, n_fft=N_FFT)\n",
    "n_frames = D.shape[1]\n",
    "\n",
    "# 2) Listen to the original audio\n",
    "print(\"▶️ Original Audio\")\n",
    "display(Audio(data=y, rate=sr, autoplay=False))\n",
    "\n",
    "# 3) Compute mel filter bank\n",
    "mel_fb = librosa.filters.mel(\n",
    "    sr=sr,\n",
    "    n_fft=N_FFT,\n",
    "    n_mels=N_MELS\n",
    ")\n",
    "\n",
    "# 4) Pick and plot the chosen frame\n",
    "mag_frame = D[:, FRAME_IDX]\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(freqs, mag_frame, label=f'|STFT|, frame {FRAME_IDX}', color='C0')\n",
    "for i in range(N_MELS):\n",
    "    plt.plot(freqs, mel_fb[i], alpha=0.5, linewidth=1)\n",
    "plt.title(f'Mel Filter Bank Overlay (n_mels = {N_MELS})')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af861d6-4312-48fd-969e-8aa43605e544",
   "metadata": {},
   "source": [
    "# 🎛️ Demo: MFCC Reconstruction Comparison\n",
    "\n",
    "In this demo, you’ll explore how **retaining more (or fewer) cepstral coefficients** affects the **quality of a reconstructed audio signal**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 What the Code Does\n",
    "\n",
    "1. **Loads your audio clip**.\n",
    "2. Computes **MFCCs** using a **mel filter bank** of `N_MELS` bands and `N_FFT` FFT size:\n",
    "   - **Short MFCC**: Keep only the first `N_MFCC` coefficients per frame  \n",
    "     → Coarse spectral envelope\n",
    "   - **Full MFCC**: Keep all `N_MELS` coefficients  \n",
    "     → Retains full log-spectrum detail\n",
    "3. **Inverts** each MFCC set back to time domain via  \n",
    "   ```python\n",
    "   librosa.feature.inverse.mfcc_to_audio\n",
    "```\n",
    "*(Uses **Griffin–Lim algorithm** under the hood)*\n",
    "\n",
    "---\n",
    "\n",
    "### ▶️ 4. Plays Back Three Versions\n",
    "\n",
    "- **Original**  \n",
    "- **Reconstructed from first `N_MFCC` coefficients**  \n",
    "- **Reconstructed from all `N_MELS` coefficients**\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Inputs (Edit at Top of the Code Cell)\n",
    "\n",
    "- **`FILENAME`**  \n",
    "  Your input file in the `sounds/` folder (`.wav` or `.mp3`)\n",
    "\n",
    "- **`N_MELS`** (e.g., 20–80)  \n",
    "  Number of **mel bands** for the filter bank\n",
    "\n",
    "- **`N_MFCC`** (≤ `N_MELS`, e.g., 8–20)  \n",
    "  Number of **cepstral coefficients** to keep in the “short” version\n",
    "\n",
    "- **`N_FFT`** (power of 2, e.g., 512, 1024, 2048)  \n",
    "  **FFT window length**\n",
    "\n",
    "- **`HOP_LENGTH`** (≤ `N_FFT`, typically `N_FFT // 4`)  \n",
    "  **Frame shift** between successive STFT frames\n",
    "\n",
    "---\n",
    "\n",
    "## 📤 Outputs to Observe\n",
    "\n",
    "### 🎧 Audio Players\n",
    "\n",
    "- **Original**: Your untouched audio clip  \n",
    "- **Short MFCC**: Captures **overall timbre** but may lose **transient fine-structure**  \n",
    "- **Full MFCC**: Closer to the original, preserving more **spectral detail**\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What to Listen For\n",
    "\n",
    "### 🎼 Envelope vs. Detail\n",
    "- Does the **“short” MFCC version** sound **muffled** or **smeared**?\n",
    "\n",
    "### 🌀 Artifacts\n",
    "- The **Griffin–Lim reconstruction** may introduce **phasiness** — how noticeable is it?\n",
    "\n",
    "### 📉 Coefficient Count Impact\n",
    "- How many **MFCCs** are needed to **approach the original quality**?\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Experiment\n",
    "\n",
    "Try varying `N_MFCC` (e.g., **5 → 13 → 40**) and observe how **reconstruction quality improves** as you include more **cepstral detail**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f91e914-9842-4aba-9882-e560d72087b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── USER SETTINGS ────────────────────────────────────────────────────────────────\n",
    "FILENAME    = 'drum_hit3.wav'  # ← place your file in `sounds/` (WAV or MP3)\n",
    "N_MELS      = 40                     # ← number of mel bands used in MFCC\n",
    "N_MFCC      = 13                     # ← number of MFCC coefficients to keep\n",
    "N_FFT       = 2048                   # ← FFT window size (power of 2)\n",
    "HOP_LENGTH  = N_FFT // 4             # ← hop length between frames\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "from pathlib import Path\n",
    "\n",
    "# ── CONFIG (don’t edit below here) ───────────────────────────────────────────────\n",
    "SOUNDS_DIR = Path('sounds')\n",
    "audio_path = SOUNDS_DIR / FILENAME\n",
    "\n",
    "# 1) Load audio\n",
    "y, sr = librosa.load(str(audio_path), sr=None)\n",
    "\n",
    "# 2) Compute MFCCs\n",
    "#  - 'short' MFCCs: only first N_MFCC coefficients\n",
    "mfcc_short = librosa.feature.mfcc(\n",
    "    y=y, sr=sr,\n",
    "    n_mfcc=N_MFCC,\n",
    "    n_mels=N_MELS,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP_LENGTH\n",
    ")\n",
    "#  - 'full' MFCCs: same number of bands as mel filters\n",
    "mfcc_full = librosa.feature.mfcc(\n",
    "    y=y, sr=sr,\n",
    "    n_mfcc=N_MELS,\n",
    "    n_mels=N_MELS,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP_LENGTH\n",
    ")\n",
    "\n",
    "# 3) Reconstruct audio from MFCCs via inverse transform + Griffin-Lim\n",
    "y_rec_short = librosa.feature.inverse.mfcc_to_audio(\n",
    "    mfcc_short,\n",
    "    sr=sr,\n",
    "    n_mels=N_MELS,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP_LENGTH\n",
    ")\n",
    "y_rec_full = librosa.feature.inverse.mfcc_to_audio(\n",
    "    mfcc_full,\n",
    "    sr=sr,\n",
    "    n_mels=N_MELS,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP_LENGTH\n",
    ")\n",
    "\n",
    "# 4) Playback: Original vs. Short MFCC vs. Full MFCC\n",
    "print(\"▶️ Original Audio\")\n",
    "display(Audio(data=y,          rate=sr, autoplay=False))\n",
    "print(f\"▶️ Reconstructed from first {N_MFCC} MFCCs\")\n",
    "display(Audio(data=y_rec_short, rate=sr, autoplay=False))\n",
    "print(f\"▶️ Reconstructed from all {N_MELS} MFCCs\")\n",
    "display(Audio(data=y_rec_full,  rate=sr, autoplay=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37999425-cfd1-450f-ac45-0ec657790748",
   "metadata": {},
   "source": [
    "# 🎛️ Demo: Δ & ΔΔ (Temporal Derivatives) Feature Visualization\n",
    "\n",
    "In this demo, you’ll compute the **static MFCCs**, their **first (Δ)** and **second (ΔΔ)** derivatives, and **plot the trajectories** of the first few coefficients over time. This helps visualize how **dynamics in the cepstral domain** capture **onsets and transitions** in audio.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ User-Configurable Inputs  \n",
    "*(Edit at the top of the code cell)*\n",
    "\n",
    "- **`FILENAME`**  \n",
    "  Name of your audio file in the `sounds/` folder (`.wav` or `.mp3`)\n",
    "\n",
    "- **`SR`**  \n",
    "  Sampling rate to use (`None` to use the file’s native rate)\n",
    "\n",
    "- **`N_MFCC`**  \n",
    "  Number of MFCC coefficients to compute (**≥ 3** recommended)\n",
    "\n",
    "- **`N_FFT`**  \n",
    "  FFT window size (power of two, e.g., `1024`, `2048`)\n",
    "\n",
    "- **`HOP_LENGTH`**  \n",
    "  Hop size between frames (≤ `N_FFT`, often `N_FFT // 4`)\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 What the Code Does\n",
    "\n",
    "1. **Loads your clip**  \n",
    "   *(Using `SoundFile` for `.wav`, `librosa` otherwise)*  \n",
    "   Plays the audio\n",
    "\n",
    "2. **Computes**:\n",
    "   - **Static MFCCs** (`n_mfcc = N_MFCC`)\n",
    "   - **Δ-MFCCs** (first derivative)\n",
    "   - **ΔΔ-MFCCs** (second derivative)\n",
    "\n",
    "3. **Plots** the time-series of the **first 3 coefficients**:\n",
    "   - **Solid lines** = Static MFCC\n",
    "   - **Dashed lines** = Δ-MFCC\n",
    "   - **Dotted lines** = ΔΔ-MFCC\n",
    "\n",
    "---\n",
    "\n",
    "## 📤 Outputs to Observe\n",
    "\n",
    "### 🎧 Audio Player\n",
    "- Verify the clip **loaded and plays correctly**\n",
    "\n",
    "### 📈 Trajectory Plots\n",
    "- **Static MFCCs** show smooth **spectral envelope changes**\n",
    "- **Δ (slope)** peaks at **onsets** or **rapid spectral shifts**\n",
    "- **ΔΔ (acceleration)** highlights **changes in slope** — very sensitive to **transient bursts**\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 How to Interpret\n",
    "\n",
    "- **Peaks in the Δ curve** correspond to **rhythmic or attack events**\n",
    "- **Sharp spikes in ΔΔ** indicate **rapid changes in dynamics** or **timbre**\n",
    "- Together, **Δ and ΔΔ features** capture **temporal patterns** that **static MFCCs alone cannot**, making them **crucial for tasks** like:\n",
    "  - **Speech/music recognition**\n",
    "  - **Onset detection**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de38e432-fe75-4161-9f2a-b5c7e70d6bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── USER SETTINGS ────────────────────────────────────────────────────────────────\n",
    "FILENAME    = 'speech.WAV'   # ← place your audio clip in `sounds/`\n",
    "SR          = None           # ← sampling rate (None to use file’s native rate)\n",
    "N_MFCC      = 13             # ← number of MFCC coefficients\n",
    "N_FFT       = 2048           # ← FFT window size (power of two)\n",
    "HOP_LENGTH  = N_FFT // 4     # ← hop length between frames\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "from IPython.display import Audio, display\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "\n",
    "# ── CONFIG (don’t edit below here) ───────────────────────────────────────────────\n",
    "SOUNDS_DIR = Path('sounds')\n",
    "path = SOUNDS_DIR / FILENAME\n",
    "\n",
    "def load_audio(path, sr=None):\n",
    "    ext = path.suffix.lower()\n",
    "    if ext == '.wav':\n",
    "        y, sr_native = sf.read(str(path), dtype='float32')\n",
    "        return (y, sr_native) if sr is None else (librosa.resample(y, sr_native, sr), sr)\n",
    "    else:\n",
    "        return librosa.load(str(path), sr=sr)\n",
    "\n",
    "# 1) Load audio\n",
    "y, sr = load_audio(path, sr=SR)\n",
    "\n",
    "# 2) Play the clip\n",
    "print(\"▶️ Original Audio\")\n",
    "display(Audio(data=y, rate=sr, autoplay=False))\n",
    "\n",
    "# 3) Compute static MFCCs\n",
    "mfcc     = librosa.feature.mfcc(\n",
    "    y=y, sr=sr,\n",
    "    n_mfcc=N_MFCC,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP_LENGTH\n",
    ")\n",
    "\n",
    "# 4) Compute Δ and ΔΔ\n",
    "mfcc_delta  = librosa.feature.delta(mfcc, order=1)\n",
    "mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "\n",
    "# 5) Prepare time axis\n",
    "times = librosa.frames_to_time(\n",
    "    np.arange(mfcc.shape[1]),\n",
    "    sr=sr,\n",
    "    hop_length=HOP_LENGTH\n",
    ")\n",
    "\n",
    "# 6) Plot trajectories for the first 3 coefficients\n",
    "coeffs_to_plot = min(3, N_MFCC)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i in range(coeffs_to_plot):\n",
    "    plt.subplot(coeffs_to_plot, 1, i+1)\n",
    "    plt.plot(times, mfcc[i],       label=f'MFCC {i+1}',    linewidth=1.5)\n",
    "    plt.plot(times, mfcc_delta[i],  '--', label='Δ-MFCC',    linewidth=1.2)\n",
    "    plt.plot(times, mfcc_delta2[i], ':',  label='ΔΔ-MFCC',   linewidth=1.2)\n",
    "    plt.title(f'Coefficient {i+1} Trends')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa21500-47e6-4900-a2ac-331369d2f039",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🛠 Exercise: Filter-Bank Shape Comparison\n",
    "\n",
    "### 🎯 Task\n",
    "Evaluate how different **auditory filter-bank shapes** — **triangular** (Mel) vs. **gammatone** — affect a **simple speech-recognition pipeline**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧾 Steps\n",
    "\n",
    "1. **Implement both filter types**:\n",
    "   - Triangular **mel filters** (e.g., via `librosa.filters.mel`)\n",
    "   - **Gammatone filterbank** (e.g., using `auditory-toolbox`, `gammatone` library, or custom code)\n",
    "\n",
    "2. **Extract filter-bank features** from a **small spoken-digit dataset**  \n",
    "   *(e.g., Free Spoken Digit Dataset — FSDD)*\n",
    "\n",
    "3. **Train a lightweight classifier**  \n",
    "   (e.g., **logistic regression**, **SVM**, or **k-NN**)  \n",
    "   to compare **recognition accuracy** across filter types\n",
    "\n",
    "---\n",
    "\n",
    "### 📦 Deliverables\n",
    "\n",
    "- 📊 **Plots** of both **filter-bank shapes** overlaid on a representative **magnitude spectrum**\n",
    "- 🧮 **Recognition accuracy table** comparing **mel vs. gammatone** features\n",
    "- ✍️ **Brief discussion**:  \n",
    "  - How does **filter-bank design** influence **feature discriminability**?\n",
    "  - Which shape better captures important cues for digit classification?\n",
    "\n",
    "---\n",
    "\n",
    "💡 *Tip:* Try reducing dimensionality (e.g., via PCA) to visualize feature separability between digits for each filter type!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120bafe5-2d4a-4fcc-a507-702a12e26bca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
