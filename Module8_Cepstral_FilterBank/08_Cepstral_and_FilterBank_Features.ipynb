{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad600a60-6b9b-4dcf-8fe2-170ec98904d6",
   "metadata": {},
   "source": [
    "## Module 8: Cepstral & Filter-Bank Features\n",
    "\n",
    "In this module, weâ€™ll study how to transform audio into perceptually meaningful representations using cepstral and filter-bank techniques.\n",
    "\n",
    "### Key Concepts\n",
    "- **Cepstrum & Quefrency Domain**  \n",
    "  - The â€œspectrum of a spectrumâ€ obtained by taking the inverse Fourier transform of log-magnitude spectra.  \n",
    "  - **Liftering** to separate source vs. filter contributions (e.g. pitch vs. envelope).\n",
    "- **Mel-Filter Banks**  \n",
    "  - Triangular filters spaced on the mel scale to mimic human auditory resolution.  \n",
    "  - Build the mel-spectrogram and derive MFCCs by DCT of log-mel energies.\n",
    "- **Î” & Î”Î” (Temporal Derivatives)**  \n",
    "  - Capture short-term dynamics by computing first and second derivatives of cepstral or filter-bank features.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ““ Notebook Demos\n",
    "\n",
    "1. **Mel-Filter Bank Visualization**  \n",
    "   - Compute magnitude spectrum of a sample frame  \n",
    "   - Overlay a bank of mel-filters  \n",
    "   - Slider to adjust the number of filters (e.g. 20â€“80) and see how coverage changes\n",
    "\n",
    "2. **MFCC Reconstruction**  \n",
    "   - Compute MFCCs from an audio clip  \n",
    "   - Reconstruct time-domain audio using only the first 13 coefficients vs. the full log-spectrum  \n",
    "   - Listen and compare how much fine detail is preserved\n",
    "\n",
    "3. **Î” & Î”Î” Feature Visualization**  \n",
    "   - Compute static MFCCs, Î”-MFCCs, and Î”Î”-MFCCs for a speech segment  \n",
    "   - Plot their trajectories over time to observe how dynamics capture transitions\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ›  Exercise: Filter-Bank Shape Comparison\n",
    "- **Task:** Evaluate how different auditory filter-bank shapes (triangular vs. gammatone) affect a simple speech-recognition pipeline.\n",
    "- **Steps:**  \n",
    "  1. Implement both triangular mel filters and gammatone filterbanks.  \n",
    "  2. Extract filter-bank features from a small spoken-digit dataset.  \n",
    "  3. Train a lightweight classifier (e.g. logistic regression) and compare recognition accuracy.  \n",
    "- **Deliverables:**  \n",
    "  - Plots of both filter-bank shapes overlaid on a spectrum.  \n",
    "  - Recognition accuracy table for each filter type.  \n",
    "  - Discussion of how filter-bank design influences feature discriminability.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6272141f-96a8-4e4e-bc62-d389b016cc39",
   "metadata": {},
   "source": [
    "## ğŸ”‘ Key Concepts\n",
    "\n",
    "- **Cepstrum & Quefrency Domain**  \n",
    "  - The **cepstrum** is the â€œspectrum of a spectrumâ€ â€” you take the log-magnitude of the Fourier transform, then apply an inverse Fourier transform to reveal **quefrency** (time-like) components.  \n",
    "  - **Quefrency** bins correspond to periodicities in the log-spectrum, making it easy to separate vocal tract envelope from pitch harmonics.  \n",
    "  - **Liftering** applies a window in the quefrency domain to isolate either the slowly-varying envelope (low-quefrency â€œfilterâ€ lifter) or the rapid pitch information (high-quefrency â€œsourceâ€ lifter).  \n",
    "  - This separation underpins techniques like **mel-cepstral analysis** and **pitchâ€“envelope decomposition**.\n",
    "    \n",
    "- **MFCC Reconstruction**  \n",
    "  - Extract the first 13 **Mel-Frequency Cepstral Coefficients** (MFCCs) from an audio signal.  \n",
    "  - Reconstruct the time-domain waveform by inverting the MFCCsâ€”first using only the low-order (1â€“13) â€œenvelopeâ€ coefficients, then using the full log-spectrum.  \n",
    "  - **Listen & compare:**  \n",
    "    - **13-coef reconstruction** retains the broad spectral shape (timbre) but loses fine transient details.  \n",
    "    - **Full-spectrum inversion** (all bins) recovers more detail at the cost of higher computational complexity.\n",
    "      \n",
    "- **Î” & Î”Î” (Temporal Derivatives)**  \n",
    "  - Capture short-term dynamics by computing first (Î”) and second (Î”Î”) time-derivatives of cepstral or filter-bank features, highlighting how spectral patterns evolve over successive frames.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bee230-09c8-401b-9071-18563fd6431d",
   "metadata": {},
   "source": [
    " \n",
    "# ğŸšï¸ Demo: Mel-Filter Bank Visualization\n",
    "\n",
    "In this demo, youâ€™ll explore how a **bank of mel-frequency filters** maps a **linear-frequency spectrum** into **perceptual bands**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” What the Code Does\n",
    "\n",
    "1. **Loads your audio clip** and computes its **STFT magnitude** \\( D \\) using `N_FFT` and `HOP_LENGTH`.\n",
    "2. Displays an **audio player** to listen to the **original signal**.\n",
    "3. **Plots the magnitude spectrum** of a single STFT frame (`FRAME_IDX`) vs. frequency.\n",
    "4. **Overlays a triangular mel-filter bank** (with `N_MELS` filters) on that spectrum.\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Inputs (Edit at the Top of the Code Cell)\n",
    "\n",
    "- **`FILENAME`**  \n",
    "  Name of your audio file in `sounds/` (supports `.wav` or `.mp3`)\n",
    "\n",
    "- **`FRAME_IDX`**  \n",
    "  Integer in `[0 â€¦ n_frames - 1]` selecting the time frame to display\n",
    "\n",
    "- **`N_FFT`**  \n",
    "  FFT window size (power of 2, e.g., `1024`, `2048`)\n",
    "\n",
    "- **`HOP_LENGTH`**  \n",
    "  Hop size between STFT frames (â‰¤ `N_FFT`, e.g., `N_FFT // 4`)\n",
    "\n",
    "- **`N_MELS`**  \n",
    "  Number of mel filters (e.g., 20 to 80)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¤ Outputs to Observe\n",
    "\n",
    "### ğŸ§ Audio Player\n",
    "- Listen to the **original recording**\n",
    "\n",
    "### ğŸ“ˆ Spectrum Plot\n",
    "- **Blue curve**: Magnitude spectrum \\|STFT\\| at frame `FRAME_IDX`\n",
    "- **Semi-transparent lines**: Each of the `N_MELS` **mel filters** overlaid on the spectrum\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  How to Interpret\n",
    "\n",
    "### ğŸ¼ Low Frequencies\n",
    "- Mel filters are **narrower** and **densely packed**  \n",
    "  â†’ Yields **higher resolution**\n",
    "\n",
    "### ğŸ¼ High Frequencies\n",
    "- Mel filters become **wider** and **sparser**  \n",
    "  â†’ Lower resolution, matching human perception\n",
    "\n",
    "### ğŸšï¸ Changing `N_MELS`\n",
    "- **More filters** â†’ Finer resolution (especially in low frequencies), but **more overlap**\n",
    "- **Fewer filters** â†’ Coarser band coverage\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ” **Try editing `N_MELS`** in the code and **re-running the cell** to see how the **mel filter bank adapts** to match **human auditory perception**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bb2ebe-76e1-4883-8157-0764962d3817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ USER SETTINGS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FILENAME    = 'speech.WAV'   # â† place your audio file in `sounds/`\n",
    "FRAME_IDX   = 0              # â† which STFT frame to visualize (0â€¦n_frames-1)\n",
    "N_FFT       = 2048           # â† FFT window size (power of 2)\n",
    "HOP_LENGTH  = 512            # â† hop length between frames\n",
    "N_MELS      = 40             # â† number of mel filters (e.g. 20â€¦80)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "from pathlib import Path\n",
    "\n",
    "# â”€â”€ CONFIG (donâ€™t edit below here) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SOUNDS_DIR = Path('sounds')\n",
    "audio_path = SOUNDS_DIR / FILENAME\n",
    "\n",
    "# 1) Load audio and compute STFT magnitudes\n",
    "y, sr = librosa.load(str(audio_path), sr=None)\n",
    "D     = np.abs(librosa.stft(y, n_fft=N_FFT, hop_length=HOP_LENGTH))\n",
    "freqs = librosa.fft_frequencies(sr=sr, n_fft=N_FFT)\n",
    "n_frames = D.shape[1]\n",
    "\n",
    "# 2) Listen to the original audio\n",
    "print(\"â–¶ï¸ Original Audio\")\n",
    "display(Audio(data=y, rate=sr, autoplay=False))\n",
    "\n",
    "# 3) Compute mel filter bank\n",
    "mel_fb = librosa.filters.mel(\n",
    "    sr=sr,\n",
    "    n_fft=N_FFT,\n",
    "    n_mels=N_MELS\n",
    ")\n",
    "\n",
    "# 4) Pick and plot the chosen frame\n",
    "mag_frame = D[:, FRAME_IDX]\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(freqs, mag_frame, label=f'|STFT|, frame {FRAME_IDX}', color='C0')\n",
    "for i in range(N_MELS):\n",
    "    plt.plot(freqs, mel_fb[i], alpha=0.5, linewidth=1)\n",
    "plt.title(f'Mel Filter Bank Overlay (n_mels = {N_MELS})')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af861d6-4312-48fd-969e-8aa43605e544",
   "metadata": {},
   "source": [
    "# ğŸ›ï¸ Demo: MFCC Reconstruction Comparison\n",
    "\n",
    "In this demo, youâ€™ll explore how **retaining more (or fewer) cepstral coefficients** affects the **quality of a reconstructed audio signal**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” What the Code Does\n",
    "\n",
    "1. **Loads your audio clip**.\n",
    "2. Computes **MFCCs** using a **mel filter bank** of `N_MELS` bands and `N_FFT` FFT size:\n",
    "   - **Short MFCC**: Keep only the first `N_MFCC` coefficients per frame  \n",
    "     â†’ Coarse spectral envelope\n",
    "   - **Full MFCC**: Keep all `N_MELS` coefficients  \n",
    "     â†’ Retains full log-spectrum detail\n",
    "3. **Inverts** each MFCC set back to time domain via  \n",
    "   ```python\n",
    "   librosa.feature.inverse.mfcc_to_audio\n",
    "```\n",
    "*(Uses **Griffinâ€“Lim algorithm** under the hood)*\n",
    "\n",
    "---\n",
    "\n",
    "### â–¶ï¸ 4. Plays Back Three Versions\n",
    "\n",
    "- **Original**  \n",
    "- **Reconstructed from first `N_MFCC` coefficients**  \n",
    "- **Reconstructed from all `N_MELS` coefficients**\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Inputs (Edit at Top of the Code Cell)\n",
    "\n",
    "- **`FILENAME`**  \n",
    "  Your input file in the `sounds/` folder (`.wav` or `.mp3`)\n",
    "\n",
    "- **`N_MELS`** (e.g., 20â€“80)  \n",
    "  Number of **mel bands** for the filter bank\n",
    "\n",
    "- **`N_MFCC`** (â‰¤ `N_MELS`, e.g., 8â€“20)  \n",
    "  Number of **cepstral coefficients** to keep in the â€œshortâ€ version\n",
    "\n",
    "- **`N_FFT`** (power of 2, e.g., 512, 1024, 2048)  \n",
    "  **FFT window length**\n",
    "\n",
    "- **`HOP_LENGTH`** (â‰¤ `N_FFT`, typically `N_FFT // 4`)  \n",
    "  **Frame shift** between successive STFT frames\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¤ Outputs to Observe\n",
    "\n",
    "### ğŸ§ Audio Players\n",
    "\n",
    "- **Original**: Your untouched audio clip  \n",
    "- **Short MFCC**: Captures **overall timbre** but may lose **transient fine-structure**  \n",
    "- **Full MFCC**: Closer to the original, preserving more **spectral detail**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  What to Listen For\n",
    "\n",
    "### ğŸ¼ Envelope vs. Detail\n",
    "- Does the **â€œshortâ€ MFCC version** sound **muffled** or **smeared**?\n",
    "\n",
    "### ğŸŒ€ Artifacts\n",
    "- The **Griffinâ€“Lim reconstruction** may introduce **phasiness** â€” how noticeable is it?\n",
    "\n",
    "### ğŸ“‰ Coefficient Count Impact\n",
    "- How many **MFCCs** are needed to **approach the original quality**?\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª Experiment\n",
    "\n",
    "Try varying `N_MFCC` (e.g., **5 â†’ 13 â†’ 40**) and observe how **reconstruction quality improves** as you include more **cepstral detail**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f91e914-9842-4aba-9882-e560d72087b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ USER SETTINGS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FILENAME    = 'drum_hit3.wav'  # â† place your file in `sounds/` (WAV or MP3)\n",
    "N_MELS      = 40                     # â† number of mel bands used in MFCC\n",
    "N_MFCC      = 13                     # â† number of MFCC coefficients to keep\n",
    "N_FFT       = 2048                   # â† FFT window size (power of 2)\n",
    "HOP_LENGTH  = N_FFT // 4             # â† hop length between frames\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "from pathlib import Path\n",
    "\n",
    "# â”€â”€ CONFIG (donâ€™t edit below here) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SOUNDS_DIR = Path('sounds')\n",
    "audio_path = SOUNDS_DIR / FILENAME\n",
    "\n",
    "# 1) Load audio\n",
    "y, sr = librosa.load(str(audio_path), sr=None)\n",
    "\n",
    "# 2) Compute MFCCs\n",
    "#  - 'short' MFCCs: only first N_MFCC coefficients\n",
    "mfcc_short = librosa.feature.mfcc(\n",
    "    y=y, sr=sr,\n",
    "    n_mfcc=N_MFCC,\n",
    "    n_mels=N_MELS,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP_LENGTH\n",
    ")\n",
    "#  - 'full' MFCCs: same number of bands as mel filters\n",
    "mfcc_full = librosa.feature.mfcc(\n",
    "    y=y, sr=sr,\n",
    "    n_mfcc=N_MELS,\n",
    "    n_mels=N_MELS,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP_LENGTH\n",
    ")\n",
    "\n",
    "# 3) Reconstruct audio from MFCCs via inverse transform + Griffin-Lim\n",
    "y_rec_short = librosa.feature.inverse.mfcc_to_audio(\n",
    "    mfcc_short,\n",
    "    sr=sr,\n",
    "    n_mels=N_MELS,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP_LENGTH\n",
    ")\n",
    "y_rec_full = librosa.feature.inverse.mfcc_to_audio(\n",
    "    mfcc_full,\n",
    "    sr=sr,\n",
    "    n_mels=N_MELS,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP_LENGTH\n",
    ")\n",
    "\n",
    "# 4) Playback: Original vs. Short MFCC vs. Full MFCC\n",
    "print(\"â–¶ï¸ Original Audio\")\n",
    "display(Audio(data=y,          rate=sr, autoplay=False))\n",
    "print(f\"â–¶ï¸ Reconstructed from first {N_MFCC} MFCCs\")\n",
    "display(Audio(data=y_rec_short, rate=sr, autoplay=False))\n",
    "print(f\"â–¶ï¸ Reconstructed from all {N_MELS} MFCCs\")\n",
    "display(Audio(data=y_rec_full,  rate=sr, autoplay=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37999425-cfd1-450f-ac45-0ec657790748",
   "metadata": {},
   "source": [
    "# ğŸ›ï¸ Demo: Î” & Î”Î” (Temporal Derivatives) Feature Visualization\n",
    "\n",
    "In this demo, youâ€™ll compute the **static MFCCs**, their **first (Î”)** and **second (Î”Î”)** derivatives, and **plot the trajectories** of the first few coefficients over time. This helps visualize how **dynamics in the cepstral domain** capture **onsets and transitions** in audio.\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ User-Configurable Inputs  \n",
    "*(Edit at the top of the code cell)*\n",
    "\n",
    "- **`FILENAME`**  \n",
    "  Name of your audio file in the `sounds/` folder (`.wav` or `.mp3`)\n",
    "\n",
    "- **`SR`**  \n",
    "  Sampling rate to use (`None` to use the fileâ€™s native rate)\n",
    "\n",
    "- **`N_MFCC`**  \n",
    "  Number of MFCC coefficients to compute (**â‰¥ 3** recommended)\n",
    "\n",
    "- **`N_FFT`**  \n",
    "  FFT window size (power of two, e.g., `1024`, `2048`)\n",
    "\n",
    "- **`HOP_LENGTH`**  \n",
    "  Hop size between frames (â‰¤ `N_FFT`, often `N_FFT // 4`)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª What the Code Does\n",
    "\n",
    "1. **Loads your clip**  \n",
    "   *(Using `SoundFile` for `.wav`, `librosa` otherwise)*  \n",
    "   Plays the audio\n",
    "\n",
    "2. **Computes**:\n",
    "   - **Static MFCCs** (`n_mfcc = N_MFCC`)\n",
    "   - **Î”-MFCCs** (first derivative)\n",
    "   - **Î”Î”-MFCCs** (second derivative)\n",
    "\n",
    "3. **Plots** the time-series of the **first 3 coefficients**:\n",
    "   - **Solid lines** = Static MFCC\n",
    "   - **Dashed lines** = Î”-MFCC\n",
    "   - **Dotted lines** = Î”Î”-MFCC\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¤ Outputs to Observe\n",
    "\n",
    "### ğŸ§ Audio Player\n",
    "- Verify the clip **loaded and plays correctly**\n",
    "\n",
    "### ğŸ“ˆ Trajectory Plots\n",
    "- **Static MFCCs** show smooth **spectral envelope changes**\n",
    "- **Î” (slope)** peaks at **onsets** or **rapid spectral shifts**\n",
    "- **Î”Î” (acceleration)** highlights **changes in slope** â€” very sensitive to **transient bursts**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  How to Interpret\n",
    "\n",
    "- **Peaks in the Î” curve** correspond to **rhythmic or attack events**\n",
    "- **Sharp spikes in Î”Î”** indicate **rapid changes in dynamics** or **timbre**\n",
    "- Together, **Î” and Î”Î” features** capture **temporal patterns** that **static MFCCs alone cannot**, making them **crucial for tasks** like:\n",
    "  - **Speech/music recognition**\n",
    "  - **Onset detection**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de38e432-fe75-4161-9f2a-b5c7e70d6bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ USER SETTINGS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FILENAME    = 'speech.WAV'   # â† place your audio clip in `sounds/`\n",
    "SR          = None           # â† sampling rate (None to use fileâ€™s native rate)\n",
    "N_MFCC      = 13             # â† number of MFCC coefficients\n",
    "N_FFT       = 2048           # â† FFT window size (power of two)\n",
    "HOP_LENGTH  = N_FFT // 4     # â† hop length between frames\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "from IPython.display import Audio, display\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "\n",
    "# â”€â”€ CONFIG (donâ€™t edit below here) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SOUNDS_DIR = Path('sounds')\n",
    "path = SOUNDS_DIR / FILENAME\n",
    "\n",
    "def load_audio(path, sr=None):\n",
    "    ext = path.suffix.lower()\n",
    "    if ext == '.wav':\n",
    "        y, sr_native = sf.read(str(path), dtype='float32')\n",
    "        return (y, sr_native) if sr is None else (librosa.resample(y, sr_native, sr), sr)\n",
    "    else:\n",
    "        return librosa.load(str(path), sr=sr)\n",
    "\n",
    "# 1) Load audio\n",
    "y, sr = load_audio(path, sr=SR)\n",
    "\n",
    "# 2) Play the clip\n",
    "print(\"â–¶ï¸ Original Audio\")\n",
    "display(Audio(data=y, rate=sr, autoplay=False))\n",
    "\n",
    "# 3) Compute static MFCCs\n",
    "mfcc     = librosa.feature.mfcc(\n",
    "    y=y, sr=sr,\n",
    "    n_mfcc=N_MFCC,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP_LENGTH\n",
    ")\n",
    "\n",
    "# 4) Compute Î” and Î”Î”\n",
    "mfcc_delta  = librosa.feature.delta(mfcc, order=1)\n",
    "mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "\n",
    "# 5) Prepare time axis\n",
    "times = librosa.frames_to_time(\n",
    "    np.arange(mfcc.shape[1]),\n",
    "    sr=sr,\n",
    "    hop_length=HOP_LENGTH\n",
    ")\n",
    "\n",
    "# 6) Plot trajectories for the first 3 coefficients\n",
    "coeffs_to_plot = min(3, N_MFCC)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i in range(coeffs_to_plot):\n",
    "    plt.subplot(coeffs_to_plot, 1, i+1)\n",
    "    plt.plot(times, mfcc[i],       label=f'MFCC {i+1}',    linewidth=1.5)\n",
    "    plt.plot(times, mfcc_delta[i],  '--', label='Î”-MFCC',    linewidth=1.2)\n",
    "    plt.plot(times, mfcc_delta2[i], ':',  label='Î”Î”-MFCC',   linewidth=1.2)\n",
    "    plt.title(f'Coefficient {i+1} Trends')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa21500-47e6-4900-a2ac-331369d2f039",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ›  Exercise: Filter-Bank Shape Comparison\n",
    "\n",
    "### ğŸ¯ Task\n",
    "Evaluate how different **auditory filter-bank shapes** â€” **triangular** (Mel) vs. **gammatone** â€” affect a **simple speech-recognition pipeline**.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§¾ Steps\n",
    "\n",
    "1. **Implement both filter types**:\n",
    "   - Triangular **mel filters** (e.g., via `librosa.filters.mel`)\n",
    "   - **Gammatone filterbank** (e.g., using `auditory-toolbox`, `gammatone` library, or custom code)\n",
    "\n",
    "2. **Extract filter-bank features** from a **small spoken-digit dataset**  \n",
    "   *(e.g., Free Spoken Digit Dataset â€” FSDD)*\n",
    "\n",
    "3. **Train a lightweight classifier**  \n",
    "   (e.g., **logistic regression**, **SVM**, or **k-NN**)  \n",
    "   to compare **recognition accuracy** across filter types\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“¦ Deliverables\n",
    "\n",
    "- ğŸ“Š **Plots** of both **filter-bank shapes** overlaid on a representative **magnitude spectrum**\n",
    "- ğŸ§® **Recognition accuracy table** comparing **mel vs. gammatone** features\n",
    "- âœï¸ **Brief discussion**:  \n",
    "  - How does **filter-bank design** influence **feature discriminability**?\n",
    "  - Which shape better captures important cues for digit classification?\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ’¡ *Tip:* Try reducing dimensionality (e.g., via PCA) to visualize feature separability between digits for each filter type!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120bafe5-2d4a-4fcc-a507-702a12e26bca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
