{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40349ffd-c383-4be4-94aa-d0294cec07c5",
   "metadata": {},
   "source": [
    "## Module 12: Advanced Modelling Strategies\n",
    "\n",
    "In this module, we’ll explore cutting-edge sequence and generative models for audio, as well as classical and deep approaches to source separation.\n",
    "\n",
    "### 🔑 Key Concepts\n",
    "- **Sequence Models for Audio**  \n",
    "  - RNNs & LSTMs for modeling temporal dependencies  \n",
    "  - Transformer architectures (self-attention) for long-range context  \n",
    "- **Generative Waveform Models**  \n",
    "  - WaveNet autoregressive synthesis  \n",
    "  - GANs for realistic waveform and spectrogram generation  \n",
    "- **Source Separation**  \n",
    "  - NMF (Non-negative Matrix Factorization) for classical spectral decomposition  \n",
    "  - Deep-learning–based separation (e.g. U-Net, OpenUnmix)\n",
    "\n",
    "---\n",
    "\n",
    "### 📓 Notebook Demos\n",
    "\n",
    "1. **Transformer-Based Speech Enhancement**  \n",
    "   - Add synthetic noise at varying SNRs  \n",
    "   - Use a small Transformer encoder–decoder to clean speech  \n",
    "   - Slider to control input noise level and hear enhanced output  \n",
    "\n",
    "2. **NMF Music/Vocal Separation**  \n",
    "   - Factorize a stereo music clip’s spectrogram via NMF  \n",
    "   - Reconstruct and play back isolated vocal & accompaniment stems  \n",
    "   - Compare to a pre-trained deep separator (e.g. OpenUnmix)\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠 Exercise: RNN-Based Voice Activity Detection\n",
    "- **Task:**  \n",
    "  Build and train a simple RNN/LSTM to detect speech vs. silence frames.  \n",
    "- **Steps:**  \n",
    "  1. Extract short-time features (e.g. MFCC or log-mel) from a speech-silence dataset.  \n",
    "  2. Train an RNN or bidirectional LSTM to output frame-wise speech probability.  \n",
    "  3. Compare its performance (ROC, F₁) against an energy-threshold baseline.  \n",
    "- **Deliverables:**  \n",
    "  - Training code and model checkpoints  \n",
    "  - Evaluation script reporting ROC curve and F₁ score  \n",
    "  - Audio examples showing frame masks from both methods  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d70b89-66a9-4ae7-9d6c-8e39c50b0131",
   "metadata": {},
   "source": [
    "# 🔑 Key Concepts: Sequence Models for Audio\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 RNNs & LSTMs\n",
    "\n",
    "**Recurrent Neural Networks (RNNs)** and their gated variants (**LSTMs**, **GRUs**) process audio as a **time-series**, maintaining a **hidden “state”** that carries information forward through time.\n",
    "\n",
    "- Ideal for **frame-by-frame tasks** like:\n",
    "  - Speech recognition\n",
    "  - Voice-activity detection\n",
    "- Capture **local temporal context**, which is crucial for sequential audio processing\n",
    "\n",
    "---\n",
    "\n",
    "## 🔀 Transformers (Self-Attention)\n",
    "\n",
    "Transformers **replace recurrence** with **multi-headed self-attention**, allowing **each time step** to attend to **all others** in the sequence.\n",
    "\n",
    "- Capture **very long-range dependencies** (e.g., across sentences or musical phrases)\n",
    "- **Parallelizable** computation makes them efficient on modern hardware\n",
    "- Widely used in:\n",
    "  - Audio captioning\n",
    "  - Music modeling\n",
    "  - Large-scale speech models\n",
    "\n",
    "---\n",
    "\n",
    "## 🎵 Generative Waveform Models\n",
    "\n",
    "### 🌀 WaveNet\n",
    "\n",
    "- An **autoregressive CNN** that models **raw audio** one sample at a time\n",
    "- Uses **dilated convolutions** to achieve large receptive fields\n",
    "- Can generate **highly realistic speech or music**\n",
    "- Often **conditioned** on linguistic or musical inputs\n",
    "\n",
    "### 🤖 GANs for Audio\n",
    "\n",
    "**Generative Adversarial Networks (GANs)** trained to produce:\n",
    "- **Waveforms** or\n",
    "- **Spectrograms**\n",
    "\n",
    "that a discriminator cannot distinguish from real audio.\n",
    "\n",
    "Used for tasks like:\n",
    "- Speech enhancement\n",
    "- Music synthesis\n",
    "- Audio style transfer\n",
    "\n",
    "---\n",
    "\n",
    "## 🎚️ Source Separation\n",
    "\n",
    "### 📊 NMF (Non-negative Matrix Factorization)\n",
    "\n",
    "A **classical unsupervised method** that decomposes a spectrogram into:\n",
    "- **Basis spectra**\n",
    "- **Activation patterns**\n",
    "\n",
    "Relies on **non-negativity** to find **additive components** (e.g., separating vocals from accompaniment).\n",
    "\n",
    "---\n",
    "\n",
    "### 🤖 Deep-Learning–Based Separation\n",
    "\n",
    "Modern architectures like:\n",
    "- **U-Nets**\n",
    "- **Temporal Convolutional Networks (TCNs)**\n",
    "- **OpenUnmix**\n",
    "\n",
    "take a **spectrogram or waveform** as input and directly **predict separated sources**.\n",
    "\n",
    "- Trained on **large datasets** of mixed/isolated tracks\n",
    "- Deliver **state-of-the-art performance** in music and speech separation tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e89f25-1de8-48b9-b838-baf50dbc392b",
   "metadata": {},
   "source": [
    "### 🔧 Demo 1: Transformer-Based Speech Enhancement (Overfit on One Clip)\n",
    "\n",
    "In this demo you’ll train a tiny Transformer encoder to remove synthetic Gaussian noise from a single speech WAV file. All inputs are edited at the top of the code cell; there are no extra GUI sliders.\n",
    "\n",
    "**What the code does:**\n",
    "1. **Load** your clean speech WAV (using `soundfile` for compatibility).  \n",
    "2. **Synthesize** a noisy version by adding Gaussian noise of standard deviation `NOISE_LEVEL`.  \n",
    "3. **Compute STFTs** of clean & noisy signals to get magnitude (`mag_*`) and phase.  \n",
    "4. **Build PyTorch tensors** of shape *(T × F)* where T = time frames, F = frequency bins.  \n",
    "5. **Define** a simple Transformer-based enhancer (`Enhancer`):  \n",
    "   - Projects input magnitudes to a `D_MODEL`-dim embedding,  \n",
    "   - Passes through `N_LAYERS` of self-attention with `N_HEADS` heads,  \n",
    "   - Projects back to frequency-bin dimension.  \n",
    "6. **Train** (overfit) for `EPOCHS` epochs on this one example, minimizing MSE between predicted & clean magnitudes.  \n",
    "7. **Enhance & invert** the noisy STFT back to waveform with the original phase.  \n",
    "8. **Play back** the noisy input and enhanced output to judge quality improvements.\n",
    "\n",
    "---\n",
    "\n",
    "**USER SETTINGS** (edit these at the top of the code cell):\n",
    "\n",
    "- `FILENAME_CLEAN`: name of your clean speech WAV file in `sounds/` (must end in `.wav`).  \n",
    "- `NOISE_LEVEL`: noise standard deviation to add (valid range `0.0 … 0.2`).  \n",
    "- `N_FFT`: STFT window size (power of two, e.g. `512, 1024, 2048`).  \n",
    "- `HOP_LENGTH`: hop size between frames (`≤ N_FFT`, e.g. `N_FFT//4`).  \n",
    "- `D_MODEL`: Transformer embedding dimension (e.g. `128, 256, 512`).  \n",
    "- `N_HEADS`: number of attention heads (divisor of `D_MODEL`).  \n",
    "- `N_LAYERS`: number of Transformer encoder layers.  \n",
    "- `LR`: learning rate for Adam (e.g. `1e-3`).  \n",
    "- `EPOCHS`: number of training epochs (e.g. `100–500` for a quick demo).\n",
    "\n",
    "---\n",
    "\n",
    "**Outputs to Observe:**\n",
    "\n",
    "- **Console logs:** Loss printed every 50 epochs—should go down towards zero as model memorizes the clip.  \n",
    "- **Audio players:**  \n",
    "  - ▶️ **Noisy Input**: hear the synthetic noise level.  \n",
    "  - ▶️ **Transformer-Enhanced Output**: listen for noise reduction and any Transformer artifacts.\n",
    "\n",
    "- **Interpretation:**  \n",
    "  - At low `NOISE_LEVEL`, enhancement will almost restore the original waveform.  \n",
    "  - At higher `NOISE_LEVEL`, Transformer may struggle and introduce artifacts.  \n",
    "  - Adjust `D_MODEL`, `N_HEADS`, and `N_LAYERS` to see how model capacity affects denoising quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19987b9-ff9c-4f0b-9a3e-f925ffe4cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── USER SETTINGS ────────────────────────────────────────────────────────────────\n",
    "FILENAME_CLEAN = 'speech.WAV'  # ← your clean speech clip in sounds/ (WAV only)\n",
    "NOISE_LEVEL    = 0.05                # ← noise std-dev (0.0 … 0.2)\n",
    "N_FFT          = 1024                # ← STFT window size (power of two)\n",
    "HOP_LENGTH     = N_FFT // 4          # ← hop size between frames\n",
    "D_MODEL        = 256                 # ← transformer embedding dim\n",
    "N_HEADS        = 4                   # ← number of self-attention heads\n",
    "N_LAYERS       = 2                   # ← number of transformer encoder layers\n",
    "LR             = 1e-3                # ← learning rate\n",
    "EPOCHS         = 200                 # ← training epochs (overfit demo)\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import librosa\n",
    "from IPython.display import Audio, display\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Load clean speech with soundfile for WAV compatibility\n",
    "file_path = Path('sounds') / FILENAME_CLEAN\n",
    "y_clean, sr = sf.read(str(file_path), dtype='float32')\n",
    "# If stereo, take first channel\n",
    "if y_clean.ndim > 1:\n",
    "    y_clean = y_clean[:, 0]\n",
    "\n",
    "# 2) Synthesize noisy signal\n",
    "y_noisy = y_clean + NOISE_LEVEL * np.random.randn(len(y_clean))\n",
    "\n",
    "# 3) Compute STFT magnitudes & phase\n",
    "D_clean     = librosa.stft(y_clean,  n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "mag_clean, phase = np.abs(D_clean), np.angle(D_clean)\n",
    "mag_noisy      = np.abs(librosa.stft(y_noisy, n_fft=N_FFT, hop_length=HOP_LENGTH))\n",
    "\n",
    "# 4) Prepare PyTorch tensors of shape (T, F)\n",
    "mag_noisy_t = torch.from_numpy(mag_noisy.T).float()\n",
    "mag_clean_t = torch.from_numpy(mag_clean.T).float()\n",
    "\n",
    "# 5) Define the simple Transformer-based enhancer\n",
    "class Enhancer(nn.Module):\n",
    "    def __init__(self, feat_dim, d_model, nhead, nlayers):\n",
    "        super().__init__()\n",
    "        self.input_proj  = nn.Linear(feat_dim, d_model)\n",
    "        encoder_layer   = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.encoder     = nn.TransformerEncoder(encoder_layer, num_layers=nlayers)\n",
    "        self.output_proj = nn.Linear(d_model, feat_dim)\n",
    "    def forward(self, x):\n",
    "        # x shape: (T, F)\n",
    "        z = self.input_proj(x)           # → (T, d_model)\n",
    "        z = self.encoder(z)              # → (T, d_model)\n",
    "        return self.output_proj(z)       # → (T, F)\n",
    "\n",
    "model     = Enhancer(mag_noisy_t.shape[1], D_MODEL, N_HEADS, N_LAYERS)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 6) Train (overfit) on this single clip\n",
    "model.train()\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(mag_noisy_t)\n",
    "    loss = criterion(pred, mag_clean_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}/{EPOCHS}  Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 7) Enhance & invert back to waveform\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    enhanced_mag = model(mag_noisy_t).cpu().numpy().T  # shape (F, T)\n",
    "\n",
    "D_enh = enhanced_mag * np.exp(1j * phase)\n",
    "y_enh = librosa.istft(D_enh, hop_length=HOP_LENGTH)\n",
    "\n",
    "# 8) Playback\n",
    "print(\"▶️ Noisy Input\")\n",
    "display(Audio(y_noisy, rate=sr))\n",
    "print(\"▶️ Transformer-Enhanced Output\")\n",
    "display(Audio(y_enh,   rate=sr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc62efc-e77e-47c5-9518-a75538a33a6d",
   "metadata": {},
   "source": [
    "# 🎶 Demo 2: NMF-Based Music/Vocal Separation\n",
    "\n",
    "In this demo, you’ll split a **stereo music clip** into two **stems** (e.g., vocals vs. accompaniment) using **Non-negative Matrix Factorization (NMF)**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 What the Code Does\n",
    "\n",
    "1. **Loads your stereo music file**  \n",
    "   → Automatically converted to **mono**\n",
    "\n",
    "2. **Computes the magnitude spectrogram** \\( D \\) via **STFT** using `N_FFT` and `HOP_LENGTH`\n",
    "\n",
    "3. **Factorizes** \\( D \\approx WH \\) using **NMF** with rank `N_COMPONENTS`\n",
    "   - \\( W \\): shape = *(frequency bins × components)*\n",
    "   - \\( H \\): shape = *(components × time frames)*\n",
    "\n",
    "4. **Splits the components in half**:\n",
    "   - Let \\( K = \\left\\lfloor \\frac{\\text{N_COMPONENTS}}{2} \\right\\rfloor \\)\n",
    "   - **Stem 1**: Reconstructed from the **first K** components\n",
    "   - **Stem 2**: Reconstructed from the **remaining components**\n",
    "\n",
    "5. **Reconstructs time-domain signals** for both stems  \n",
    "   → Uses the **original phase** for inverse STFT\n",
    "\n",
    "6. **Plays back**:\n",
    "   - The **original mix**\n",
    "   - **Stem 1**\n",
    "   - **Stem 2**\n",
    "\n",
    "7. **Plots** side-by-side:\n",
    "   - Original magnitude spectrogram\n",
    "   - Stem 1 spectrogram\n",
    "   - Stem 2 spectrogram\n",
    "\n",
    "📝 *Note: We’ve dropped the “compare to OpenUnmix” step for simplicity.*\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Inputs (Edit at the Top)\n",
    "\n",
    "- **`FILENAME_MUSIC`**  \n",
    "  Your stereo music clip in `sounds/` (`.wav` or `.mp3`)\n",
    "\n",
    "- **`N_FFT`**  \n",
    "  STFT window size (power of 2, e.g., `1024`, `2048`)\n",
    "\n",
    "- **`HOP_LENGTH`**  \n",
    "  Hop size between frames (≤ `N_FFT`, e.g., `N_FFT // 4` or `512`)\n",
    "\n",
    "- **`N_COMPONENTS`**  \n",
    "  NMF rank (**integer ≥ 2**)  \n",
    "  Typical range: **2–16**\n",
    "\n",
    "---\n",
    "\n",
    "## 📤 Outputs to Observe\n",
    "\n",
    "### 🎧 Audio Players\n",
    "\n",
    "- ▶️ **Original Mix**\n",
    "- ▶️ **Stem 1** (components `1…K`)\n",
    "- ▶️ **Stem 2** (components `K+1…N_COMPONENTS`)\n",
    "\n",
    "---\n",
    "\n",
    "### 📈 Spectrograms\n",
    "\n",
    "- Original **magnitude spectrogram**\n",
    "- **Stem 1** spectrogram\n",
    "- **Stem 2** spectrogram\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 How to Interpret\n",
    "\n",
    "### 🎧 Listening Test\n",
    "- Does **Stem 1** emphasize **vocals** (or lead instruments)?\n",
    "- Does **Stem 2** retain **rhythm/harmony**?\n",
    "- Listen for any **“bleed-through” artifacts**\n",
    "\n",
    "### 📊 Spectrogram Comparison\n",
    "- Stem spectrograms reveal **which frequency regions** each group of components captures\n",
    "- Clearer separation usually occurs when **components align with distinct timbral structures**\n",
    "\n",
    "---\n",
    "\n",
    "🔧 **Try this**:  \n",
    "Vary `N_COMPONENTS` (and thus \\( K \\)) to explore how the **model rank** affects separation quality!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb0a3d-7563-4bcc-8bd9-b7aacc6e7471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── USER SETTINGS ────────────────────────────────────────────────────────────────\n",
    "FILENAME_MUSIC = 'song-2-302326.mp3'     # ← stereo music clip in sounds/\n",
    "N_FFT          = 2048                # ← STFT window size\n",
    "HOP_LENGTH     = 512                 # ← hop size between frames\n",
    "N_COMPONENTS   = 8                   # ← NMF rank (e.g. 2–16)\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import NMF\n",
    "from IPython.display import Audio, display\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Load stereo music, convert to mono\n",
    "y, sr = librosa.load(str(Path('sounds')/FILENAME_MUSIC), sr=None, mono=True)\n",
    "\n",
    "# 2) Compute magnitude spectrogram\n",
    "D = np.abs(librosa.stft(y, n_fft=N_FFT, hop_length=HOP_LENGTH))\n",
    "\n",
    "# 3) Fit NMF to magnitude\n",
    "model = NMF(n_components=N_COMPONENTS, init='random', random_state=0, max_iter=200)\n",
    "W = model.fit_transform(D)   # (F, K)\n",
    "H = model.components_        # (K, T)\n",
    "\n",
    "# 4) Assign half of the components to “vocals” and rest to “accompaniment”\n",
    "#    (for a simple 2-stem split you might use N_COMPONENTS=2)\n",
    "#    Here we take first K//2 → stem1, rest → stem2\n",
    "K = N_COMPONENTS//2\n",
    "D1 = W[:,:K] @ H[:K,:]\n",
    "D2 = W[:,K:] @ H[K:,:]\n",
    "\n",
    "# 5) Reconstruct time-domain signals (use noisy phase)\n",
    "y1 = librosa.istft(D1 * np.exp(1j*np.angle(librosa.stft(y, n_fft=N_FFT, hop_length=HOP_LENGTH))),\n",
    "                   hop_length=HOP_LENGTH)\n",
    "y2 = librosa.istft(D2 * np.exp(1j*np.angle(librosa.stft(y, n_fft=N_FFT, hop_length=HOP_LENGTH))),\n",
    "                   hop_length=HOP_LENGTH)\n",
    "\n",
    "# 6) Playback and plot\n",
    "print(\"▶️ Original Mix\")\n",
    "display(Audio(y, rate=sr))\n",
    "print(\"▶️ Stem 1 (components 1…{})\".format(K))\n",
    "display(Audio(y1, rate=sr))\n",
    "print(\"▶️ Stem 2 (components {}…{})\".format(K+1, N_COMPONENTS))\n",
    "display(Audio(y2, rate=sr))\n",
    "\n",
    "# Plot spectrograms for comparison\n",
    "fig, axs = plt.subplots(1,3, figsize=(15,4))\n",
    "for ax, spec, title in zip(\n",
    "    axs,\n",
    "    [D, D1, D2],\n",
    "    ['Original Magnitude','Stem 1 Mag','Stem 2 Mag']\n",
    "):\n",
    "    img = librosa.amplitude_to_db(spec, ref=np.max)\n",
    "    librosa.display.specshow(img, sr=sr, hop_length=HOP_LENGTH,\n",
    "                             x_axis='time', y_axis='log', ax=ax)\n",
    "    ax.set_title(title)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb8e740-0c2f-4b2c-8bb5-965fdcf1c8e7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🛠 Exercise: RNN-Based Voice Activity Detection\n",
    "\n",
    "### 🎯 Task  \n",
    "Build and train a simple **RNN/LSTM** to detect **speech vs. silence** on a **frame-by-frame** basis.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧾 Steps\n",
    "\n",
    "#### 📥 Feature Extraction\n",
    "1. Load a **labeled speech–silence dataset**  \n",
    "   *(e.g., audio clips with annotated speech segments)*\n",
    "\n",
    "2. Compute **short-time features**:\n",
    "   - MFCCs  \n",
    "   - Or **log-mel spectrograms**  \n",
    "   For **overlapping windows**\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧠 Model Training\n",
    "3. Define an **RNN** or **Bidirectional LSTM**:\n",
    "   - Input: each frame’s feature vector\n",
    "   - Output: probability of **speech**\n",
    "\n",
    "4. Split data into **train/validation** sets (e.g., 80/20)\n",
    "\n",
    "5. Train using:\n",
    "   - **Binary cross-entropy loss**\n",
    "   - Track **frame-level accuracy** and **loss**\n",
    "\n",
    "---\n",
    "\n",
    "#### ⚖️ Baseline Comparison\n",
    "6. Implement a simple **energy-threshold detector**:\n",
    "   - Classify a frame as speech if **RMS energy > threshold**\n",
    "\n",
    "7. Tune the threshold on the **validation set**\n",
    "\n",
    "---\n",
    "\n",
    "### 📈 Evaluation\n",
    "\n",
    "- Compute:\n",
    "  - **ROC curve** and **AUC**\n",
    "  - **Precision / Recall**\n",
    "  - **F₁ score**\n",
    "  - For both:\n",
    "    - The **RNN model**\n",
    "    - The **energy baseline**\n",
    "\n",
    "- Plot:\n",
    "  - Frame-wise **speech probability** vs. **ground truth**\n",
    "  - Overlay **detected speech/non-speech regions** on waveform\n",
    "\n",
    "---\n",
    "\n",
    "### 📦 Deliverables\n",
    "\n",
    "- ✅ **Well-documented training script**  \n",
    "  + Saved **model checkpoint(s)**\n",
    "\n",
    "- 📊 **Evaluation notebook or script** that outputs:\n",
    "  - ROC curves\n",
    "  - Confusion matrices\n",
    "  - F₁ scores for both methods\n",
    "\n",
    "- 🎧 **Audio playback examples**  \n",
    "  Showing the **predicted speech mask** vs. the **baseline mask**\n",
    "\n",
    "---\n",
    "\n",
    "💻 **Tools**\n",
    "- Python + TensorFlow or PyTorch\n",
    "- `librosa` for feature extraction\n",
    "- `matplotlib` / `seaborn` for plotting\n",
    "- `scikit-learn` for metrics and ROC computation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd577a7e-8c25-4f1a-ac96-92c8c2d9458d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
