{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e491fd9e-6ed1-46a1-b110-3a1394fdab6f",
   "metadata": {},
   "source": [
    "# Module 2: Time-Domain Analysis\n",
    "\n",
    "### Concepts\n",
    "\n",
    "This module dives into the analysis of audio signals in the time domain, which involves studying how the signal behaves over time. Key concepts covered in this module are:\n",
    "\n",
    "#### Amplitude Envelope (Attack, Decay, Sustain, Release):\n",
    "Understanding how the amplitude of a signal changes over time, which is essential for describing the envelope of musical sounds, such as how a note is played, sustained, and then released.\n",
    "\n",
    "#### Zero-Crossing Rate & RMS Energy:\n",
    "\n",
    "- **Zero-Crossing Rate (ZCR):** Measures how often a signal crosses the zero axis, which is commonly used for distinguishing between different types of sounds, such as speech or noise.\n",
    "- **Root Mean Square (RMS) Energy:** Helps quantify the power or loudness of a signal over time, useful in detecting the intensity of sound signals.\n",
    "\n",
    "#### Autocorrelation for Pitch Detection:\n",
    "Autocorrelation is a technique used for pitch detection, especially in speech and music analysis. It is used to identify periodicity within a signal by measuring its correlation with itself over time.\n",
    "\n",
    "---\n",
    "\n",
    "### Notebook Demos\n",
    "\n",
    "The notebook demos will allow the users to visualize and experiment with these concepts.\n",
    "\n",
    "#### Compute and Plot the Envelope of a Drum Hit:\n",
    "\n",
    "- **Goal:** Visualize the attack, decay, sustain, and release stages of a drum hit.\n",
    "- **User Action:** Users can use a slider to adjust the window size and observe how the envelope changes.\n",
    "\n",
    "#### Listen to Segments with High vs. Low Zero-Crossing Rate (ZCR):\n",
    "\n",
    "- **Goal:** Compare segments of audio that have different ZCR values.\n",
    "- **User Action:** Users will hear the difference between sounds with high ZCR (e.g., noise, cymbals) vs. low ZCR (e.g., speech, sustained notes).\n",
    "\n",
    "#### Pitch Detection: Compare Autocorrelation vs. librosa.pyin on a Sustained Note:\n",
    "\n",
    "- **Goal:** Understand how pitch detection works using two methods: autocorrelation and the `librosa.pyin` method, which is specifically designed for pitch tracking in musical signals.\n",
    "- **User Action:** Users will compare the results of pitch detection using both methods on a sustained note.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise\n",
    "\n",
    "#### Build a Simple Voice-Activity Detector Using Energy Thresholding\n",
    "\n",
    "- **Objective:** Create a voice-activity detector (VAD) that can distinguish between periods of speech and silence based on energy thresholding.\n",
    "\n",
    "The VAD will use energy levels to detect if speech is present in an audio signal.\n",
    "\n",
    "##### Steps:\n",
    "1. Load an audio clip (e.g., a speech recording).\n",
    "2. Compute the RMS energy for the signal over time.\n",
    "3. Set an energy threshold to distinguish speech from silence.\n",
    "4. Visualize the VAD results by plotting the energy over time and marking the speech segments.\n",
    "\n",
    "This exercise will help users understand how energy-based methods can be applied to detect voice activity in audio recordings.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "Time-domain analysis is critical for audio signal processing, where we look at how signals evolve over time.\n",
    "\n",
    "In this module, you will learn about amplitude envelope, zero-crossing rate, RMS energy, and autocorrelation techniques for pitch detection.\n",
    "\n",
    "The exercises will provide hands-on experience with detecting speech, visualizing energy envelopes, and comparing pitch detection methods.\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps:\n",
    "For Module 2, follow the notebook demos and exercises to explore time-domain features like amplitude envelopes, ZCR, RMS energy, and autocorrelation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9c87d4-e844-49ee-9b1e-8cf087dfa03d",
   "metadata": {},
   "source": [
    "## Demo: Interactive Amplitude Envelope Explorer\n",
    "\n",
    "In this demo you will:\n",
    "\n",
    "1. **Choose a sound**  \n",
    "   Use the dropdown menu to select one of three drum-hit samples.\n",
    "\n",
    "2. **Play the sound**  \n",
    "   Click the **â–¶ï¸ Play** button to listen to the selected audio clip.\n",
    "\n",
    "3. **Visualize its envelope**  \n",
    "   Click the **ðŸ“ˆ Plot Envelope** button to compute and display its RMS amplitude envelope (with a fixed window size of 2048 samples).  \n",
    "\n",
    "Feel free to switch sounds at any time and replay or re-plot to see how each drum hitâ€™s attack, decay and release differ in the time domain!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfa7d75-4132-4a11-926f-5d805213d361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Audio, display, clear_output\n",
    "from pathlib import Path\n",
    "\n",
    "# --- configuration -----------------------------------\n",
    "SOUNDS_DIR = Path('sounds')\n",
    "FILES = {\n",
    "    'Drum Hit 1': 'drum_hit1.wav',\n",
    "    'Drum Hit 2': 'drum_hit2.wav',\n",
    "    'Drum Hit 3': 'drum_hit3.wav'\n",
    "}\n",
    "\n",
    "WINDOW_SIZE = 2048  # you can expose this later as a slider if you like\n",
    "\n",
    "# --- widgets -----------------------------------------\n",
    "sound_dropdown = widgets.Dropdown(\n",
    "    options=list(FILES.keys()),\n",
    "    value='Drum Hit 1',\n",
    "    description='Sound:'\n",
    ")\n",
    "\n",
    "play_button = widgets.Button(\n",
    "    description='â–¶ï¸ Play',\n",
    "    button_style='info'\n",
    ")\n",
    "plot_button = widgets.Button(\n",
    "    description='ðŸ“ˆ Plot Envelope',\n",
    "    button_style='primary'\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "# --- helper functions -------------------------------\n",
    "def load_audio(name):\n",
    "    path = SOUNDS_DIR / FILES[name]\n",
    "    return librosa.load(str(path), sr=None)\n",
    "\n",
    "# --- callbacks --------------------------------------\n",
    "def on_play_clicked(_):\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        y, sr = load_audio(sound_dropdown.value)\n",
    "        display(Audio(data=y, rate=sr, autoplay=False))\n",
    "\n",
    "def on_plot_clicked(_):\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        y, sr = load_audio(sound_dropdown.value)\n",
    "        hop = WINDOW_SIZE // 2\n",
    "        rms = librosa.feature.rms(\n",
    "            y=y,\n",
    "            frame_length=WINDOW_SIZE,\n",
    "            hop_length=hop\n",
    "        )[0]\n",
    "        t = librosa.times_like(rms, sr=sr, hop_length=hop)\n",
    "        plt.figure(figsize=(10, 3))\n",
    "        plt.plot(t, rms, alpha=0.8)\n",
    "        plt.title(f\"{sound_dropdown.value} envelope (window={WINDOW_SIZE})\")\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.ylabel(\"RMS energy\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# wire up events\n",
    "play_button.on_click(on_play_clicked)\n",
    "plot_button.on_click(on_plot_clicked)\n",
    "\n",
    "# --- layout -----------------------------------------\n",
    "ui = widgets.VBox([\n",
    "    sound_dropdown,\n",
    "    widgets.HBox([play_button, plot_button]),\n",
    "    output\n",
    "])\n",
    "\n",
    "display(ui)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522399f6-e98a-4cd9-af52-774a7e74f124",
   "metadata": {},
   "source": [
    "## Demo: Zero-Crossing Rate (ZCR) Comparison\n",
    "\n",
    "In this demo you will:\n",
    "\n",
    "1. **Choose a sound**  \n",
    "   Select either a low-ZCR example (speech) or a high-ZCR example (noise/cymbal) from the dropdown.\n",
    "\n",
    "2. **Play the sound**  \n",
    "   Click the **â–¶ï¸ Play** button to listen to the selected clip.\n",
    "\n",
    "3. **Plot its ZCR**  \n",
    "   Click the **ðŸ“ˆ Plot ZCR** button to compute and display the frame-wise zero-crossing rate over time, showing how often the waveform crosses the zero axis in each frame.\n",
    "\n",
    "Explore how speech (low ZCR) and noise (high ZCR) differ in their time-domain behavior!  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc872cb-83d4-442c-8a06-9e3aeb2e43ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Audio, display, clear_output\n",
    "from pathlib import Path\n",
    "\n",
    "# --- configuration -----------------------------------\n",
    "SOUNDS_DIR = Path('sounds')\n",
    "FILES = {\n",
    "    'Low ZCR (low_noise.mp3)':  'low_noise.mp3',\n",
    "    'High ZCR (high-zcore.mp3)': 'high-zcore.mp3'\n",
    "}\n",
    "\n",
    "FRAME_LENGTH = 2048\n",
    "HOP_LENGTH   = FRAME_LENGTH // 2\n",
    "\n",
    "# --- widgets -----------------------------------------\n",
    "sound_dropdown = widgets.Dropdown(\n",
    "    options=list(FILES.keys()),\n",
    "    value='Low ZCR (low_noise.mp3)',\n",
    "    description='Sound:'\n",
    ")\n",
    "\n",
    "play_button = widgets.Button(description='â–¶ï¸ Play',    button_style='info')\n",
    "plot_button = widgets.Button(description='ðŸ“ˆ Plot ZCR', button_style='primary')\n",
    "output      = widgets.Output()\n",
    "\n",
    "# --- helper functions -------------------------------\n",
    "def load_audio(name):\n",
    "    \"\"\"Read WAV via soundfile or MP3 via librosa.\"\"\"\n",
    "    path = SOUNDS_DIR / FILES[name]\n",
    "    ext  = path.suffix.lower()\n",
    "    if ext == '.wav':\n",
    "        y, sr = sf.read(str(path), dtype='float32')\n",
    "    else:\n",
    "        y, sr = librosa.load(str(path), sr=None)\n",
    "    # if stereo, take left channel\n",
    "    if y.ndim > 1:\n",
    "        y = y[:,0]\n",
    "    return y, sr\n",
    "\n",
    "# --- callbacks --------------------------------------\n",
    "def on_play_clicked(_):\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        y, sr = load_audio(sound_dropdown.value)\n",
    "        display(Audio(data=y, rate=sr, autoplay=False))\n",
    "\n",
    "def on_plot_clicked(_):\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        y, sr = load_audio(sound_dropdown.value)\n",
    "        zcr = librosa.feature.zero_crossing_rate(\n",
    "            y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH\n",
    "        )[0]\n",
    "        t = librosa.frames_to_time(np.arange(len(zcr)), sr=sr, hop_length=HOP_LENGTH)\n",
    "        plt.figure(figsize=(10, 3))\n",
    "        plt.plot(t, zcr, alpha=0.8)\n",
    "        plt.title(f\"{sound_dropdown.value} â€” Zero-Crossing Rate\")\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.ylabel(\"ZCR\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# --- wire up events -------------------------------\n",
    "play_button.on_click(on_play_clicked)\n",
    "plot_button.on_click(on_plot_clicked)\n",
    "\n",
    "# --- layout -----------------------------------------\n",
    "ui = widgets.VBox([\n",
    "    sound_dropdown,\n",
    "    widgets.HBox([play_button, plot_button]),\n",
    "    output\n",
    "])\n",
    "display(ui)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d25dd8f-1d53-4efe-b62b-a75112e9bc92",
   "metadata": {},
   "source": [
    "## Pitch Detection Comparison\n",
    "\n",
    "> **Note:** On some machines, clicking **ðŸ“ˆ Plot Pitch** may take **30â€“60 seconds** to complete. Please be patient after you click!\n",
    "\n",
    "In this demo, you'll compare two different pitch-detection methods on a sustained note:\n",
    "\n",
    "- **Autocorrelation**: a simple time-domain method that finds periodicity by correlating the signal with itself.  \n",
    "- **librosa.pyin**: a state-of-the-art probabilistic estimator designed for musical pitch tracking.\n",
    "\n",
    "**Instructions:**\n",
    "1. Use the **Sound** dropdown to select the sustained note.  \n",
    "2. Click **â–¶ï¸ Play** to listen.  \n",
    "3. Click **ðŸ“ˆ Plot Pitch** to visualize both pitch contours over time:  \n",
    "   - **Blue curve** = Autocorrelation  \n",
    "   - **Orange curve** = librosa.pyin  \n",
    "4. Observe where the two methods agree or diverge.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499f0c98-5b0a-4672-9b90-7b78298b2b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Audio, display, clear_output\n",
    "from pathlib import Path\n",
    "\n",
    "# --- configuration -----------------------------------\n",
    "SOUNDS_DIR   = Path('sounds')\n",
    "FILES        = {\n",
    "    'Pitch 1': 'pitch1.mp3',\n",
    "    'Pitch 2': 'pitch2.mp3',\n",
    "    'Pitch 3': 'pitch3.mp3'\n",
    "}\n",
    "FRAME_LENGTH = 2048\n",
    "HOP_LENGTH   = FRAME_LENGTH // 4   # e.g. 512\n",
    "FMIN, FMAX   = 50, 2000             # search range in Hz\n",
    "\n",
    "# --- widgets -----------------------------------------\n",
    "sound_dropdown = widgets.Dropdown(\n",
    "    options=list(FILES.keys()),\n",
    "    value='Pitch 1',\n",
    "    description='Sound:'\n",
    ")\n",
    "play_button = widgets.Button(description='â–¶ï¸ Play',       button_style='info')\n",
    "plot_button = widgets.Button(description='ðŸ“ˆ Plot Pitch', button_style='primary')\n",
    "output      = widgets.Output()\n",
    "\n",
    "# --- helper functions -------------------------------\n",
    "def load_audio(name):\n",
    "    path = SOUNDS_DIR / FILES[name]\n",
    "    y, sr = librosa.load(str(path), sr=None)\n",
    "    return y, sr\n",
    "\n",
    "def pitch_autocorr(y, sr, frame_length, hop_length, fmin, fmax):\n",
    "    # Pad and frame\n",
    "    y_pad = np.pad(y, (frame_length//2, frame_length//2), mode='reflect')\n",
    "    frames = librosa.util.frame(y_pad, frame_length=frame_length, hop_length=hop_length).T\n",
    "    pitches = []\n",
    "    for frame in frames:\n",
    "        frame = frame - np.mean(frame)\n",
    "        ac = librosa.autocorrelate(frame, max_size=frame_length)\n",
    "        ac[:1] = 0\n",
    "        i_min = int(sr / fmax)\n",
    "        i_max = min(int(sr / fmin), len(ac))\n",
    "        lag   = np.argmax(ac[i_min:i_max]) + i_min\n",
    "        pitches.append(sr / lag)\n",
    "    times = librosa.frames_to_time(np.arange(len(pitches)), sr=sr, hop_length=hop_length)\n",
    "    return np.array(pitches), times\n",
    "\n",
    "# --- callbacks --------------------------------------\n",
    "def on_play_clicked(_):\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        y, sr = load_audio(sound_dropdown.value)\n",
    "        display(Audio(data=y, rate=sr, autoplay=False))\n",
    "\n",
    "def on_plot_clicked(_):\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        y, sr = load_audio(sound_dropdown.value)\n",
    "\n",
    "        # Autocorrelation pitch\n",
    "        pitch_ac, t_ac = pitch_autocorr(\n",
    "            y, sr,\n",
    "            FRAME_LENGTH, HOP_LENGTH,\n",
    "            FMIN, FMAX\n",
    "        )\n",
    "\n",
    "        # librosa.pyin pitch\n",
    "        f0_py, voiced_flag, voiced_prob = librosa.pyin(\n",
    "            y, fmin=FMIN, fmax=FMAX, sr=sr,\n",
    "            frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH\n",
    "        )\n",
    "        t_py = librosa.times_like(f0_py, sr=sr, hop_length=HOP_LENGTH)\n",
    "\n",
    "        # Plot both contours\n",
    "        plt.figure(figsize=(10, 3))\n",
    "        plt.plot(t_ac,  pitch_ac, label='Autocorrelation', alpha=0.8)\n",
    "        plt.plot(t_py,  f0_py,    label='librosa.pyin',    alpha=0.8)\n",
    "        plt.title(f\"{sound_dropdown.value} â€” Pitch Detection\")\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.ylabel(\"Frequency (Hz)\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# --- wire up events -------------------------------\n",
    "play_button.on_click(on_play_clicked)\n",
    "plot_button.on_click(on_plot_clicked)\n",
    "\n",
    "# --- initial clear -------------------------------\n",
    "with output:\n",
    "    clear_output()\n",
    "\n",
    "# --- layout -----------------------------------------\n",
    "ui = widgets.VBox([\n",
    "    sound_dropdown,\n",
    "    widgets.HBox([play_button, plot_button]),\n",
    "    output\n",
    "])\n",
    "\n",
    "display(ui)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd19c961-0aa3-4725-9759-1b5ea7909ce5",
   "metadata": {},
   "source": [
    "## ðŸ›  Exercise: Build a Simple Voice-Activity Detector (VAD)\n",
    "\n",
    "**Objective:**  \n",
    "Implement a VAD that distinguishes speech from silence using RMS-energy thresholding.\n",
    "\n",
    "---\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. **Load an audio clip**  \n",
    "   Use `librosa.load()` (or `soundfile.read()`) to load your speech file into a waveform `y` and sample rate `sr`.\n",
    "\n",
    "2. **Compute RMS energy**  \n",
    "   Slice your waveform into overlapping frames (use `frame_length=2048`, `hop_length=512`) and compute  \n",
    "   ```python\n",
    "   rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]\n",
    "   times = librosa.frames_to_time(np.arange(len(rms)), sr=sr, hop_length=hop_length) ### Choose an energy threshold\n",
    "- Experiment with different values (e.g. `mean(rms)`, or a fixed value) to separate â€œspeechâ€ vs. â€œsilence.â€\n",
    "  ```\n",
    "\n",
    "### Detect speech segments\n",
    "- Create a boolean mask:\n",
    "  ```python\n",
    "  speech_mask = rms > threshold\n",
    "  ```\n",
    "\n",
    "  ### Find contiguous speech regions\n",
    "- Identify and group contiguous regions where `speech_mask` is `True`.\n",
    "\n",
    "### Visualize\n",
    "- Plot RMS energy vs. time, overlaying the threshold line.\n",
    "- Shade the speech-active regions using `plt.axvspan(start_time, end_time, alpha=0.3)`.\n",
    "- Play the clip so you can listen and verify your VAD results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f51ffe0-97e4-41e8-bde7-d04b67c58a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Audio, display, clear_output\n",
    "from pathlib import Path\n",
    "\n",
    "# â€” Configuration â€”\n",
    "SOUNDS_DIR    = Path('sounds')\n",
    "AUDIO_FILE    = 'speech.wav'    # replace with your file\n",
    "FRAME_LENGTH  = 2048\n",
    "HOP_LENGTH    = FRAME_LENGTH // 4  # 512\n",
    "\n",
    "# â€” Widgets â€”\n",
    "play_btn      = widgets.Button(description='â–¶ï¸ Play Audio', button_style='info')\n",
    "threshold_sl  = widgets.FloatSlider(\n",
    "    value=0.02, min=0.000, max=0.1, step=0.001,\n",
    "    description='Threshold:'\n",
    ")\n",
    "run_btn       = widgets.Button(description='ðŸ–‹ï¸ Run VAD', button_style='primary')\n",
    "out           = widgets.Output()\n",
    "\n",
    "# â€” Helper to load & play â€”\n",
    "def load_audio():\n",
    "    path = SOUNDS_DIR / AUDIO_FILE\n",
    "    y, sr = librosa.load(str(path), sr=None)\n",
    "    return y, sr\n",
    "\n",
    "def on_play(_):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        y, sr = load_audio()\n",
    "        display(Audio(data=y, rate=sr, autoplay=False))\n",
    "\n",
    "def on_run(_):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        y, sr = load_audio()\n",
    "\n",
    "        # 1) Compute RMS\n",
    "        rms = librosa.feature.rms(\n",
    "            y=y,\n",
    "            frame_length=FRAME_LENGTH,\n",
    "            hop_length=HOP_LENGTH\n",
    "        )[0]\n",
    "        times = librosa.frames_to_time(\n",
    "            np.arange(len(rms)),\n",
    "            sr=sr,\n",
    "            hop_length=HOP_LENGTH\n",
    "        )\n",
    "\n",
    "        # 2) Thresholding\n",
    "        thresh = threshold_sl.value\n",
    "        speech_mask = rms > thresh\n",
    "\n",
    "        # 3) Plot energy + threshold\n",
    "        plt.figure(figsize=(10, 3))\n",
    "        plt.plot(times, rms, label='RMS Energy')\n",
    "        plt.hlines(thresh, times[0], times[-1],\n",
    "                   colors='r', linestyles='--',\n",
    "                   label=f'Threshold = {thresh:.3f}')\n",
    "        # 4) Shade speech regions\n",
    "        in_speech = False\n",
    "        start_t = None\n",
    "        for t, is_sp in zip(times, speech_mask):\n",
    "            if is_sp and not in_speech:\n",
    "                in_speech = True\n",
    "                start_t = t\n",
    "            if not is_sp and in_speech:\n",
    "                in_speech = False\n",
    "                plt.axvspan(start_t, t, color='orange', alpha=0.3)\n",
    "        # cover tail\n",
    "        if in_speech:\n",
    "            plt.axvspan(start_t, times[-1], color='orange', alpha=0.3)\n",
    "\n",
    "        plt.title('Voice Activity Detection')\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('RMS Energy')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# â€” Wire up events â€”\n",
    "play_btn.on_click(on_play)\n",
    "run_btn.on_click(on_run)\n",
    "\n",
    "# â€” Layout â€”\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([play_btn, threshold_sl, run_btn]),\n",
    "    out\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aeec23-d24b-4c7c-8ef1-e189b3383932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
