{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0342af91-22de-4211-89c1-be46b6713044",
   "metadata": {},
   "source": [
    "## Module 9: Statistical & Texture Features\n",
    "\n",
    "In this module we’ll explore higher‐level statistical representations and “texture” descriptors that capture repeating patterns and timbral characteristics beyond frame-based spectra.\n",
    "\n",
    "### 🔑 Key Concepts\n",
    "- **Bag-of-Aural-Words & Chroma Features**  \n",
    "  Represent audio as “codewords” (e.g. spectral clusters) or pitch–class (12-bin) profiles for harmonic analysis.  \n",
    "- **Recurrence Plots & Self-Similarity Matrices**  \n",
    "  Visualize how audio frames repeat or evolve over time, revealing structure (e.g. motifs, rhythm).  \n",
    "- **Texture Descriptors**  \n",
    "  Compute local patterns (e.g. Local Binary Patterns) on spectrogram “images” to characterize textures like rain, applause, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### 📓 Notebook Demos\n",
    "\n",
    "1. **Interactive Chroma Plot & Key Changes**  \n",
    "   - Compute a chromagram for a song  \n",
    "   - Highlight detected chord/key changes with vertical markers  \n",
    "   - Listen to playback while stepping through harmonic transitions  \n",
    "\n",
    "2. **Spectrogram Texture & LBP Histograms**  \n",
    "   - Load two “texture” sounds (e.g. rain vs. applause)  \n",
    "   - Compute spectrogram and apply Local Binary Patterns (LBP) to each time–frequency patch  \n",
    "   - Plot LBP histograms side by side  \n",
    "   - Play both clips to connect timbral texture with their statistical signatures  \n",
    "\n",
    "---\n",
    "\n",
    "### 🛠 Exercise: Texture-Based SVM Classification\n",
    "- **Task:** Extract texture descriptors (e.g. LBP histograms, recurrence-based features) from a set of environmental sounds.  \n",
    "- **Analysis:** Use scikit-learn to train an **SVM classifier** to distinguish classes (rain, applause, traffic, etc.).  \n",
    "- **Deliverables:**  \n",
    "  - Feature extraction code + normalized feature matrix  \n",
    "  - SVM training script + cross-validation accuracy report  \n",
    "  - Confusion matrix and sample audio misclassifications  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42dc286-bb7a-4d20-874c-02a6152b96a0",
   "metadata": {},
   "source": [
    "## 🔑 Key Concepts\n",
    "\n",
    "### 📦 Bag-of-Aural-Words & Chroma Features\n",
    "- **Bag-of-Aural-Words**  \n",
    "  - Treat short audio segments as “words” by clustering spectral or cepstral descriptors.  \n",
    "  - Represent each clip by a histogram of these codewords, analogous to text retrieval.  \n",
    "  - Useful for audio classification and retrieval tasks where timbral textures matter.\n",
    "\n",
    "- **Chroma Features**  \n",
    "  - Collapse the full spectrum into 12 pitch classes (C, C♯, D, …, B), regardless of octave.  \n",
    "  - Capture the harmonic/pitch content of a signal—ideal for key detection, chord recognition, and music similarity.  \n",
    "  - Represented as a 12-dimensional vector per frame, showing energy in each semitone class.\n",
    "\n",
    "### 🔄 Recurrence Plots & Self-Similarity Matrices\n",
    "- **Recurrence Plot**  \n",
    "  - A binary or continuous map showing when a feature vector (e.g. a spectral frame) recurs at later times.  \n",
    "  - Constructed by comparing each frame to every other frame and thresholding similarity.\n",
    "\n",
    "- **Self-Similarity Matrix**  \n",
    "  - A continuous-valued matrix of pairwise similarities (e.g. cosine or Gaussian kernel) between feature vectors over time.  \n",
    "  - Highlights repeating patterns, sections, and structural motifs—diagonal lines indicate sustained similarity, off-diagonal blocks show repeated motifs.\n",
    "\n",
    "> These visualizations help you spot rhythmic patterns, song structure (verse–chorus), or repeated textures in environmental sounds.\n",
    "\n",
    "### **Texture Descriptors**  \n",
    "  - Treat a spectrogram as an image and extract local texture features (e.g. Local Binary Patterns, Gabor filters).  \n",
    "  - **Local Binary Patterns (LBP):** Compare each pixel’s intensity to its neighbors to encode micro-texture “codes.”  \n",
    "  - Capture characteristics of non-stationary textures (e.g. rainfall, applause, engine noise) that are not well described by pitch or harmony.  \n",
    "  - Useful for environmental sound classification and timbral texture analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0166f806-177d-43c2-9b03-4da4d195a60c",
   "metadata": {},
   "source": [
    "# 🎼 Demo: Interactive Chroma Plot & Key/Chord Change Detection\n",
    "\n",
    "In this demo, you’ll compute a **time–pitch representation** (the **chromagram**) of your audio and automatically detect **candidate key or chord changes** by finding peaks in an **onset “novelty” envelope**.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Inputs  \n",
    "*(Edit only in the `USER SETTINGS` block at the top of the cell)*\n",
    "\n",
    "- **`FILENAME`**  \n",
    "  Name of your audio file in the `sounds/` folder (`.wav` or `.mp3`)\n",
    "\n",
    "- **`N_FFT`**  \n",
    "  FFT window size for STFT (must be a power of two, e.g., `1024`, `2048`, `4096`)\n",
    "\n",
    "- **`HOP_LENGTH`**  \n",
    "  Hop size between STFT frames (samples)  \n",
    "  *(typically `N_FFT // 4`)*\n",
    "\n",
    "- **`DETECT_HOP`**  \n",
    "  Hop size for onset-strength computation  \n",
    "  *(usually same as `HOP_LENGTH`)*\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 What the Code Does\n",
    "\n",
    "1. **Loads your audio clip** via `librosa.load`\n",
    "2. Computes a **chromagram `C`** using `librosa.feature.chroma_stft`  \n",
    "   → A 12-row matrix of **pitch-class intensities** over time\n",
    "3. Derives an **onset-strength envelope `nov`** directly from the chromagram  \n",
    "   → Highlights **rapid harmonic changes**\n",
    "4. Detects **peak frames** in `nov` using `librosa.onset.onset_detect`  \n",
    "   → These are **candidate key/chord transitions**\n",
    "5. Converts those frame indices into **times (`change_times`)**\n",
    "6. Plays back the **original audio**\n",
    "7. Plots the **chromagram** as a **time–pitch heatmap**, overlaid with **vertical lines** at `change_times`\n",
    "8. Prints the **numeric timestamps** of each detected change\n",
    "\n",
    "---\n",
    "\n",
    "## 📤 Outputs & Interpretation\n",
    "\n",
    "### 🎧 Audio Player\n",
    "- Listen to the original clip to get familiar with its **harmonic rhythm**\n",
    "\n",
    "### 📈 Chroma Plot\n",
    "- The **12 rows** correspond to the **12 pitch classes**:  \n",
    "  `C, C♯, D, D♯, E, F, F♯, G, G♯, A, A♯, B`\n",
    "- **Brighter colors** = stronger pitch energy at that moment\n",
    "\n",
    "### 🟡 Change Markers\n",
    "- **Yellow dashed vertical lines** = peaks in the chroma onset envelope  \n",
    "  → These often indicate **key or chord changes**\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 How to Interpret\n",
    "\n",
    "- Do the **vertical markers** align with **audible harmonic changes**?\n",
    "- If a chord change is audible **but not marked**:\n",
    "  - Try lowering `HOP_LENGTH` or `DETECT_HOP` (smaller = finer time resolution)\n",
    "  - Or pass `backtrack=True` to `onset_detect` for better alignment\n",
    "\n",
    "- If you see **too many detections**:\n",
    "  - Try **smoothing** the novelty curve\n",
    "  - Or **raise the threshold** via arguments to `onset_detect`\n",
    "\n",
    "---\n",
    "\n",
    "💡 *This approach uses chroma dynamics to infer harmonic shifts — useful for chord recognition, segmentation, or musical structure analysis.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2322efc-aeef-4e4b-acbb-146eb0963eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── USER SETTINGS ────────────────────────────────────────────────────────────────\n",
    "FILENAME    = 'pianos-by-jtwayne-7-174717.mp3'  # ← your audio file in `sounds/` (WAV or MP3)\n",
    "N_FFT       = 4096                             # ← FFT window size for chroma (power of 2)\n",
    "HOP_LENGTH  = 512                              # ← hop length for chroma (samples)\n",
    "DETECT_HOP  = HOP_LENGTH                       # ← hop length for onset detection (usually same)\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "from pathlib import Path\n",
    "\n",
    "# ── CONFIG (don’t edit below here) ───────────────────────────────────────────────\n",
    "SOUNDS_DIR = Path('sounds')\n",
    "audio_path = SOUNDS_DIR / FILENAME\n",
    "\n",
    "# 1) Load audio\n",
    "y, sr = librosa.load(str(audio_path), sr=None)\n",
    "\n",
    "# 2) Compute a STFT‐based chromagram\n",
    "C = librosa.feature.chroma_stft(\n",
    "    y=y,\n",
    "    sr=sr,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    norm=2\n",
    ")\n",
    "\n",
    "# 3) Compute a “novelty” onset envelope from the chromagram\n",
    "nov = librosa.onset.onset_strength(\n",
    "    S=C,\n",
    "    sr=sr,\n",
    "    hop_length=DETECT_HOP\n",
    ")\n",
    "\n",
    "# 4) Detect frames where the chroma‐envelope has peaks → candidate key/chord changes\n",
    "change_frames = librosa.onset.onset_detect(\n",
    "    onset_envelope=nov,\n",
    "    sr=sr,\n",
    "    hop_length=DETECT_HOP,\n",
    "    backtrack=False,\n",
    "    units='frames'\n",
    ")\n",
    "change_times = librosa.frames_to_time(\n",
    "    change_frames,\n",
    "    sr=sr,\n",
    "    hop_length=DETECT_HOP\n",
    ")\n",
    "\n",
    "# 5) Audio playback\n",
    "print(\"▶️ Original Audio\")\n",
    "display(Audio(data=y, rate=sr, autoplay=False))\n",
    "\n",
    "# 6) Plot chromagram and overlay change markers\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.specshow(\n",
    "    C,\n",
    "    sr=sr,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    x_axis='time',\n",
    "    y_axis='chroma',\n",
    "    cmap='coolwarm'\n",
    ")\n",
    "for t in change_times:\n",
    "    plt.axvline(t, color='yellow', linestyle='--', linewidth=1.5, alpha=0.8)\n",
    "plt.title('Chroma STFT + Detected Key/Chord Changes')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.colorbar(label='Chroma intensity')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7) Print change time stamps for reference\n",
    "print(\"Detected change times (s):\")\n",
    "print(np.round(change_times, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b557971d-cb4a-40a1-9eca-59d86be846d8",
   "metadata": {},
   "source": [
    "## Demo: Spectrogram Texture & LBP Histograms\n",
    "\n",
    "In this demo, you’ll compare two “texture” sounds (e.g. rain vs. applause) by extracting **Local Binary Pattern (LBP)** features from their spectrograms.\n",
    "\n",
    "**What the code does:**\n",
    "1. **Loads** two audio clips (`FILENAME1` and `FILENAME2`) from the `sounds/` folder.  \n",
    "2. Computes their **STFT** magnitudes (`N_FFT`, `HOP_LENGTH`) and converts to dB.  \n",
    "3. **Normalizes** each spectrogram to 8-bit range and computes **LBP codes** with parameters:\n",
    "   - `LBP_P`: number of neighbor points  \n",
    "   - `LBP_R`: radius (in pixels)  \n",
    "   - `LBP_METHOD`: LBP variant (`'default'`, `'ror'`, `'uniform'`, etc.)  \n",
    "4. Builds **normalized histograms** of the LBP codes for each texture.  \n",
    "5. **Plays back** both original audio clips.  \n",
    "6. **Plots** side-by-side:\n",
    "   - Top row: log-frequency spectrograms of each clip  \n",
    "   - Bottom row: corresponding LBP code histograms  \n",
    "\n",
    "**Inputs (edit at top of code):**\n",
    "- `FILENAME1`, `FILENAME2`: filenames in `sounds/` (WAV/MP3)  \n",
    "- `N_FFT`: FFT window size (power of 2, e.g. 512, 1024, 2048)  \n",
    "- `HOP_LENGTH`: hop size (≤ `N_FFT`, e.g. `N_FFT//4`)  \n",
    "- `LBP_P`: integer ≥ 1, number of sampling points on the circle  \n",
    "- `LBP_R`: integer ≥ 1, radius of the circle (in spectrogram “pixels”)  \n",
    "- `LBP_METHOD`: one of `'default'`, `'ror'`, `'uniform'`, etc.\n",
    "\n",
    "**Outputs to observe:**\n",
    "- **Audio players**: listen and compare the two textures.  \n",
    "- **Spectrograms**: visually inspect time–frequency energy distributions.  \n",
    "- **LBP histograms**: see how local texture patterns differ statistically.\n",
    "\n",
    "**How to interpret:**\n",
    "- **Spectrogram** differences (e.g., sustained broadband vs. pulsed patterns) map to different **LBP histograms**.  \n",
    "- A texture with more fine-grained, random fluctuations (e.g. rain) will exhibit a broader LBP distribution than a more rhythmic or tonal texture (e.g. applause).  \n",
    "- Use these histograms as compact “texture fingerprints” for classification or visualization tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0317ab-7b6c-49b6-809c-8910d128ac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── USER SETTINGS ────────────────────────────────────────────────────────────────\n",
    "FILENAME1    = 'calming-rain-257596.mp3'       # ← place your first texture clip in `sounds/` (WAV/MP3)\n",
    "FILENAME2    = 'applause-180037.mp3'   # ← place your second texture clip in `sounds/`\n",
    "N_FFT        = 2048            # ← STFT window size (power of 2)\n",
    "HOP_LENGTH   = 512             # ← hop length between frames\n",
    "LBP_P        = 8               # ← number of circularly symmetric neighbor set points\n",
    "LBP_R        = 1               # ← radius of circle (in pixels) for LBP\n",
    "LBP_METHOD   = 'uniform'       # ← LBP method: 'default', 'ror', 'uniform', etc.\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "from skimage.feature import local_binary_pattern\n",
    "from IPython.display import Audio, display\n",
    "from pathlib import Path\n",
    "\n",
    "# ── CONFIG (don’t edit below here) ───────────────────────────────────────────────\n",
    "SOUNDS_DIR = Path('sounds')\n",
    "\n",
    "def load_audio(fname):\n",
    "    path = SOUNDS_DIR / fname\n",
    "    y, sr = librosa.load(str(path), sr=None)\n",
    "    return y, sr\n",
    "\n",
    "# 1) Load both texture clips\n",
    "y1, sr1 = load_audio(FILENAME1)\n",
    "y2, sr2 = load_audio(FILENAME2)\n",
    "\n",
    "# 2) Compute log-magnitude spectrograms\n",
    "D1 = np.abs(librosa.stft(y1, n_fft=N_FFT, hop_length=HOP_LENGTH))\n",
    "S1 = librosa.amplitude_to_db(D1, ref=np.max)\n",
    "D2 = np.abs(librosa.stft(y2, n_fft=N_FFT, hop_length=HOP_LENGTH))\n",
    "S2 = librosa.amplitude_to_db(D2, ref=np.max)\n",
    "\n",
    "# 3) Compute LBP codes over the spectrograms\n",
    "#    Normalize to [0, 255] and cast to uint8 for LBP compatibility\n",
    "img1 = ((S1 - S1.min()) / (S1.max() - S1.min()) * 255).astype(np.uint8)\n",
    "img2 = ((S2 - S2.min()) / (S2.max() - S2.min()) * 255).astype(np.uint8)\n",
    "\n",
    "lbp1 = local_binary_pattern(img1, P=LBP_P, R=LBP_R, method=LBP_METHOD)\n",
    "lbp2 = local_binary_pattern(img2, P=LBP_P, R=LBP_R, method=LBP_METHOD)\n",
    "\n",
    "# 4) Build histograms of LBP codes\n",
    "n_bins = int(lbp1.max() + 1)\n",
    "hist1, _ = np.histogram(lbp1.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
    "hist2, _ = np.histogram(lbp2.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
    "bins = np.arange(n_bins)\n",
    "\n",
    "# 5) Playback the two texture sounds\n",
    "print(\"▶️ Texture 1:\", FILENAME1)\n",
    "display(Audio(data=y1, rate=sr1, autoplay=False))\n",
    "print(\"▶️ Texture 2:\", FILENAME2)\n",
    "display(Audio(data=y2, rate=sr2, autoplay=False))\n",
    "\n",
    "# 6) Plot spectrograms and LBP histograms\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Spectrogram of Texture 1\n",
    "librosa.display.specshow(S1, sr=sr1, hop_length=HOP_LENGTH,\n",
    "                         x_axis='time', y_axis='log', ax=axes[0,0])\n",
    "axes[0,0].set_title(f'Spectrogram: {FILENAME1}')\n",
    "\n",
    "# Spectrogram of Texture 2\n",
    "librosa.display.specshow(S2, sr=sr2, hop_length=HOP_LENGTH,\n",
    "                         x_axis='time', y_axis='log', ax=axes[0,1])\n",
    "axes[0,1].set_title(f'Spectrogram: {FILENAME2}')\n",
    "\n",
    "# LBP histogram for Texture 1\n",
    "axes[1,0].bar(bins, hist1, width=0.8, color='C0', alpha=0.7)\n",
    "axes[1,0].set_title('LBP Histogram: Texture 1')\n",
    "axes[1,0].set_xlabel('LBP Code')\n",
    "axes[1,0].set_ylabel('Normalized Frequency')\n",
    "\n",
    "# LBP histogram for Texture 2\n",
    "axes[1,1].bar(bins, hist2, width=0.8, color='C1', alpha=0.7)\n",
    "axes[1,1].set_title('LBP Histogram: Texture 2')\n",
    "axes[1,1].set_xlabel('LBP Code')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4eadd2-98ae-41a1-9cb7-8bcabd734a35",
   "metadata": {},
   "source": [
    "### 🛠 Exercise: Texture-Based SVM Classification\n",
    "\n",
    "- **Task:**  \n",
    "  Extract texture descriptors from a collection of environmental sounds. For example:  \n",
    "  - **LBP histograms** computed on log-magnitude spectrogram patches  \n",
    "  - **Recurrence-based features** such as spectral self-similarity statistics  \n",
    "\n",
    "- **Analysis:**  \n",
    "  1. Assemble a feature matrix where each row represents one audio clip and columns are your chosen texture features.  \n",
    "  2. Normalize or standardize each feature.  \n",
    "  3. Use `scikit-learn` to train an **SVM classifier** (e.g. `sklearn.svm.SVC`) to distinguish sound classes (rain, applause, traffic, etc.).  \n",
    "  4. Evaluate with cross-validation to report classification accuracy.\n",
    "\n",
    "- **Deliverables:**  \n",
    "  - Python code for feature extraction and normalization  \n",
    "  - SVM training script with cross-validation results (e.g. accuracy, precision/recall)  \n",
    "  - A **confusion matrix** plot, and a few example audio clips that were misclassified (with their true vs. predicted labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b90b2ca-275d-45d7-8c9c-8d7747821796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
