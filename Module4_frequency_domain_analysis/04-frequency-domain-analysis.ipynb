{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d4c8aff-7416-4e5a-b006-3a04a60ca476",
   "metadata": {},
   "source": [
    "## Module 4: Frequency-Domain Analysis\n",
    "\n",
    "In this module, weâ€™ll dive into analyzing audio signals in the frequency domain and extract perceptually meaningful features.\n",
    "\n",
    "### Key Concepts\n",
    "- **Magnitude vs. Phase Spectra**  \n",
    "  How the amplitude and phase components of the Fourier transform each contribute to sound reconstruction.  \n",
    "- **Spectral Centroid, Bandwidth & Roll-off**  \n",
    "  Measures of â€œbrightness,â€ spectral spread, and the high-frequency energy cutoff.  \n",
    "- **Harmonic vs. Inharmonic Content**  \n",
    "  Identifying tonal (harmonic) structure versus noise-like (inharmonic) elements.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ““ Notebook Demos\n",
    "\n",
    "1. **Phase vs. Magnitude Reconstruction**  \n",
    "   - Extract just the phase spectrum (zero-out magnitudes), invert back to time-domain and listen  \n",
    "   - Extract just the magnitude spectrum (discard phase), invert (using zero phase) and listen  \n",
    "   - Compare what each component contributes to timbre  \n",
    "\n",
    "2. **Spectral Centroid Slider**  \n",
    "   - Take a musical recording and apply a simple low- or high-pass filter whose cutoff you can drag  \n",
    "   - Compute and plot the spectral centroid as you move the slider to hear/see how â€œbrightnessâ€ changes  \n",
    "   - Observe how filtering shifts the centroid (â€œbrightnessâ€) of the sound  \n",
    "\n",
    "3. **Harmonic vs. Inharmonic Content**  \n",
    "   - Use librosaâ€™s harmonic-percussive source separation (HPSS) to split a recording into its tonal (harmonic) parts and its noise-like (percussive/inharmonic) parts  \n",
    "   - Plot their spectra side-by-side, compute a simple â€œharmonic-to-noise ratioâ€ measure  \n",
    "   - Provide two audio players to A/B listen to the harmonic vs. inharmonic streams  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ›  Exercise: Spectral Feature Clustering\n",
    "- **Task:** Extract spectral features (centroid, bandwidth, roll-off, harmonicity) from example recordings of different instruments.  \n",
    "- **Analysis:** Use clustering (e.g. K-means) to group similar timbres.  \n",
    "- **Visualization:** Plot feature scatter-plots and show cluster assignments.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e5c0d4-b34f-42e3-8088-0e556a37e54c",
   "metadata": {},
   "source": [
    "### Key Concepts\n",
    "\n",
    "- **Magnitude vs. Phase Spectra**  \n",
    "  How the amplitude (magnitude) and timing (phase) components of the Fourier transform each contribute to reconstructing the original sound:  \n",
    "  - The **magnitude spectrum** determines â€œhow loudâ€ each frequency component is.  \n",
    "  - The **phase spectrum** governs â€œwhenâ€ each sinusoid starts, shaping the precise waveform and transient details.  \n",
    "  Both are required for an accurate time-domain reconstruction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4df461-5c80-46f7-afed-f2413fc68484",
   "metadata": {},
   "source": [
    "## Demo 1: Phase vs. Magnitude Reconstruction\n",
    "\n",
    "In this demo, you will explore how the **magnitude** and **phase** components of the Fourier transform each contribute to reconstructing a sound.\n",
    "\n",
    "**What to do:**  \n",
    "1. Edit the `FILENAME` in the â€œUSER SETTINGSâ€ section at the top of the code cell to point to your audio file in `sounds/`.  \n",
    "2. Run the cell to compute three versions of the signal:\n",
    "   - **Original**  \n",
    "   - **Magnitude-Only Reconstruction** (uses the original magnitudes, but sets all phases to zero)  \n",
    "   - **Phase-Only Reconstruction** (uses the original phases, but sets all magnitudes to one)  \n",
    "3. Use the built-in audio players to listen to each version in turn.\n",
    "\n",
    "**What to observe:**  \n",
    "- **Magnitude-Only:** You should hear the overall spectral envelope (timbre), but transients and fine waveform details will be smeared or distorted.  \n",
    "- **Phase-Only:** Youâ€™ll hear the timing and transient details (attacks, rhythmic cues), but the sound may be quieter or â€œthinnerâ€ since magnitude information is lost.  \n",
    "- Compare each reconstruction to the original and think about how magnitude shapes *what* you hear, while phase shapes *when* you hear it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf191d94-7361-4485-abaa-b05973c895ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ USER SETTINGS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FILENAME = 'pitch3.mp3'   # â† place your audio file in the `sounds/` folder\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "from IPython.display import Audio, display\n",
    "from pathlib import Path\n",
    "\n",
    "# â”€â”€ CONFIG (donâ€™t edit below here) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SOUNDS_DIR = Path('sounds')\n",
    "\n",
    "# 1) Load your clip\n",
    "audio_path = SOUNDS_DIR / FILENAME\n",
    "y, sr = librosa.load(str(audio_path), sr=None)\n",
    "\n",
    "# 2) Compute (real-valued) FFT via rfft\n",
    "Y     = np.fft.rfft(y)\n",
    "mag   = np.abs(Y)\n",
    "phase = np.angle(Y)\n",
    "\n",
    "# 3a) Magnitude-only reconstruction (zero phase)\n",
    "Y_mag = mag              # implicit phase = 0\n",
    "y_mag = np.fft.irfft(Y_mag, n=len(y))\n",
    "\n",
    "# 3b) Phase-only reconstruction (unit magnitude)\n",
    "Y_ph  = np.exp(1j * phase)\n",
    "y_ph  = np.fft.irfft(Y_ph, n=len(y))\n",
    "\n",
    "# 4) Play original and reconstructions\n",
    "print(\"â–¶ï¸ Original Audio\")\n",
    "display(Audio(data=y,      rate=sr, autoplay=False))\n",
    "print(\"â–¶ï¸ Magnitude-Only Reconstruction\")\n",
    "display(Audio(data=y_mag,  rate=sr, autoplay=False))\n",
    "print(\"â–¶ï¸ Phase-Only Reconstruction\")\n",
    "display(Audio(data=y_ph,   rate=sr, autoplay=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc4d8e2-57fa-4e95-bb73-f1bae3cbf6e4",
   "metadata": {},
   "source": [
    "## Key Concepts: Spectral Centroid, Bandwidth & Roll-Off\n",
    "\n",
    "In frequency-domain analysis we often want simple numbers that capture the â€œshapeâ€ of a spectrum. Three of the most common are:\n",
    "\n",
    "- **Spectral Centroid**  \n",
    "  The â€œcenter of massâ€ of the spectrum, computed as the weighted mean of frequencies by their magnitudes:  \n",
    "  \\[\n",
    "    \\text{centroid} = \\frac{\\sum_{k} f_k \\, |X[k]|}{\\sum_{k} |X[k]|}\n",
    "  \\]  \n",
    "  A higher centroid corresponds to a â€œbrighterâ€ or more high-frequency-rich sound.\n",
    "\n",
    "- **Spectral Bandwidth**  \n",
    "  A measure of how spread out the spectrum is around its centroid. Often defined as the root-mean-square deviation of frequencies from the centroid, weighted by magnitude. A wider bandwidth indicates a more â€œnoisyâ€ or spectrally diverse timbre.\n",
    "\n",
    "- **Spectral Roll-Off**  \n",
    "  The frequency below which a fixed percentage (e.g. 85% or 95%) of the total spectral energy is contained. It provides a simple cutoff measure for the â€œupper edgeâ€ of the spectrumâ€”useful for distinguishing dark vs. bright sounds.\n",
    "\n",
    "Together, these features quantify perceptual qualities like brightness, sharpness, and noisiness, and form the basis for many audio analysis and classification tasks.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf1bb46-a4d5-4b4e-bc5a-5d8de31ced61",
   "metadata": {},
   "source": [
    "## Demo 2: Spectral Centroid with Low-/High-Pass Filtering\n",
    "\n",
    "In this demo youâ€™ll see how simple low-pass and high-pass filters shift the â€œbrightnessâ€ of a sound, as measured by its spectral centroid.\n",
    "\n",
    "---\n",
    "\n",
    "### How to use:\n",
    "\n",
    "1. **Edit the inputs at the top of the code cell**  \n",
    "   - `FILENAME`: name of your audio file in the `sounds/` folder  \n",
    "   - `CUTOFF_FREQ`: cutoff frequency in Hz (must satisfy `0 < CUTOFF_FREQ < sr/2`)  \n",
    "   - `FRAME_LENGTH`: window size (in samples) for computing the spectral centroid  \n",
    "   - `HOP_LENGTH`: hop size between frames (typically `FRAME_LENGTH/4` or similar)  \n",
    "\n",
    "2. **Run the cell**  \n",
    "   - The script will load your clip, design 4th-order Butterworth low- and high-pass filters, and apply them with zero-phase filtering.  \n",
    "   - It computes three audio streamsâ€”original, low-pass, and high-passâ€”and three corresponding spectral-centroid curves.\n",
    "\n",
    "---\n",
    "\n",
    "### Listen & compare\n",
    "\n",
    "- Use the built-in players to A/B listen to the **Original** vs. **Low-Pass** vs. **High-Pass** versions.  \n",
    "- Note how the low-pass version sounds **â€œdullerâ€** (high frequencies removed) and the high-pass version sounds **â€œthinnerâ€** or **â€œbrighterâ€** (low frequencies removed).\n",
    "\n",
    "---\n",
    "\n",
    "### Observe the plots\n",
    "\n",
    "- **Low-Pass plot** shows the original centroid (blue) vs. the low-pass centroid (orange). You should see the centroid **drop** whenever high frequencies are attenuated.  \n",
    "- **High-Pass plot** shows the original centroid (blue) vs. the high-pass centroid (green). Here the centroid **rises**, indicating dominance of higher-frequency content.\n",
    "\n",
    "---\n",
    "\n",
    "### What to look for\n",
    "\n",
    "- **Filter effect on centroid:** Low-pass â†’ lower centroid; High-pass â†’ higher centroid.  \n",
    "- **Time-varying behavior:** How the â€œbrightnessâ€ of the signal evolves over time under each filter.  \n",
    "- **Audio confirmation:** Does what you hear (dull vs. bright) match the centroid curves?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f6d052-63a4-4c2a-9a7f-ebde23f1b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ USER SETTINGS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FILENAME     = 'pitch2.mp3'    # â† place your audio file in the `sounds/` folder\n",
    "CUTOFF_FREQ  = 2000            # â† cutoff frequency in Hz (0 < CUTOFF_FREQ < sr/2)\n",
    "FRAME_LENGTH = 2048            # â† frame length for spectral centroid\n",
    "HOP_LENGTH   = FRAME_LENGTH // 4  # â† hop length for spectral centroid\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from scipy.signal import butter, filtfilt\n",
    "from IPython.display import Audio, display\n",
    "from pathlib import Path\n",
    "\n",
    "# â”€â”€ CONFIG (donâ€™t edit below here) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SOUNDS_DIR = Path('sounds')\n",
    "\n",
    "# 1) Load audio\n",
    "y, sr = librosa.load(str(SOUNDS_DIR / FILENAME), sr=None)\n",
    "\n",
    "# 2) Design Butterworth low-pass and high-pass filters\n",
    "nyq = sr / 2\n",
    "b_low,  a_low  = butter(N=4, Wn=CUTOFF_FREQ/nyq, btype='lowpass')\n",
    "b_high, a_high = butter(N=4, Wn=CUTOFF_FREQ/nyq, btype='highpass')\n",
    "\n",
    "# 3) Apply zero-phase filtering\n",
    "y_low  = filtfilt(b_low,  a_low,  y)\n",
    "y_high = filtfilt(b_high, a_high, y)\n",
    "\n",
    "# 4) Compute spectral centroids\n",
    "cent_orig = librosa.feature.spectral_centroid(\n",
    "    y=y, sr=sr,\n",
    "    n_fft=FRAME_LENGTH,\n",
    "    hop_length=HOP_LENGTH\n",
    ")[0]\n",
    "cent_low = librosa.feature.spectral_centroid(\n",
    "    y=y_low, sr=sr,\n",
    "    n_fft=FRAME_LENGTH,\n",
    "    hop_length=HOP_LENGTH\n",
    ")[0]\n",
    "cent_high = librosa.feature.spectral_centroid(\n",
    "    y=y_high, sr=sr,\n",
    "    n_fft=FRAME_LENGTH,\n",
    "    hop_length=HOP_LENGTH\n",
    ")[0]\n",
    "times = librosa.frames_to_time(np.arange(len(cent_orig)),\n",
    "                               sr=sr, hop_length=HOP_LENGTH)\n",
    "\n",
    "# 5) Play original, low-pass & high-pass audio\n",
    "print(\"â–¶ï¸ Original Audio\")\n",
    "display(Audio(data=y,      rate=sr, autoplay=False))\n",
    "print(f\"â–¶ï¸ Low-Pass @ {CUTOFF_FREQ} Hz\")\n",
    "display(Audio(data=y_low,  rate=sr, autoplay=False))\n",
    "print(f\"â–¶ï¸ High-Pass @ {CUTOFF_FREQ} Hz\")\n",
    "display(Audio(data=y_high, rate=sr, autoplay=False))\n",
    "\n",
    "# 6a) Plot spectral centroid: Low-Pass vs. Original\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(times, cent_orig,  label='Original', linewidth=1.5)\n",
    "plt.plot(times, cent_low,   label=f'Low-Pass @ {CUTOFF_FREQ} Hz', linewidth=1.5)\n",
    "plt.title('Spectral Centroid â€” Low-Pass Filter')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Centroid (Hz)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 6b) Plot spectral centroid: High-Pass vs. Original\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(times, cent_orig,   label='Original', linewidth=1.5)\n",
    "plt.plot(times, cent_high,   label=f'High-Pass @ {CUTOFF_FREQ} Hz', linewidth=1.5)\n",
    "plt.title('Spectral Centroid â€” High-Pass Filter')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Centroid (Hz)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04ba350-782e-4bd8-a810-47e8a567a549",
   "metadata": {},
   "source": [
    "### Harmonic vs. Inharmonic Content\n",
    "\n",
    "In this section youâ€™ll learn how to separate and compare the **tonal (harmonic)** parts of a signalâ€”those with clear, integerâ€multiple frequency relationshipsâ€”from the **noiseâ€like (inharmonic)** parts that lack periodic structure.\n",
    "\n",
    "- **Harmonic content**  \n",
    "  - Consists of sinusoidal components whose frequencies are integer multiples of a fundamental.  \n",
    "  - Carries pitch and tonal information (e.g. sustained notes, vocals, strings).  \n",
    "\n",
    "- **Inharmonic content**  \n",
    "  - Composed of broad, non-periodic energy (e.g. drum hits, cymbals, breath noise).  \n",
    "  - Lacks clear harmonic relationships and sounds more â€œnoisyâ€ or percussive.\n",
    "\n",
    "**Why it matters:**  \n",
    "- Many audio-processing tasks (timbre analysis, source separation, synthesis) rely on distinguishing tonal vs. noise elements.  \n",
    "- Being able to isolate each component can improve pitch tracking, denoising, and creative effects.\n",
    "\n",
    "> In the next demo weâ€™ll use librosaâ€™s **Harmonicâ€“Percussive Source Separation (HPSS)** to split a recording into its harmonic and inharmonic streams, listen to each, and compare their spectra and energy ratios.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4f8b9a-ffaa-412b-8c19-3cbbf168ef67",
   "metadata": {},
   "source": [
    "### Demo 3: Harmonic vs. Inharmonic Separation\n",
    "\n",
    "In this demo youâ€™ll use **Harmonicâ€“Percussive Source Separation (HPSS)** to split a recording into its tonal (harmonic) and noise-like (inharmonic/percussive) components, listen to each, and compare their spectra.\n",
    "\n",
    "**How to use:**\n",
    "\n",
    "1. **Edit the USER SETTINGS** at the top of the code cell:\n",
    "   - `FILENAME` â€“ name of your audio file in the `sounds/` folder  \n",
    "   - `MARGIN_HARM` â€“ harmonic margin (>0, e.g. 0.1â€“10.0)  \n",
    "   - `MARGIN_PERC` â€“ percussive margin (>0, e.g. 0.1â€“10.0)  \n",
    "   - `STFT_N_FFT` â€“ window length for the STFT (power of 2, e.g. 512, 1024, 2048)  \n",
    "   - `STFT_HOP_LEN` â€“ hop length between STFT frames (typically `STFT_N_FFT//4`)\n",
    "\n",
    "2. **Run the cell**. The script will:\n",
    "   - Load your clip\n",
    "   - Perform HPSS to obtain `y_harm` and `y_perc`\n",
    "   - Display two audio players to listen to each component\n",
    "   - Compute and plot the average magnitude spectrum of each\n",
    "   - Print out the harmonic-to-inharmonic energy ratio\n",
    "\n",
    "**What to observe:**\n",
    "\n",
    "- **Audio**  \n",
    "  - The *harmonic* stream retains sustained, pitched elements (notes, vocals)  \n",
    "  - The *percussive* stream captures transients and noise-like sounds (drums, cymbals)\n",
    "\n",
    "- **Spectra**  \n",
    "  - The harmonic spectrum will show sharp peaks at integer multiples of the fundamental  \n",
    "  - The inharmonic/percussive spectrum will be broader and noisier  \n",
    "\n",
    "- **Energy Ratio**  \n",
    "  - A value >1 indicates more energy in harmonic content; <1 means percussive dominates\n",
    "\n",
    "Feel free to tweak the `MARGIN_HARM` and `MARGIN_PERC` to adjust the separation aggressiveness, or change the STFT parameters to refine the spectral view.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f63ad6-b4e7-4de8-829a-e3555d75ff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ USER SETTINGS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FILENAME      = 'pitch3.mp3'   # â† place your file in the `sounds/` folder\n",
    "MARGIN_HARM   = 1.0                     # â† harmonic margin (>0, e.g. 0.1â€“10.0)\n",
    "MARGIN_PERC   = 1.0                     # â† percussive margin (>0, e.g. 0.1â€“10.0)\n",
    "STFT_N_FFT    = 2048                    # â† window size for STFT (power of 2)\n",
    "STFT_HOP_LEN  = STFT_N_FFT // 4         # â† hop length between frames\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# â”€â”€ CONFIG (donâ€™t edit below here) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SOUNDS_DIR = Path('sounds')\n",
    "audio_path = SOUNDS_DIR / FILENAME\n",
    "\n",
    "# 1) Load audio\n",
    "y, sr = librosa.load(str(audio_path), sr=None)\n",
    "\n",
    "# 2) Harmonicâ€“Percussive Source Separation\n",
    "y_harm, y_perc = librosa.effects.hpss(y,\n",
    "    margin=(MARGIN_HARM, MARGIN_PERC)\n",
    ")\n",
    "\n",
    "# 3) Listen to each component\n",
    "print(\"â–¶ï¸ Harmonic Component\")\n",
    "display(Audio(data=y_harm, rate=sr, autoplay=False))\n",
    "print(\"â–¶ï¸ Inharmonic (Percussive) Component\")\n",
    "display(Audio(data=y_perc, rate=sr, autoplay=False))\n",
    "\n",
    "# 4) Compute average magnitude spectra\n",
    "D_h = np.abs(librosa.stft(y_harm, n_fft=STFT_N_FFT, hop_length=STFT_HOP_LEN))\n",
    "D_p = np.abs(librosa.stft(y_perc, n_fft=STFT_N_FFT, hop_length=STFT_HOP_LEN))\n",
    "mag_h = D_h.mean(axis=1)\n",
    "mag_p = D_p.mean(axis=1)\n",
    "freqs = np.linspace(0, sr/2, len(mag_h))\n",
    "\n",
    "# 5) Plot spectra side-by-side\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 4), sharey=True)\n",
    "\n",
    "ax[0].plot(freqs, 20*np.log10(mag_h + 1e-8))\n",
    "ax[0].set_title('Harmonic Spectrum')\n",
    "ax[0].set_xlabel('Frequency (Hz)')\n",
    "ax[0].set_ylabel('Magnitude (dB)')\n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].plot(freqs, 20*np.log10(mag_p + 1e-8), color='C1')\n",
    "ax[1].set_title('Inharmonic (Percussive) Spectrum')\n",
    "ax[1].set_xlabel('Frequency (Hz)')\n",
    "ax[1].grid(True)\n",
    "\n",
    "fig.suptitle('Harmonic vs. Inharmonic Spectra', fontsize=16)\n",
    "plt.tight_layout(rect=[0,0,1,0.95])\n",
    "plt.show()\n",
    "\n",
    "# 6) Compute and display energy ratio\n",
    "energy_h = np.sum(y_harm**2)\n",
    "energy_p = np.sum(y_perc**2)\n",
    "print(f\"ğŸ”¢ Harmonic-to-Inharmonic Energy Ratio: {energy_h/energy_p:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe295262-bf50-4dee-b468-9fd4bf218ae7",
   "metadata": {},
   "source": [
    "### ğŸ›  Exercise: Spectral Feature Clustering\n",
    "\n",
    "**Objective:**  \n",
    "Apply unsupervised learning to group audio recordings by timbre based on their spectral characteristics.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Gather Data**  \n",
    "   - Place several short audio clips (e.g., 2â€“5 s) of different instrumentsâ€”piano, violin, flute, trumpet, drums, etc.â€”in your `sounds/` folder.\n",
    "\n",
    "2. **Extract Features**  \n",
    "   For each clip, compute a feature vector of spectral descriptors:\n",
    "   - **Spectral Centroid**  \n",
    "   - **Spectral Bandwidth**  \n",
    "   - **Spectral Roll-off** (e.g., 85 % energy cutoff)  \n",
    "   - **Harmonic-to-Noise Ratio** (use HPSS or `librosa.effects.hpss` and compute energy ratio)  \n",
    "\n",
    "3. **Build a Feature Matrix**  \n",
    "   - Assemble an \\(M\\times4\\) matrix, where \\(M\\) is the number of clips and each column is one of the four features.\n",
    "\n",
    "4. **Cluster with K-Means**  \n",
    "   - Use scikit-learnâ€™s `KMeans` to partition the feature matrix into \\(K\\) clusters (choose \\(K\\) equal to the number of instrument types you have).\n",
    "   - Examine the cluster labels and see which instruments are grouped together.\n",
    "\n",
    "5. **Visualize**  \n",
    "   - Create 2D scatter plots of feature pairs (e.g., centroid vs. bandwidth, roll-off vs. H/N ratio), coloring points by their cluster assignment.\n",
    "   - Optionally, reduce to 2 D with PCA and plot the clusters in PCA space.\n",
    "\n",
    "6. **Interpret**  \n",
    "   - Do the clusters make sense perceptually?  \n",
    "   - Which features were most discriminative?  \n",
    "   - How would you refine your feature set or clustering parameters?\n",
    "\n",
    "> **Bonus:** Try another clustering algorithm (e.g., DBSCAN or Agglomerative Clustering) and compare the results!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2a7d85-6afd-478d-b09e-d63945426638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
