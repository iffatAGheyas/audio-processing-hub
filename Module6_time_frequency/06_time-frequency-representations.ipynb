{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15be34fc-55b4-4be7-9182-00638a281441",
   "metadata": {},
   "source": [
    "## Module 6: Time–Frequency Representations\n",
    "\n",
    "In this module, we’ll explore how to analyze audio signals simultaneously in time and frequency using a variety of transforms and visualizations.\n",
    "\n",
    "### Key Concepts\n",
    "- **Short–Time Fourier Transform (STFT) & Spectrograms**  \n",
    "  How sliding-window FFTs reveal the time-varying spectral content of a signal.  \n",
    "- **Constant-Q Transform (CQT) vs. Mel-Spectrogram**  \n",
    "  Log-frequency analysis tailored to musical pitches (CQT) or perceptual frequency bands (mel).  \n",
    "- **Wavelet Transforms (Continuous & Discrete)**  \n",
    "  Multi-scale decomposition using localized time–frequency “atoms.”\n",
    "\n",
    "---\n",
    "\n",
    "### 📓 Notebook Demos\n",
    "\n",
    "1. **STFT Window Visualization**  \n",
    "   - Play an audio clip while drawing a moving time-window on the waveform  \n",
    "   - Display the corresponding spectrogram slice in real time  \n",
    "\n",
    "2. **Linear vs. Mel-Scale Spectrograms**  \n",
    "   - Compute both representations for the same audio  \n",
    "   - Visualize and listen to each to appreciate perceptual frequency scaling\n",
    "3. **Constant-Q Transform (CQT) Analysis**  \n",
    "   - Compute the CQT spectrogram of your audio (log-spaced bins)  \n",
    "   - Visualize how bin widths grow with frequency  \n",
    "   - Listen to the original clip and compare to the CQT-reconstructed audio     \n",
    "\n",
    "4. **Interactive Continuous Wavelet Transform**  \n",
    "   - Choose a mother wavelet (Morlet, Mexican hat, etc.)  \n",
    "   - See its time–frequency tiling and corresponding CWT coefficients  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠 Exercise: CQT-Based Octave Filter Bank\n",
    "- **Task:** Design and implement a filter bank using the Constant-Q Transform to isolate individual musical octaves.  \n",
    "- **Steps:**  \n",
    "  1. Compute the CQT of an audio signal.  \n",
    "  2. Group CQT bins into octave bands.  \n",
    "  3. Reconstruct and play back each octave band separately.  \n",
    "- **Deliverables:**  \n",
    "  - Plots of the CQT spectrogram with octave overlays.  \n",
    "  - Reconstructed audio players for each isolated octave.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903c6514-4907-4302-914e-e2952a58dc5b",
   "metadata": {},
   "source": [
    "### Key Concept: Short–Time Fourier Transform (STFT) & Spectrograms\n",
    "\n",
    "The **Short–Time Fourier Transform (STFT)** slices a signal into short, overlapping frames and computes an FFT on each one, turning a 1D time-series into a 2D time–frequency map. By windowing each frame (e.g. Hann, Hamming) and sliding it forward by a fixed hop size, you capture how the spectral content evolves over time.\n",
    "\n",
    "A **spectrogram** is simply the magnitude (or log-magnitude) of the STFT plotted with:\n",
    "\n",
    "- **X-axis:** Time  \n",
    "- **Y-axis:** Frequency  \n",
    "- **Color/Intensity:** Signal energy at each time–frequency point  \n",
    "\n",
    "With a spectrogram you can clearly see events such as:\n",
    "\n",
    "- Harmonic stacks moving with a melody  \n",
    "- Transient “bursts” from percussive hits  \n",
    "- Formant shifts in speech  \n",
    "\n",
    "In the next demo, you’ll play an audio clip while a moving window highlights the current frame on the waveform and shows its corresponding slice on the spectrogram, so you can directly connect “what you hear” to “what you see.”  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba30089a-6155-45da-adad-0a072b6788d5",
   "metadata": {},
   "source": [
    "## Demo: STFT Window Visualization\n",
    "\n",
    "In this demo, you’ll see exactly how a sliding-window FFT “peels off” the time-varying spectrum of an audio signal—frame by frame.\n",
    "\n",
    "### What the code does\n",
    "1. **Loads** your audio file and computes its Short-Time Fourier Transform (STFT) using a Hann window of length `FRAME_SIZE` and hop size `HOP_LENGTH`.  \n",
    "2. **Displays** an audio player so you can listen to the clip.  \n",
    "3. **Provides** a slider (`Frame:`) that selects which STFT frame to inspect.  \n",
    "4. For the selected frame:  \n",
    "   - **Highlights** the corresponding time-window on the waveform plot.  \n",
    "   - **Plots** the magnitude spectrum (frequency vs. amplitude) of that window.\n",
    "\n",
    "### Inputs (edit in the code cell)\n",
    "- `FILENAME`: Name of your audio file in the `sounds/` folder (WAV or MP3).  \n",
    "- `FRAME_SIZE`: FFT window length in samples (power of 2, e.g. 512, 1024, 2048).  \n",
    "- `HOP_LENGTH`: Hop size in samples between successive windows (commonly `FRAME_SIZE//4`).  \n",
    "\n",
    "### How to run\n",
    "1. Update the **USER SETTINGS** at the top of the code cell with your file name and desired parameters.  \n",
    "2. Run the cell to compute the STFT, render the audio player, and show the frame slider.  \n",
    "3. Drag the **Frame** slider to move through time-frames.  \n",
    "\n",
    "### What to observe\n",
    "- **Waveform view**: The shaded region shows which slice of the signal the FFT is analyzing.  \n",
    "- **Spectrum view**: Peaks reveal the dominant frequencies in that time-window.  \n",
    "- **Transients** (e.g., drum hits) appear as broad, flat spectra; **tonal** sections (e.g., vocals or sustained notes) show clear spectral lines.  \n",
    "- By stepping frame by frame, you can link time-domain events to their frequency-domain signatures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d458cebf-d48c-4ed7-869f-9fea588c9f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── USER SETTINGS ────────────────────────────────────────────────────────────────\n",
    "FILENAME     = 'cymbal.mp3'   # ← place your audio file in the `sounds/` folder\n",
    "FRAME_SIZE   = 2048               # ← STFT window size (power of 2)\n",
    "HOP_LENGTH   = FRAME_SIZE // 4    # ← hop size between frames\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Audio, display, clear_output\n",
    "from pathlib import Path\n",
    "\n",
    "# ── CONFIG (don’t edit below here) ───────────────────────────────────────────────\n",
    "SOUNDS_DIR = Path('sounds')\n",
    "audio_path = SOUNDS_DIR / FILENAME\n",
    "\n",
    "# 1) Load audio and compute STFT\n",
    "y, sr = librosa.load(str(audio_path), sr=None)\n",
    "D     = librosa.stft(y, n_fft=FRAME_SIZE, hop_length=HOP_LENGTH, window='hann')\n",
    "mag   = np.abs(D)\n",
    "\n",
    "# 2) Precompute time & frequency axes\n",
    "times = librosa.frames_to_time(np.arange(mag.shape[1]), sr=sr, hop_length=HOP_LENGTH)\n",
    "freqs = librosa.fft_frequencies(sr=sr, n_fft=FRAME_SIZE)\n",
    "\n",
    "# 3) Display audio player\n",
    "print(\"▶️ Play Audio\")\n",
    "display(Audio(data=y, rate=sr, autoplay=False))\n",
    "\n",
    "# 4) Create frame slider\n",
    "frame_slider = widgets.IntSlider(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=mag.shape[1] - 1,\n",
    "    step=1,\n",
    "    description='Frame:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "# 5) Output area for plots\n",
    "output = widgets.Output()\n",
    "\n",
    "# 6) Update function\n",
    "def update(frame):\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        t0 = times[frame]\n",
    "        t1 = t0 + FRAME_SIZE/sr\n",
    "        \n",
    "        # a) Waveform + window highlight\n",
    "        plt.figure(figsize=(10, 3))\n",
    "        t_axis = np.arange(len(y)) / sr\n",
    "        plt.plot(t_axis, y, alpha=0.7)\n",
    "        plt.axvspan(t0, t1, color='C1', alpha=0.3)\n",
    "        plt.title(f'Waveform (window: {t0:.3f}–{t1:.3f} s)')\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # b) Spectrum slice at this frame\n",
    "        plt.figure(figsize=(8, 3))\n",
    "        plt.plot(freqs, mag[:, frame], alpha=0.7)\n",
    "        plt.title(f'Spectrum at {t0:.3f} s (frame {frame})')\n",
    "        plt.xlabel('Frequency (Hz)')\n",
    "        plt.ylabel('Magnitude')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 7) Wire slider to update\n",
    "frame_slider.observe(lambda change: update(change['new']), names='value')\n",
    "\n",
    "# 8) Initial draw & display UI\n",
    "update(0)\n",
    "display(widgets.VBox([frame_slider, output]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbc35e1-b637-48a8-91ce-add5c0e7fc64",
   "metadata": {},
   "source": [
    "### Key Concept: Constant-Q Transform vs. Mel-Spectrogram\n",
    "\n",
    "Both CQT and mel-spectrograms provide “time–frequency” views of audio, but they differ in how they space their frequency bins:\n",
    "\n",
    "- **Constant-Q Transform (CQT)**  \n",
    "  - **Logarithmic frequency axis:** Bin centers follow musical pitch ratios (e.g. semitones), so each octave is divided into the same number of bins.  \n",
    "  - **Constant Q-factor:** The ratio of center frequency to bandwidth is constant, giving higher frequency resolution at low frequencies and finer temporal resolution at high frequencies.  \n",
    "  - **Ideal for music analysis:** Notes, chords, and harmonics align naturally on CQT bins.\n",
    "\n",
    "- **Mel-Spectrogram**  \n",
    "  - **Perceptual scale:** Frequencies are warped according to the mel scale, which approximates human pitch perception (more resolution at low frequencies, compressed at high frequencies).  \n",
    "  - **Linear windowing:** Uses a uniform FFT followed by triangular mel-filterbanks.  \n",
    "  - **Common in speech / audio ML:** Mel features correlate well with perceptual loudness and are a standard input to many audio-processing models.\n",
    "\n",
    "**When to choose which?**  \n",
    "- Use **CQT** when your application is explicitly musical (pitch estimation, chord recognition, transcription).  \n",
    "- Use **mel-spectrograms** when modeling or visualizing general audio in a perceptually meaningful way (speech recognition, audio embeddings, feature extraction).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365d34cf-5876-4cb9-8519-ed21faba91cf",
   "metadata": {},
   "source": [
    "## Demo 2: Linear-Frequency vs. Mel-Scale Spectrograms\n",
    "\n",
    "In this demo you’ll compute and compare two common time–frequency representations for the same audio:\n",
    "\n",
    "1. **Linear-Frequency Spectrogram**  \n",
    "   - Uses a standard STFT to show energy at evenly spaced frequency bins.  \n",
    "2. **Mel-Scale Spectrogram**  \n",
    "   - Aggregates energy into perceptually-scaled bands (more resolution at low frequencies, less at high).\n",
    "\n",
    "---\n",
    "\n",
    "### What the code does\n",
    "\n",
    "1. **Loads** your chosen audio file (`FILENAME`) at its native sampling rate.  \n",
    "2. **Computes**:  \n",
    "   - A linear-frequency spectrogram via `librosa.stft`.  \n",
    "   - A mel-spectrogram via `librosa.feature.melspectrogram`.  \n",
    "3. **Converts** both to dB (`amplitude_to_db` / `power_to_db`) for display.  \n",
    "4. **Plays back** the original audio so you can listen.  \n",
    "5. **Plots** the two spectrograms side-by-side with a shared color scale.\n",
    "\n",
    "---\n",
    "\n",
    "### Inputs you can edit at the top of the code cell\n",
    "\n",
    "- `FILENAME`  \n",
    "  - Name of your WAV/MP3 file in the `sounds/` folder.  \n",
    "- `N_FFT`  \n",
    "  - FFT window size (power of 2, e.g. 512, 1024, 2048).  \n",
    "- `HOP_LENGTH`  \n",
    "  - Hop size between frames (commonly `N_FFT/4`).  \n",
    "- `N_MELS`  \n",
    "  - Number of mel bands (e.g. 64, 128, 256).\n",
    "\n",
    "---\n",
    "\n",
    "### What to observe\n",
    "\n",
    "- **Frequency axis**  \n",
    "  - Linear spectrogram is uniform in Hz, mel spectrogram is warped to match human perception.  \n",
    "- **Spectral detail**  \n",
    "  - Mel-scale will show finer detail in the low-frequency range and coarser detail at high frequencies.  \n",
    "- **Perceptual relevance**  \n",
    "  - Listen for how the mel spectrogram representation better captures perceptual timbral changes in speech or music.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0808b4-825e-453c-a8c2-94d478a94479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── USER SETTINGS ────────────────────────────────────────────────────────────────\n",
    "FILENAME     = 'cymbal.mp3'   # ← place your file in `sounds/` (WAV/MP3)\n",
    "N_FFT        = 2048           # ← FFT window size (power of 2)\n",
    "HOP_LENGTH   = N_FFT // 4     # ← hop length between frames\n",
    "N_MELS       = 128            # ← number of mel bands\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "from IPython.display import Audio, display\n",
    "from pathlib import Path\n",
    "\n",
    "# ── CONFIG (don’t edit below here) ───────────────────────────────────────────────\n",
    "SOUNDS_DIR = Path('sounds')\n",
    "audio_path = SOUNDS_DIR / FILENAME\n",
    "\n",
    "# 1) Load audio\n",
    "y, sr = librosa.load(str(audio_path), sr=None)\n",
    "\n",
    "# 2) Compute linear-frequency spectrogram (magnitude)\n",
    "S_lin    = np.abs(librosa.stft(y, n_fft=N_FFT, hop_length=HOP_LENGTH))\n",
    "S_lin_db = librosa.amplitude_to_db(S_lin, ref=np.max)\n",
    "\n",
    "# 3) Compute mel-spectrogram\n",
    "S_mel    = librosa.feature.melspectrogram(\n",
    "    y=y,\n",
    "    sr=sr,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    n_mels=N_MELS\n",
    ")\n",
    "S_mel_db = librosa.power_to_db(S_mel, ref=np.max)\n",
    "\n",
    "# 4) Play original audio\n",
    "print(\"▶️ Original Audio\")\n",
    "display(Audio(data=y, rate=sr, autoplay=False))\n",
    "\n",
    "# 5) Plot both spectrograms side by side with constrained_layout\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), constrained_layout=True)\n",
    "\n",
    "# Linear-frequency spectrogram\n",
    "im1 = librosa.display.specshow(\n",
    "    S_lin_db,\n",
    "    sr=sr,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    x_axis='time',\n",
    "    y_axis='linear',\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title('Linear-Frequency Spectrogram')\n",
    "axes[0].set_xlabel('Time (s)')\n",
    "axes[0].set_ylabel('Frequency (Hz)')\n",
    "\n",
    "# Mel-scale spectrogram\n",
    "im2 = librosa.display.specshow(\n",
    "    S_mel_db,\n",
    "    sr=sr,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    x_axis='time',\n",
    "    y_axis='mel',\n",
    "    fmax=sr/2,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(f'Mel-Spectrogram ({N_MELS} bands)')\n",
    "axes[1].set_xlabel('Time (s)')\n",
    "\n",
    "# 6) Add a single colorbar for the last image, anchored to the right\n",
    "fig.colorbar(im2, ax=axes, format='%+2.0f dB', location='right', pad=0.02)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e1b4d3-1c54-4d34-816b-bc2d2607940c",
   "metadata": {},
   "source": [
    "# 🎶 Demo 3: Constant-Q Transform (CQT) Analysis\n",
    "\n",
    "In this demo, you'll explore a **logarithmic time–frequency representation** that’s tuned to **musical pitches**. You'll compute a **CQT spectrogram** with **geometrically spaced bins**, examine how the **bin widths grow with frequency**, and even **reconstruct the audio** from the CQT to hear what information is preserved.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ How to Use\n",
    "\n",
    "Edit the inputs at the top of the code cell:\n",
    "\n",
    "- **`FILENAME`**  \n",
    "  Name of your audio file in the `sounds/` folder (`.wav` or `.mp3`).\n",
    "\n",
    "- **`N_BINS`**  \n",
    "  Total number of CQT bins (e.g. `84` for 7 octaves × 12 bins).\n",
    "\n",
    "- **`BINS_PER_OCTAVE`**  \n",
    "  Bins per octave (commonly `12` for semitones).\n",
    "\n",
    "- **`HOP_LENGTH`**  \n",
    "  Hop size between frames (in samples).\n",
    "\n",
    "- **`FMIN`**  \n",
    "  Minimum frequency in Hz (e.g. `C1 = 32.70 Hz`).\n",
    "\n",
    "---\n",
    "\n",
    "## ▶️ What Happens When You Run the Cell\n",
    "\n",
    "1. **Load your clip**\n",
    "2. **Compute the CQT**  \n",
    "   `librosa.cqt(y, sr, n_bins=N_BINS, bins_per_octave=BINS_PER_OCTAVE)`\n",
    "3. **Display the CQT spectrogram** on a log-frequency axis\n",
    "4. **Plot bin width growth** with respect to center frequency\n",
    "5. **Reconstruct audio** from the CQT and provide an interactive player\n",
    "\n",
    "---\n",
    "\n",
    "## 👀 What to Observe\n",
    "\n",
    "### 🎼 CQT Spectrogram\n",
    "- **Logarithmic frequency scaling**: Equal-width spacing in **octaves**, not linear Hz\n",
    "- **Higher bins** cover **wider frequency bands** — this is ideal for musical analysis\n",
    "\n",
    "### 📈 Bin-Width Growth Plot\n",
    "- Plots the **difference between successive center frequencies**\n",
    "- Confirms that **higher-frequency bins are broader**\n",
    "\n",
    "### 🎧 Audio Comparison\n",
    "- Compare **original** vs. **CQT-reconstructed** audio\n",
    "- Listen for how well the CQT preserves **tonal content** vs. **transient detail**\n",
    "\n",
    "---\n",
    "\n",
    "🧠 *Tip*: Use this analysis when working with **melodic**, **harmonic**, or **pitch-based** audio data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918dcaba-f3d3-4b51-9437-2f330a013ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── USER SETTINGS ────────────────────────────────────────────────────────────────\n",
    "FILENAME          = 'cymbal.mp3'  # ← place your file in `sounds/` (WAV/MP3)\n",
    "N_BINS            = 84                     # ← total CQT bins (e.g. 84 for 7 octaves × 12 bins)\n",
    "BINS_PER_OCTAVE   = 12                     # ← bins per octave (commonly 12)\n",
    "HOP_LENGTH        = 512                    # ← hop size between frames\n",
    "FMIN              = 32.70                  # ← minimum frequency in Hz (e.g. C1 = 32.70 Hz)\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "from IPython.display import Audio, display\n",
    "from pathlib import Path\n",
    "\n",
    "# Load audio\n",
    "SOUNDS_DIR = Path('sounds')\n",
    "y, sr = librosa.load(str(SOUNDS_DIR / FILENAME), sr=None)\n",
    "\n",
    "# Compute CQT\n",
    "C = librosa.cqt(\n",
    "    y,\n",
    "    sr=sr,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    fmin=FMIN,\n",
    "    n_bins=N_BINS,\n",
    "    bins_per_octave=BINS_PER_OCTAVE\n",
    ")\n",
    "C_mag = np.abs(C)\n",
    "C_db  = librosa.amplitude_to_db(C_mag, ref=np.max)\n",
    "\n",
    "# Get CQT bin center frequencies\n",
    "freqs = librosa.cqt_frequencies(\n",
    "    n_bins=N_BINS,\n",
    "    fmin=FMIN,\n",
    "    bins_per_octave=BINS_PER_OCTAVE\n",
    ")\n",
    "\n",
    "# 1) Play original audio\n",
    "print(\"▶️ Original Audio\")\n",
    "display(Audio(data=y, rate=sr, autoplay=False))\n",
    "\n",
    "# 2) Plot CQT spectrogram (log-frequency axis)\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(\n",
    "    C_db,\n",
    "    sr=sr,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    x_axis='time',\n",
    "    y_axis='cqt_hz',\n",
    "    fmin=FMIN,\n",
    "    bins_per_octave=BINS_PER_OCTAVE\n",
    ")\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Constant-Q Transform (CQT) Spectrogram')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3) Visualize bin-width growth\n",
    "bin_widths = np.diff(freqs)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(freqs[:-1], bin_widths, marker='o')\n",
    "plt.title('CQT Bin Width vs. Center Frequency')\n",
    "plt.xlabel('Center Frequency (Hz)')\n",
    "plt.ylabel('Bin Width (Hz)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4) Reconstruct audio from CQT and play\n",
    "y_rec = librosa.icqt(\n",
    "    C,\n",
    "    sr=sr,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    fmin=FMIN,\n",
    "    bins_per_octave=BINS_PER_OCTAVE\n",
    ")\n",
    "print(\"▶️ CQT-Reconstructed Audio\")\n",
    "display(Audio(data=y_rec, rate=sr, autoplay=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6c242e-0cb8-469d-a4b4-806e291ac467",
   "metadata": {},
   "source": [
    "## 🔑 Key Concept: Wavelet Transforms (Continuous & Discrete)\n",
    "\n",
    "Wavelet transforms perform a **multi-scale decomposition** of a signal using **localized time–frequency “atoms”**, allowing you to **zoom in on both fast transients and slow oscillations simultaneously**.\n",
    "\n",
    "### 🌊 Continuous Wavelet Transform (CWT)\n",
    "- Provides a **dense**, highly **redundant** representation.\n",
    "- Ideal for **detailed time–frequency analysis**.\n",
    "- Useful for examining signals with **non-stationary features**.\n",
    "\n",
    "### 📦 Discrete Wavelet Transform (DWT)\n",
    "- Produces a **compact**, **hierarchical set of coefficients**.\n",
    "- Commonly used for **data compression** and **denoising**.\n",
    "- Efficient for **storage and reconstruction**.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Summary**:  \n",
    "Wavelet transforms offer a powerful way to analyze signals across **multiple time and frequency scales**, making them essential for **non-stationary signal processing** such as in **audio, EEG, image compression**, and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32fa086-93bf-4f8e-92ba-7d4b75d8e80b",
   "metadata": {},
   "source": [
    "# 🎛️ Demo 4: Continuous & Discrete Wavelet Transforms\n",
    "\n",
    "In this final demo, you’ll explore **two complementary wavelet-based analyses**:\n",
    "\n",
    "---\n",
    "\n",
    "## 🌊 Continuous Wavelet Transform (CWT)\n",
    "- Computes a **dense, redundant time–frequency representation** using a chosen *“mother” wavelet*.\n",
    "- Enables you to **zoom in on transients at fine scales** and **sustained oscillations at coarse scales**.\n",
    "\n",
    "## 📦 Discrete Wavelet Transform (DWT)\n",
    "- Decomposes the signal into a **compact, hierarchical set of coefficients** (approximation + detail bands).\n",
    "- Commonly used for **denoising** or **compression**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ What to Do\n",
    "\n",
    "Edit the `USER SETTINGS` at the top of the code cell:\n",
    "\n",
    "- **`FILENAME`**: Your audio file (`.wav` or `.mp3`) in the `sounds/` folder.\n",
    "- **`CWT_WAVELET`**: Name of the continuous wavelet (e.g. `'morl'`, `'mexh'`).\n",
    "- **`CWT_SCALES`**: List of CWT scales (integers ≥ 1) to analyze different resolutions.\n",
    "- **`DWT_WAVELET`**: Name of the discrete wavelet (e.g. `'db4'`, `'sym5'`).\n",
    "- **`DWT_LEVEL`**: Number of decomposition levels (integer ≥ 1).\n",
    "\n",
    "Then **run the cell**.\n",
    "\n",
    "---\n",
    "\n",
    "## ▶️ What It Does\n",
    "\n",
    "1. **Loads your audio clip** and plays the **original audio**.\n",
    "\n",
    "### 📉 CWT\n",
    "- Computes and displays a **scalogram**:  \n",
    "  - \\|coefficients\\| vs. time & pseudo-frequency on a **log scale**.\n",
    "\n",
    "### 📊 DWT\n",
    "- Performs a **multilevel wavelet decomposition**.\n",
    "- Plots the **energy** in each band (approximation + details).\n",
    "- **Reconstructs** the final-level approximation and plays it back.\n",
    "\n",
    "---\n",
    "\n",
    "## 👀 What to Observe\n",
    "\n",
    "### 🎼 CWT Scalogram\n",
    "- Observe how **high-scale (coarse)** coefficients capture **slow, low-frequency content**.\n",
    "- Notice how **low-scale (fine)** coefficients capture **fast transients**.\n",
    "\n",
    "### ⚡ DWT Energy Plot\n",
    "- Identify **which levels carry the most energy**:\n",
    "  - e.g., Level 0 = Coarse approximation  \n",
    "  - Higher levels = Finer detail bands\n",
    "\n",
    "### 🎧 DWT Reconstruction\n",
    "- Compare the **approximation-only audio** to the **original**.\n",
    "- Consider: *What details are lost when you exclude all detail bands?*\n",
    "\n",
    "---\n",
    "\n",
    "🧠 **Insight**: This hands-on comparison highlights how **wavelets provide flexible, multi-scale views** of audio — offering richer insights than fixed-resolution FFT methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cdfc8c-c5b9-430d-b1a0-9c43dbe30a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── USER SETTINGS ────────────────────────────────────────────────────────────────\n",
    "FILENAME      = 'cymbal.mp3'         # ← place your file in `sounds/` (WAV/MP3)\n",
    "CWT_WAVELET   = 'morl'               # ← continuous wavelet name (e.g. 'morl','mexh')\n",
    "CWT_SCALES    = list(range(1, 129))  # ← list of scales for CWT (integers ≥1)\n",
    "DWT_WAVELET   = 'db4'                # ← discrete wavelet name (e.g. 'db1','db4','sym5')\n",
    "DWT_LEVEL     = 4                    # ← decomposition level (integer ≥1)\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import pywt\n",
    "from IPython.display import Audio, display\n",
    "from pathlib import Path\n",
    "\n",
    "# ── CONFIG (don’t edit below here) ───────────────────────────────────────────────\n",
    "SOUNDS_DIR = Path('sounds')\n",
    "audio_path = SOUNDS_DIR / FILENAME\n",
    "\n",
    "# 1) Load audio\n",
    "y, sr = librosa.load(str(audio_path), sr=None)\n",
    "\n",
    "# 2) Play original\n",
    "print(\"▶️ Original Audio\")\n",
    "display(Audio(data=y, rate=sr, autoplay=False))\n",
    "\n",
    "# ── Continuous Wavelet Transform (CWT) ────────────────────────────────────────────\n",
    "coeffs, freqs = pywt.cwt(\n",
    "    y,\n",
    "    scales=CWT_SCALES,\n",
    "    wavelet=CWT_WAVELET,\n",
    "    sampling_period=1/sr\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(\n",
    "    np.abs(coeffs),\n",
    "    extent=[0, len(y)/sr, freqs[-1], freqs[0]],\n",
    "    aspect='auto',\n",
    "    cmap='viridis'\n",
    ")\n",
    "plt.yscale('log')\n",
    "plt.colorbar(label='Magnitude')\n",
    "plt.title(f'Continuous Wavelet Transform (wavelet={CWT_WAVELET})')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Pseudo-frequency (Hz)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ── Discrete Wavelet Transform (DWT) ─────────────────────────────────────────────\n",
    "coeff_list = pywt.wavedec(y, wavelet=DWT_WAVELET, level=DWT_LEVEL)\n",
    "\n",
    "# energies of each sub-band\n",
    "levels = list(range(len(coeff_list)))\n",
    "energies = [np.sum(c**2) for c in coeff_list]\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.stem(levels, energies, basefmt=\" \")\n",
    "plt.title(f'DWT Coefficients Energy (wavelet={DWT_WAVELET})')\n",
    "plt.xlabel('Level (0 = approximation)')\n",
    "plt.ylabel('Coefficient Energy')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3) Reconstruct & play final-level approximation\n",
    "print(f\"▶️ DWT Approximation (Level = {DWT_LEVEL})\")\n",
    "# rebuild only the approximation band at index 0 plus zeros for details\n",
    "y_dwt_approx = pywt.waverec(\n",
    "    [coeff_list[0]] + [None]*DWT_LEVEL,\n",
    "    wavelet=DWT_WAVELET\n",
    ")\n",
    "# trim/pad to original length\n",
    "y_dwt_approx = y_dwt_approx[:len(y)]\n",
    "display(Audio(data=y_dwt_approx, rate=sr, autoplay=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b1a6ae-e8b0-4d9a-b8bf-950d7e6fb866",
   "metadata": {},
   "source": [
    "# 🎼 Exercise: CQT-Based Octave Filter Bank\n",
    "\n",
    "## 🎯 Task\n",
    "Design and implement a **filter bank** using the **Constant-Q Transform (CQT)** to **isolate individual musical octaves** from a recording.\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 Steps\n",
    "\n",
    "1. **Compute the CQT** of an audio signal using  \n",
    "   ```python\n",
    "   librosa.cqt(...)\n",
    "   ```\n",
    "   ### ⚙️ Octave Band Processing Instructions\n",
    "\n",
    "- Use a suitable number of **bins per octave**  \n",
    "  *(e.g., `12` for semitone resolution).*\n",
    "\n",
    "---\n",
    "\n",
    "### 🎚️ Group the CQT Bins into Octave Bands:\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- **Bins 0–11** → Octave 1  \n",
    "- **Bins 12–23** → Octave 2  \n",
    "- **Bins 24–35** → Octave 3  \n",
    "- *(...and so on)*\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 For Each Octave Band:\n",
    "\n",
    "1. **Zero out all other CQT bins** not belonging to that octave.\n",
    "2. **Invert the modified CQT** back to the time domain using:\n",
    "\n",
    "```python\n",
    "librosa.icqt(...)\n",
    "```\n",
    "### ▶️ Playback or Export\n",
    "\n",
    "- **Play back** or **export** the **isolated octave audio** for analysis and comparison.\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 Deliverables\n",
    "\n",
    "- 📊 **Plot** of the **full CQT spectrogram** with **colored overlays or annotations** marking each **octave band**.\n",
    "\n",
    "- 🔊 A **set of audio players** (or exported `.wav` files) to **listen to each reconstructed octave band** in isolation.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Tip\n",
    "\n",
    "Use the following command to help **visualize note/octave positions** on the spectrogram:\n",
    "\n",
    "```python\n",
    "librosa.display.specshow(..., y_axis='cqt_note')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73b68db-a0a9-49ea-b6b8-489820e45b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
