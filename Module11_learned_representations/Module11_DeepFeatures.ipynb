{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3a88910-e051-45ed-b1ea-e6791c165c2e",
   "metadata": {},
   "source": [
    "## Module 11: Learned Representations & Deep Features\n",
    "\n",
    "In this module, we’ll move beyond hand-crafted descriptors and explore how deep neural networks learn powerful audio representations.\n",
    "\n",
    "### Key Concepts\n",
    "- **Autoencoders & CNNs on Spectrograms**  \n",
    "  - Use convolutional architectures to compress and reconstruct spectrograms (autoencoders)  \n",
    "  - Learn hierarchical time–frequency features directly from data  \n",
    "- **Pre-trained Models**  \n",
    "  - VGGish, Wav2Vec2, YAMNet, etc. provide off-the-shelf embeddings  \n",
    "  - Leverage large-scale, pre-trained audio networks for downstream tasks  \n",
    "- **Transfer Learning Strategies**  \n",
    "  - Feature extraction: freeze a base model and train a shallow classifier on top  \n",
    "  - Fine-tuning: unfreeze some layers to adapt representations to your domain  \n",
    "\n",
    "---\n",
    "\n",
    "### 📓 Notebook Demos\n",
    "\n",
    "1. **Wav2Vec2 Embedding Visualization**  \n",
    "   - Load a pre-trained **Facebook Wav2Vec2** audio transformer  \n",
    "   - Compute 768-dim frame-level embeddings for each clip, then average to clip-level  \n",
    "   - Project to 2D with **PCA** or **t-SNE** and display an interactive scatter colored by genre/speaker  \n",
    "\n",
    "2. **CNN Fine-Tuning on Commands vs. Noise**  \n",
    "   - Build a simple spectrogram-based CNN in PyTorch or TensorFlow  \n",
    "   - Freeze early layers of a pre-trained backbone (e.g. VGGish)  \n",
    "   - Fine-tune last few layers on a small “voice commands vs. background noise” dataset  \n",
    "   - Monitor training/validation accuracy live in the notebook  \n",
    "\n",
    "---\n",
    "\n",
    "### 🛠 Exercise: Deep Feature Comparison  \n",
    "- **Task:**  \n",
    "  Extract deep embeddings (e.g. from Wav2Vec2, VGGish, or YAMNet) and train a classifier (random forest or small MLP) on your own labeled dataset.  \n",
    "- **Analysis:**  \n",
    "  Compare classification accuracy and confusion matrices against a baseline using hand-crafted features (MFCCs + spectral features).  \n",
    "- **Deliverables:**  \n",
    "  - Code to load the pre-trained model and extract embeddings  \n",
    "  - Classification scripts for both deep and hand-crafted feature sets  \n",
    "  - A performance report discussing which representation performs better—and why  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078522bd-6316-4d42-80bf-c311e7663180",
   "metadata": {},
   "source": [
    "### Key Concepts\n",
    "\n",
    "- **Autoencoders & CNNs on Spectrograms**  \n",
    "  - Use convolutional architectures to compress and reconstruct spectrograms (autoencoders).  \n",
    "  - Learn hierarchical time–frequency features directly from raw audio data.\n",
    "\n",
    "- **Pre-trained Models**  \n",
    "  - VGGish, OpenL3, YAMNet, etc., provide off-the-shelf audio embeddings.  \n",
    "  - Leverage representations learned on large-scale datasets for your own tasks.\n",
    "\n",
    "- **Transfer Learning Strategies**  \n",
    "  - **Feature Extraction:** Freeze the base model’s weights and train a shallow classifier on the extracted embeddings.  \n",
    "  - **Fine-Tuning:** Unfreeze one or more layers of the pre-trained network to adapt its representations to your specific domain.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40b1360-8584-4d7e-afef-36a2e21524f6",
   "metadata": {},
   "source": [
    "## Demo 1: Deep Audio Embedding Visualization\n",
    "\n",
    "In this demo we use a **pre-trained Wav2Vec2** model (from Facebook AI) to extract semantically rich embeddings from raw audio, then project them to 2D for visualization.\n",
    "\n",
    "**What you’ll learn:**  \n",
    "- How to load and run a pre-trained audio transformer  \n",
    "- How to average frame‐level embeddings into clip‐level vectors  \n",
    "- How to compare PCA vs. t-SNE for visualizing high-dim representations  \n",
    "\n",
    "---\n",
    "\n",
    "### How to use\n",
    "\n",
    "1. **Edit the USER SETTINGS** at the top of the code cell:  \n",
    "   - `FILES`: list of `(Label, Filename)` pairs in `sounds/`  \n",
    "   - `MODEL_NAME`: Hugging Face model name (e.g. `'facebook/wav2vec2-base-960h'`)  \n",
    "\n",
    "2. **Run the cell** to:  \n",
    "   - Load each clip (resampling to 16 kHz as required)  \n",
    "   - Compute 768-dim Wav2Vec2 embeddings, averaged over time  \n",
    "   - Reduce to 2D with either **PCA** or **t-SNE**  \n",
    "   - Plot an interactive scatter (Plotly) colored by label  \n",
    "\n",
    "---\n",
    "\n",
    "### What to observe\n",
    "\n",
    "- **Cluster formation:** Groups of points indicate that the embedder captures shared characteristics (genre, speaker, instrument, etc.).  \n",
    "- **Separation vs. overlap:** PCA may give a rough separation; t-SNE can reveal finer local structure at the cost of global geometry.  \n",
    "- **Model choice:** You can swap in other pre-trained audio models (YAMNet, TRILL, OpenL3 if installable) by changing `MODEL_NAME` and the forward pass.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e25b03-f1bf-4b53-994d-a24839218730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── USER SETTINGS ────────────────────────────────────────────────────────────────\n",
    "# List your clips and labels here; place files in `sounds/`\n",
    "FILES = [\n",
    "    ('Classical', 'beethoven-symphony-no-7-2nd-movement-246122.mp3'),\n",
    "    ('Jazz',      'sax-jazz-77053.mp3'),\n",
    "    ('Rock',      'rock-music-6211.mp3'),\n",
    "    ('Speech',    'speech.WAV'),\n",
    "    # … add more (label, filename) pairs as desired …\n",
    "]\n",
    "MODEL_NAME = 'facebook/wav2vec2-base-960h'   # ← Hugging Face Wav2Vec2 model\n",
    "USE_TSNE   = False                           # ← False→PCA, True→t-SNE\n",
    "TSNE_PERP  = 5                               # ← t-SNE perplexity (5–min(n_samples−1))\n",
    "TSNE_ITER  = 1000                            # ← t-SNE iterations\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "\n",
    "# Load processor & model\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "model     = Wav2Vec2Model.from_pretrained(MODEL_NAME)\n",
    "\n",
    "SOUNDS_DIR = Path('sounds')\n",
    "\n",
    "def load_and_prepare(path):\n",
    "    y, sr = sf.read(str(path), dtype='float32') if path.suffix.lower()=='.wav' \\\n",
    "           else librosa.load(str(path), sr=None)\n",
    "    if y.ndim>1:    # stereo → mono\n",
    "        y = y.mean(axis=1)\n",
    "    if sr != 16000:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=16000)\n",
    "        sr = 16000\n",
    "    return y, sr\n",
    "\n",
    "# 1) Compute embeddings\n",
    "embeddings, labels = [], []\n",
    "for label, fname in FILES:\n",
    "    path = SOUNDS_DIR / fname\n",
    "    y, sr = load_and_prepare(path)\n",
    "    inputs = processor(y, sampling_rate=sr, return_tensors='pt', padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # mean over time → clip-level vector\n",
    "    emb = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "    embeddings.append(emb)\n",
    "    labels.append(label)\n",
    "\n",
    "X = np.stack(embeddings)  # (n_clips, 768)\n",
    "\n",
    "# 2) Reduce to 2D\n",
    "if USE_TSNE:\n",
    "    perp = min(TSNE_PERP, X.shape[0]-1)\n",
    "    dr = TSNE(n_components=2, perplexity=perp, n_iter=TSNE_ITER, init='random', random_state=0)\n",
    "    title = f\"t-SNE (perp={perp})\"\n",
    "else:\n",
    "    dr = PCA(n_components=2, random_state=0)\n",
    "    title = \"PCA (2D)\"\n",
    "X2 = dr.fit_transform(X)\n",
    "\n",
    "# 3) Interactive scatter\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'Dim1': X2[:,0],\n",
    "    'Dim2': X2[:,1],\n",
    "    'Label': labels\n",
    "})\n",
    "fig = px.scatter(\n",
    "    df, x='Dim1', y='Dim2',\n",
    "    color='Label', symbol='Label',\n",
    "    title=f\"{title} of Wav2Vec2 Embeddings\",\n",
    "    width=700, height=500\n",
    ")\n",
    "fig.update_layout(xaxis_title='Component 1', yaxis_title='Component 2')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821df314-d1e7-4c70-865a-378d211f1e1d",
   "metadata": {},
   "source": [
    "# 🧠 Demo 2: CNN Fine-Tuning on Voice Commands vs. Background Noise\n",
    "\n",
    "In this demo, you’ll build a **spectrogram-based classifier** by repurposing an **ImageNet-pretrained MobileNetV2** backbone.\n",
    "\n",
    "---\n",
    "\n",
    "## 📥 Data Loading & Preprocessing\n",
    "\n",
    "### 🔧 Inputs (Edit at Top)\n",
    "\n",
    "- **`COMMANDS_DIR` / `NOISE_DIR`**: Folders containing your **voice commands** and **background noise** files (`.wav` or `.mp3`)\n",
    "- **`SAMPLE_RATE`** (e.g., `16000 Hz`): Target resampling rate for all clips\n",
    "- **`DURATION`** (seconds): Clips are padded or truncated to this exact length\n",
    "- **`N_MELS`**, **`FFT_WINDOW`**, **`HOP_LENGTH`**: Mel-spectrogram extraction parameters\n",
    "- **`BATCH_SIZE`**, **`EPOCHS`**, **`LEARNING_RATE`**: Training hyperparameters\n",
    "\n",
    "### 🧪 Process\n",
    "\n",
    "1. **Read each file** and **resample** to `SAMPLE_RATE`\n",
    "2. **Pad or truncate** to match `DURATION`\n",
    "3. **Compute log-Mel spectrogram** with `N_MELS` bands\n",
    "4. **Normalize** to zero-mean / unit-variance\n",
    "5. **Add a channel axis** for CNN input\n",
    "\n",
    "---\n",
    "\n",
    "## 🧱 Dataset Construction\n",
    "\n",
    "- Builds a single `tf.data.Dataset` of **(spectrogram, label)** pairs\n",
    "- **Splits 80/20** into **training vs. validation** sets\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Model Architecture\n",
    "\n",
    "- **Input**: (`N_MELS × time_frames × 1`) Mel-spectrogram\n",
    "- **Channel Replication**: Stack to 3 channels for ImageNet CNN input\n",
    "- **Resizing**: `layers.Resizing(224, 224)` to match **MobileNetV2** input size\n",
    "- **Backbone**: `MobileNetV2(include_top=False, weights='imagenet')` — **frozen**\n",
    "- **Head**:\n",
    "  - Global Average Pooling\n",
    "  - `Dropout(0.3)`\n",
    "  - `Dense(1, activation='sigmoid')`\n",
    "\n",
    "---\n",
    "\n",
    "## 🏋️‍♂️ Training\n",
    "\n",
    "- **Loss**: Binary cross-entropy\n",
    "- **Optimizer**: Adam with `LEARNING_RATE`\n",
    "- **Metrics**: Accuracy\n",
    "- Train for `EPOCHS`, monitoring:\n",
    "  - Training loss/accuracy\n",
    "  - Validation loss/accuracy\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Outputs to Observe\n",
    "\n",
    "### 📈 Training Curves\n",
    "\n",
    "- **Loss Plot**:\n",
    "  - Training vs. Validation loss\n",
    "  - Ensures the model is learning and not overfitting\n",
    "\n",
    "- **Accuracy Plot**:\n",
    "  - Training vs. Validation accuracy\n",
    "  - Gauges classification performance\n",
    "\n",
    "### ✅ Final Validation Accuracy\n",
    "\n",
    "- A quick summary of how well the fine-tuned CNN distinguishes **voice commands** from **background noise**\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Interpretation Tips\n",
    "\n",
    "- If **validation accuracy lags training**:\n",
    "  - Try **unfreezing more backbone layers**\n",
    "  - Add **regularization** (e.g., dropout, weight decay)\n",
    "\n",
    "- If **both train & val accuracy are low**:\n",
    "  - Your dataset may need **more examples**\n",
    "  - Or consider using a **custom CNN architecture**\n",
    "\n",
    "- 📏 **Note on Resizing**:\n",
    "  - The `224×224` resizing **trades time–frequency resolution** for compatibility with the pretrained model\n",
    "  - Alternatives include using **custom CNNs** that accept native spectrogram sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1547b473-efcc-4220-847f-e1b07cbb9afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── USER SETTINGS ────────────────────────────────────────────────────────────────\n",
    "COMMANDS_DIR   = 'sounds/commands'     # ← folder containing your “voice commands” WAV/MP3 files\n",
    "NOISE_DIR      = 'sounds/noise'        # ← folder containing your “background noise” WAV/MP3 files\n",
    "SAMPLE_RATE    = 16000                 # ← target sample rate for all audio\n",
    "DURATION       = 1.0                   # ← clip length in seconds (pad/truncate)\n",
    "N_MELS         = 64                    # ← number of mel bands for spectrograms\n",
    "FFT_WINDOW     = 512                   # ← FFT window size (samples)\n",
    "HOP_LENGTH     = 256                   # ← hop between frames (samples)\n",
    "BATCH_SIZE     = 16                    # ← batch size for training/validation\n",
    "EPOCHS         = 10                    # ← number of training epochs\n",
    "LEARNING_RATE  = 1e-4                  # ← optimizer learning rate\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models, applications, optimizers\n",
    "\n",
    "# ── DATA LOADING & PREPROCESSING ────────────────────────────────────────────────\n",
    "\n",
    "def preprocess_file(path):\n",
    "    \"\"\"\n",
    "    path may be a Python str or tf.Tensor of type string.\n",
    "    We convert it to a native str before loading with librosa.\n",
    "    \"\"\"\n",
    "    # If tf.Tensor, extract Python value\n",
    "    if hasattr(path, \"numpy\"):\n",
    "        path = path.numpy()\n",
    "    if isinstance(path, bytes):\n",
    "        filepath = path.decode(\"utf-8\")\n",
    "    else:\n",
    "        filepath = str(path)\n",
    "    # 1) Load & resample\n",
    "    wav, sr = librosa.load(filepath, sr=None)\n",
    "    if sr != SAMPLE_RATE:\n",
    "        wav = librosa.resample(wav, orig_sr=sr, target_sr=SAMPLE_RATE)\n",
    "    # 2) Pad or truncate to exactly DURATION seconds\n",
    "    target_len = int(SAMPLE_RATE * DURATION)\n",
    "    if wav.shape[0] < target_len:\n",
    "        wav = np.pad(wav, (0, target_len - wav.shape[0]))\n",
    "    else:\n",
    "        wav = wav[:target_len]\n",
    "    # 3) Compute log-mel spectrogram (disable centering to keep frame count fixed)\n",
    "    mel = librosa.feature.melspectrogram(\n",
    "        y=wav,\n",
    "        sr=SAMPLE_RATE,\n",
    "        n_fft=FFT_WINDOW,\n",
    "        hop_length=HOP_LENGTH,\n",
    "        n_mels=N_MELS,\n",
    "        center=False\n",
    "    )\n",
    "    log_mel = np.log(mel + 1e-6).astype(np.float32)\n",
    "    # 4) Normalize to zero mean / unit variance\n",
    "    log_mel = (log_mel - log_mel.mean()) / (log_mel.std() + 1e-6)\n",
    "    # 5) Add channel dimension\n",
    "    return np.expand_dims(log_mel, -1)\n",
    "\n",
    "def tf_preprocess(path, label):\n",
    "    # Wrap our preprocessing in tf.py_function\n",
    "    spec = tf.py_function(preprocess_file, inp=[path], Tout=tf.float32)\n",
    "    # We know the output shape exactly\n",
    "    time_frames = int(np.floor((SAMPLE_RATE*DURATION - FFT_WINDOW) / HOP_LENGTH)) + 1\n",
    "    spec.set_shape([N_MELS, time_frames, 1])\n",
    "    return spec, label\n",
    "\n",
    "# Gather file lists\n",
    "cmd_wavs  = glob.glob(os.path.join(COMMANDS_DIR, '*.wav'))\n",
    "cmd_mp3s  = glob.glob(os.path.join(COMMANDS_DIR, '*.mp3'))\n",
    "cmd_files = cmd_wavs + cmd_mp3s\n",
    "noise_wavs  = glob.glob(os.path.join(NOISE_DIR, '*.wav'))\n",
    "noise_mp3s  = glob.glob(os.path.join(NOISE_DIR, '*.mp3'))\n",
    "noise_files = noise_wavs + noise_mp3s\n",
    "\n",
    "if not cmd_files:\n",
    "    raise ValueError(f\"No audio files found in {COMMANDS_DIR}\")\n",
    "if not noise_files:\n",
    "    raise ValueError(f\"No audio files found in {NOISE_DIR}\")\n",
    "\n",
    "# Compute train/validation split counts\n",
    "total_samples = len(cmd_files) + len(noise_files)\n",
    "train_count   = int(0.8 * total_samples)\n",
    "\n",
    "# Build tf.data.Dataset of (filepath, label)\n",
    "ds_cmd   = tf.data.Dataset.from_tensor_slices((cmd_files, [1]*len(cmd_files)))\n",
    "ds_noise = tf.data.Dataset.from_tensor_slices((noise_files, [0]*len(noise_files)))\n",
    "dataset  = ds_cmd.concatenate(ds_noise)\n",
    "dataset  = dataset.shuffle(total_samples, reshuffle_each_iteration=True)\n",
    "dataset  = dataset.map(tf_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Split into train/validation\n",
    "train_ds = dataset.take(train_count).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds   = dataset.skip(train_count).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# ── MODEL DEFINITION ─────────────────────────────────────────────────────────────\n",
    "from tensorflow.keras import layers, models, applications, optimizers\n",
    "\n",
    "# ── MODEL DEFINITION ─────────────────────────────────────────────────────────────\n",
    "\n",
    "# Compute time_frames again for model input shape\n",
    "time_frames = int(np.floor((SAMPLE_RATE*DURATION - FFT_WINDOW) / HOP_LENGTH)) + 1\n",
    "input_shape = (N_MELS, time_frames, 1)\n",
    "\n",
    "inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "# 1) Convert 1-channel → 3-channel\n",
    "x = layers.Concatenate()([inputs, inputs, inputs])   # now shape = (None, N_MELS, time_frames, 3)\n",
    "\n",
    "# 2) Resize to 224×224 so we match MobileNetV2’s expected input\n",
    "x = layers.Resizing(224, 224)(x)                     # now shape = (None, 224, 224, 3)\n",
    "\n",
    "# 3) Pre-trained backbone\n",
    "backbone = applications.MobileNetV2(\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "backbone.trainable = False\n",
    "\n",
    "x = backbone(x, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = models.Model(inputs, outputs)\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(LEARNING_RATE),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "# ── TRAINING ─────────────────────────────────────────────────────────────────────\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "# ── PLOTTING TRAIN/VAL METRICS ────────────────────────────────────────────────────\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'],     label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['accuracy'],     label='train acc')\n",
    "plt.plot(history.history['val_accuracy'], label='val acc')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9589cc-13c7-4e17-99ea-b67567e3d726",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🛠 Exercise: Deep Feature Comparison\n",
    "\n",
    "### 🎯 Task  \n",
    "Extract **deep audio embeddings** (e.g., from **Wav2Vec2**, **VGGish**, or **YAMNet**) and train a classifier (**Random Forest** or a small **MLP**) on your own **labeled audio dataset**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Analysis  \n",
    "Compare the **classification performance** of deep features vs. a **baseline model using hand-crafted features** like MFCCs and spectral descriptors.\n",
    "\n",
    "---\n",
    "\n",
    "### 📦 Deliverables\n",
    "\n",
    "#### ✅ Feature Extraction Code\n",
    "- Load a **pretrained model** (e.g., Wav2Vec2, VGGish, YAMNet)\n",
    "- Extract **deep embeddings** from each audio clip\n",
    "\n",
    "#### ✅ Classification Scripts\n",
    "\n",
    "1. **Deep Embedding Pipeline**\n",
    "   - Use embeddings + **Random Forest (RF)** or **Multi-Layer Perceptron (MLP)**\n",
    "\n",
    "2. **Hand-Crafted Feature Pipeline**\n",
    "   - Use features like:\n",
    "     - MFCCs\n",
    "     - Spectral centroid\n",
    "     - Bandwidth\n",
    "   - Train using the same **RF or MLP** classifier\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Performance Report\n",
    "\n",
    "- **Metrics**:\n",
    "  - Accuracy\n",
    "  - Precision / Recall / F₁-score\n",
    "\n",
    "- **Confusion Matrices**:\n",
    "  - For both **deep** and **hand-crafted** models\n",
    "  - Analyze **error patterns**\n",
    "\n",
    "- **Discussion**:\n",
    "  - Which feature representation performed **best**?\n",
    "  - Provide **hypotheses** to explain your observations\n",
    "\n",
    "---\n",
    "\n",
    "💡 *Tip*: Use dimensionality reduction (e.g., PCA or t-SNE) on embeddings to **visualize feature separability**!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e9d5a-94e2-4c4f-b9f0-1ab04b17dcb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
