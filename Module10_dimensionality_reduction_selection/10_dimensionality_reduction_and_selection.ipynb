{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f060dba5-4123-42ea-8357-39afe8ffb150",
   "metadata": {},
   "source": [
    "## Module 10: Dimensionality Reduction & Selection\n",
    "\n",
    "In this module, we’ll tackle methods for reducing and selecting features to combat high dimensionality, improve visualization, and prevent overfitting.\n",
    "\n",
    "### Key Concepts\n",
    "- **Principal Component Analysis (PCA)**  \n",
    "  Linear projection that captures maximum variance in orthogonal directions.\n",
    "- **Nonlinear Embeddings: t-SNE & UMAP**  \n",
    "  Manifold learning techniques for visualizing high-dimensional data in 2D/3D.\n",
    "- **Feature Selection**  \n",
    "  - **Mutual Information**: measure dependency between features and labels  \n",
    "  - **Variance Thresholding**: remove near-constant features  \n",
    "- **Overfitting & Curse of Dimensionality**  \n",
    "  Why too many features can degrade model performance and generalization.\n",
    "\n",
    "---\n",
    "\n",
    "### 📓 Notebook Demos\n",
    "\n",
    "1. **3D PCA Scatter of Instrument Embeddings**  \n",
    "   - Compute feature vectors (e.g., MFCC means) for various instruments  \n",
    "   - Perform PCA to reduce to 3 components  \n",
    "   - Plot interactive 3D scatter, colored by instrument class, with rotation controls  \n",
    "\n",
    "2. **Comparing UMAP vs. t-SNE on MFCC Vectors**  \n",
    "   - Collect MFCC feature vectors for a dataset of audio clips  \n",
    "   - Compute 2D projections using UMAP and t-SNE  \n",
    "   - Display side-by-side scatter plots to compare clustering and neighborhood structure  \n",
    "\n",
    "---\n",
    "\n",
    "### 🛠 Exercise: 2D Embedding & Clustering\n",
    "\n",
    "- **Task:**  \n",
    "  Take a 40-dimensional feature representation of environmental sounds and reduce it to 2 dimensions using your choice of PCA, t-SNE, or UMAP.  \n",
    "\n",
    "- **Steps:**  \n",
    "  1. Load or compute a 40-dim feature matrix (e.g., MFCC mean/variance, spectral contrast).  \n",
    "  2. Apply dimensionality reduction to 2D.  \n",
    "  3. Cluster the 2D points (e.g., K-Means) and visualize cluster assignments.  \n",
    "\n",
    "- **Deliverables:**  \n",
    "  - Plots of the 2D embedding with clusters labeled  \n",
    "  - Quantitative evaluation (e.g., silhouette score) comparing methods  \n",
    "  - Brief discussion of which technique best preserves class structure  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d218b4-dc20-4fe7-90e8-e1480531df5d",
   "metadata": {},
   "source": [
    "### Key Concepts\n",
    "\n",
    "- **Principal Component Analysis (PCA)**  \n",
    "  A linear dimensionality-reduction technique that finds orthogonal directions (principal components) capturing the maximum variance in your data. Useful for compressing high-dimensional features while preserving as much “energy” (variance) as possible.\n",
    "\n",
    "- **Nonlinear Embeddings: t-SNE & UMAP**  \n",
    "  Manifold learning methods that map high-dimensional data into 2D or 3D for visualization:  \n",
    "  - **t-SNE** focuses on preserving local neighborhood structure (clusters).  \n",
    "  - **UMAP** preserves both local and some global structure and tends to run faster on large datasets.\n",
    "\n",
    "- **Feature Selection**  \n",
    "  Techniques to pick the most informative features and discard the rest:  \n",
    "  - **Mutual Information**  \n",
    "    Quantifies the dependency between each feature and the target labels; higher MI ⇒ more predictive.  \n",
    "  - **Variance Thresholding**  \n",
    "    Removes features whose variance falls below a chosen threshold (near-constant features carry little information).\n",
    "\n",
    "- **Overfitting & Curse of Dimensionality**  \n",
    "  - **Overfitting:** Too many features relative to data points can cause models to “memorize” noise instead of learning generalizable patterns.  \n",
    "  - **Curse of Dimensionality:** As dimensionality grows, the volume of feature space expands exponentially, making data sparse and distance metrics less meaningful.\n",
    "\n",
    "Understanding and mitigating these issues is critical for building robust, high-performing audio-analysis pipelines.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6c1c95-319a-451d-ac35-d4513d28aee4",
   "metadata": {},
   "source": [
    "## Demo: 3D PCA Scatter of Instrument Embeddings\n",
    "\n",
    "In this demo you’ll see how simple timbral features (mean MFCCs) can be embedded in a low-dimensional space and visualized in 3D to reveal clustering by instrument.\n",
    "\n",
    "**What the code does:**\n",
    "1. **Loads** each audio clip from the `sounds/` folder, using the `FILES` list of `(Label, Filename)` pairs.\n",
    "2. **Extracts** Mel-frequency cepstral coefficients (MFCCs) from each clip:\n",
    "   - Computes `N_MFCC` coefficients per frame with an `N_FFT`-point FFT and `HOP_LENGTH` hop.\n",
    "   - Averages each coefficient across time to form a single feature vector per clip.\n",
    "3. **Stacks** these feature vectors into a matrix `X` of shape `(n_clips, N_MFCC)`.\n",
    "4. **Performs PCA** to reduce `X` to 3 principal components.\n",
    "5. **Plots** an interactive 3D scatter (via Plotly) of the resulting coordinates, coloring and symbolizing points by instrument label.\n",
    "\n",
    "**USER SETTINGS** (edit these at the top of the code cell):\n",
    "- `FILES`: list of `(Instrument Label, Filename)` tuples.  \n",
    "  - Filenames must exist in `sounds/` (supported formats: WAV, MP3).  \n",
    "- `N_MFCC`: integer ≥ 1, number of MFCC coefficients to extract (e.g. 13).  \n",
    "- `N_FFT`: FFT window size (power of two, e.g. 1024, 2048).  \n",
    "- `HOP_LENGTH`: hop size in samples (≤ `N_FFT`, e.g. `N_FFT // 4`).\n",
    "\n",
    "**What to observe:**\n",
    "- **Clusters**: Instruments with similar timbral characteristics (MFCC means) will appear close together.  \n",
    "- **Separation**: Distinct timbres form well‐separated groups in 3D space.  \n",
    "- **Axes**: PC1–PC3 capture the directions of greatest variance in the feature set.\n",
    "\n",
    "**How to interpret:**\n",
    "- Points that cluster by label indicate that mean-MFCC features differentiate those instruments.  \n",
    "- Interactive rotation helps you explore which PCs best separate particular instruments.  \n",
    "- Consider adding or removing instruments or adjusting `N_MFCC` to see how the embedding changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784b5c5e-bfe1-465b-a2cb-659eb55a6ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── USER SETTINGS ────────────────────────────────────────────────────────────────\n",
    "# List your instrument clips and labels here. Place files in the `sounds/` folder.\n",
    "FILES = [\n",
    "    ('Violin',   'violin-loop-154193.mp3'),\n",
    "    ('Flute',    'flute-rain-flute-loop-ambient-short-loop-340800.mp3'),\n",
    "    ('Piano',    'soft-piano-100-bpm-121529.mp3'),\n",
    "    ('Trumpet',  'trumpet-75426.mp3'),\n",
    "    # add more as desired...\n",
    "]\n",
    "\n",
    "N_MFCC     = 13        # ← number of MFCC coefficients to extract\n",
    "N_FFT      = 2048      # ← FFT window size (power of 2)\n",
    "HOP_LENGTH = N_FFT // 4  # ← hop length between frames\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "\n",
    "# ── CONFIG (don’t edit below here) ───────────────────────────────────────────────\n",
    "SOUNDS_DIR = Path('sounds')\n",
    "\n",
    "# 1) Load each clip, extract mean MFCC feature vector\n",
    "feature_list = []\n",
    "labels       = []\n",
    "\n",
    "for label, fname in FILES:\n",
    "    path = SOUNDS_DIR / fname\n",
    "    ext  = path.suffix.lower()\n",
    "    if ext == '.wav':\n",
    "        y, sr = sf.read(str(path), dtype='float32')\n",
    "    else:\n",
    "        y, sr = librosa.load(str(path), sr=None)\n",
    "    # if stereo, take left channel\n",
    "    if y.ndim > 1:\n",
    "        y = y[:,0]\n",
    "    \n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y=y, sr=sr,\n",
    "        n_mfcc=N_MFCC,\n",
    "        n_fft=N_FFT,\n",
    "        hop_length=HOP_LENGTH\n",
    "    )\n",
    "    # mean over time frames\n",
    "    feat = mfcc.mean(axis=1)\n",
    "    feature_list.append(feat)\n",
    "    labels.append(label)\n",
    "\n",
    "X = np.stack(feature_list)  # shape: (n_clips, N_MFCC)\n",
    "\n",
    "# 2) PCA to 3 components\n",
    "pca = PCA(n_components=3)\n",
    "coords = pca.fit_transform(X)\n",
    "\n",
    "# 3) Build a DataFrame for plotting\n",
    "df = pd.DataFrame({\n",
    "    'PC1': coords[:,0],\n",
    "    'PC2': coords[:,1],\n",
    "    'PC3': coords[:,2],\n",
    "    'Instrument': labels\n",
    "})\n",
    "\n",
    "# 4) Interactive 3D scatter plot\n",
    "fig = px.scatter_3d(\n",
    "    df,\n",
    "    x='PC1', y='PC2', z='PC3',\n",
    "    color='Instrument',\n",
    "    symbol='Instrument',\n",
    "    title='3D PCA of Instrument MFCC Embeddings',\n",
    "    width=800, height=600\n",
    ")\n",
    "fig.update_layout(scene=dict(\n",
    "    xaxis_title='PC1',\n",
    "    yaxis_title='PC2',\n",
    "    zaxis_title='PC3'\n",
    "))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871b4bb3-e74d-4b8e-a325-b3db026442f0",
   "metadata": {},
   "source": [
    "# 📊 Demo: Comparing t-SNE vs. UMAP on MFCC Embeddings\n",
    "\n",
    "In this demo, you’ll **project high-dimensional MFCC feature vectors** of audio clips down to 2D using two popular manifold learning techniques—**t-SNE** and **UMAP**—and compare how each preserves **neighborhood structure**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 What the Code Does\n",
    "\n",
    "### 🎼 Load & Feature Extraction\n",
    "- Iterates through your list of `(label, filename)` pairs in `FILES`\n",
    "- Loads each clip:\n",
    "  - `.wav` via `soundfile`\n",
    "  - Other formats via `librosa`\n",
    "  - Takes **first channel** if stereo\n",
    "- Computes **MFCCs** with:\n",
    "  - `N_MFCC` coefficients\n",
    "  - `N_FFT` window size\n",
    "  - `HOP_LENGTH` frame spacing\n",
    "- Averages MFCCs **over time** to get one **feature vector per clip**\n",
    "\n",
    "---\n",
    "\n",
    "### 📉 t-SNE Projection\n",
    "- **Caps `perplexity`** to be at most `n_samples - 1`\n",
    "- Runs t-SNE with:\n",
    "  - `perplexity = perp_capped`\n",
    "  - `max_iter = TSNE_N_ITER`\n",
    "  - Random initialization\n",
    "- Produces **2D coordinates**\n",
    "\n",
    "---\n",
    "\n",
    "### 🌐 UMAP Projection\n",
    "- Runs UMAP with:\n",
    "  - `n_neighbors = UMAP_N_NEIGHBORS`\n",
    "  - `min_dist = UMAP_MIN_DIST`\n",
    "  - `metric = UMAP_METRIC`\n",
    "- Produces another **2D projection**\n",
    "\n",
    "---\n",
    "\n",
    "### 🖼 Visualization\n",
    "- Creates **side-by-side scatter plots**:\n",
    "  - One for **t-SNE**\n",
    "  - One for **UMAP**\n",
    "- Points are **colored and marked** by **instrument label**\n",
    "- Lets you compare how well each algorithm **clusters similar sounds**\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Inputs (Edit at Top of the Code Cell)\n",
    "\n",
    "- **`FILES`**: List of `(label, filename)` tuples (audio files in `sounds/`)\n",
    "- **`N_MFCC`**: Number of MFCC coefficients (e.g., `13–40`)\n",
    "- **`N_FFT`**: FFT window size (e.g., `1024`, `2048`)\n",
    "- **`HOP_LENGTH`**: Hop size between frames (e.g., `N_FFT // 4`)\n",
    "- **`TSNE_PERPLEXITY`**: t-SNE perplexity (must be < number of clips, e.g., `5–50`)\n",
    "- **`TSNE_N_ITER`**: t-SNE iterations (e.g., `250–2000`)\n",
    "- **`UMAP_N_NEIGHBORS`**: UMAP neighborhood size (e.g., `5–50`)\n",
    "- **`UMAP_MIN_DIST`**: UMAP minimum distance (`0.0–0.5`)\n",
    "- **`UMAP_METRIC`**: Distance metric for UMAP (e.g., `'euclidean'`, `'cosine'`)\n",
    "\n",
    "---\n",
    "\n",
    "## 📤 Outputs to Observe\n",
    "\n",
    "### 🖼 Scatter Plots: t-SNE vs. UMAP\n",
    "- Look for **tight clusters** by instrument label\n",
    "- **t-SNE** may form **local “islands”**\n",
    "- **UMAP** often preserves **global layout and continuity**\n",
    "\n",
    "### 🏷 Plot Titles Show:\n",
    "- **t-SNE**: Final capped perplexity value\n",
    "- **UMAP**: Neighborhood size and minimum distance used\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 How to Interpret\n",
    "\n",
    "### 🎯 Cluster Cohesion\n",
    "- Are **similar instruments grouped closely**?\n",
    "\n",
    "### ✂️ Separation\n",
    "- Can you **visually separate different classes**?\n",
    "\n",
    "---\n",
    "\n",
    "### 🌍 Global vs. Local Behavior\n",
    "\n",
    "- **t-SNE**:\n",
    "  - Excels at **local cluster detail**\n",
    "  - May distort **global relationships**\n",
    "\n",
    "- **UMAP**:\n",
    "  - Balances **local and global structure**\n",
    "  - Often clearer global layout\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 Parameter Effects\n",
    "\n",
    "- **Higher perplexity** → t-SNE considers more neighbors → smoother, more global structure\n",
    "- **Larger n_neighbors** → UMAP includes broader context → more global cohesion\n",
    "- **Smaller min_dist** → UMAP clusters points more tightly\n",
    "\n",
    "---\n",
    "\n",
    "💡 **Experiment**:  \n",
    "Try tweaking `TSNE_PERPLEXITY`, `UMAP_N_NEIGHBORS`, and `UMAP_MIN_DIST`, then re-run the cell to observe how **projection quality and cluster layout** change!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb47d2a-8193-46aa-b5da-82371a028340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# … your USER SETTINGS above …\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Helper to load audio …\n",
    "def load_audio(path, sr=None):\n",
    "    ext = path.suffix.lower()\n",
    "    if ext == '.wav':\n",
    "        y, sr_native = sf.read(str(path), dtype='float32')\n",
    "        return (y, sr_native) if sr is None else (librosa.resample(y, sr_native, sr), sr)\n",
    "    else:\n",
    "        return librosa.load(str(path), sr=sr)\n",
    "\n",
    "# 1) Extract mean‐MFCCs\n",
    "feature_list = []\n",
    "labels       = []\n",
    "SOUNDS_DIR   = Path('sounds')\n",
    "for label, fname in FILES:\n",
    "    y, sr = load_audio(SOUNDS_DIR / fname, sr=None)\n",
    "    if y.ndim > 1: y = y[:,0]\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr,\n",
    "                                n_mfcc=N_MFCC,\n",
    "                                n_fft=N_FFT,\n",
    "                                hop_length=HOP_LENGTH)\n",
    "    feature_list.append(mfcc.mean(axis=1))\n",
    "    labels.append(label)\n",
    "X = np.stack(feature_list)\n",
    "\n",
    "# 2) t-SNE projection (cap perplexity < n_samples)\n",
    "n_samples    = X.shape[0]\n",
    "perp_capped  = min(TSNE_PERPLEXITY, n_samples - 1)\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=perp_capped,\n",
    "    max_iter=TSNE_N_ITER,    # renamed from n_iter\n",
    "    init='random',\n",
    "    random_state=0\n",
    ")\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# 3) UMAP projection\n",
    "umap_model = UMAP(\n",
    "    n_components=2,\n",
    "    n_neighbors=UMAP_N_NEIGHBORS,\n",
    "    min_dist=UMAP_MIN_DIST,\n",
    "    metric=UMAP_METRIC,\n",
    "    random_state=0\n",
    ")\n",
    "X_umap = umap_model.fit_transform(X)\n",
    "\n",
    "# 4) Plot side-by-side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,6), constrained_layout=True)\n",
    "for ax, data, title in zip(\n",
    "    axes,\n",
    "    [X_tsne, X_umap],\n",
    "    [\n",
    "      f\"t-SNE (perplexity={perp_capped})\",\n",
    "      f\"UMAP (n_neighbors={UMAP_N_NEIGHBORS}, min_dist={UMAP_MIN_DIST})\"\n",
    "    ]\n",
    "):\n",
    "    for lbl in set(labels):\n",
    "        mask = np.array(labels) == lbl\n",
    "        ax.scatter(data[mask,0], data[mask,1], label=lbl, alpha=0.8)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Dim 1')\n",
    "    ax.set_ylabel('Dim 2')\n",
    "    ax.grid(True)\n",
    "    ax.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7f5814-af63-489d-9e98-6ef4dc3613b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🛠 Exercise: 2D Embedding & Clustering\n",
    "\n",
    "### 🎯 Task  \n",
    "Take a **40-dimensional feature representation** of **environmental sounds** and reduce it to **2 dimensions** using your choice of **PCA**, **t-SNE**, or **UMAP**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧾 Steps\n",
    "\n",
    "1. **Prepare Features**  \n",
    "   Load or compute a **40-D feature matrix** for a collection of environmental audio clips.  \n",
    "   Example features:\n",
    "   - MFCC means/variances\n",
    "   - Spectral contrast\n",
    "   - Zero-crossing rate\n",
    "   - RMS energy\n",
    "\n",
    "2. **Dimensionality Reduction**  \n",
    "   Apply one of the following methods to project the 40-D data down to 2D:\n",
    "   - **PCA**\n",
    "   - **t-SNE**\n",
    "   - **UMAP**\n",
    "\n",
    "3. **Clustering**  \n",
    "   Run a clustering algorithm (e.g., **K-Means**) on the 2D embeddings.\n",
    "\n",
    "4. **Visualization**  \n",
    "   Plot the 2D points:\n",
    "   - **Color-coded by cluster label**\n",
    "   - Optionally **overlay true class labels** if available\n",
    "\n",
    "---\n",
    "\n",
    "### 📦 Deliverables\n",
    "\n",
    "- 📊 **2D Embedding Plots**  \n",
    "  - Clear cluster visualization for each method you try\n",
    "\n",
    "- 🔍 **Quantitative Evaluation**  \n",
    "  - Use metrics like:\n",
    "    - **Silhouette score**\n",
    "    - **Davies–Bouldin index**\n",
    "  - Compare performance across PCA, t-SNE, and UMAP\n",
    "\n",
    "- ✍️ **Brief Discussion**  \n",
    "  - Which dimensionality reduction method **best preserves class structure**, and **why**?\n",
    "  - Reflect on the **strengths/limitations** of each approach\n",
    "\n",
    "---\n",
    "\n",
    "💡 *Tip:* Run all three techniques side-by-side to compare not only **visual separability**, but also **quantitative clustering quality**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c60ed6-1a9b-4d1b-a568-91aba4e419fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
